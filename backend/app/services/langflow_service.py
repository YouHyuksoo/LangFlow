import os
import json
from typing import Dict, Any, List, Optional
from ..core.config import settings
from datetime import datetime

class LangflowService:
    """Langflowì™€ì˜ ì—°ë™ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # Flow ì„œë¹„ìŠ¤ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬ (ìˆœí™˜ import ë°©ì§€)
        self._flow_service = None
        # íŒŒì¼ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ ì§€ì—° ë¡œë”©)
        self._file_service = None
        # ì˜ˆì‹œ Flow ë°ì´í„° (ì‹¤ì œë¡œëŠ” LangFlow APIì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        self.example_flows = [
            {
                "flow_id": "example-rag-flow",
                "name": "RAG ê²€ìƒ‰ Flow",
                "description": "ë¬¸ì„œ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µì„ ìœ„í•œ RAG Flow",
                "created_at": "2024-01-15T10:00:00Z",
                "updated_at": "2024-01-20T14:30:00Z",
                "is_active": True,
                "components": [
                    {
                        "id": "input-node",
                        "type": "node",
                        "name": "ì‚¬ìš©ì ì…ë ¥",
                        "position": {"x": 100, "y": 100},
                        "data": {"input_type": "text"}
                    },
                    {
                        "id": "vector-store",
                        "type": "node", 
                        "name": "ë²¡í„° ì €ì¥ì†Œ",
                        "position": {"x": 300, "y": 100},
                        "data": {"store_type": "chroma"}
                    },
                    {
                        "id": "llm-node",
                        "type": "node",
                        "name": "LLM ì²˜ë¦¬",
                        "position": {"x": 500, "y": 100},
                        "data": {"model": "gpt-3.5-turbo"}
                    },
                    {
                        "id": "edge-1",
                        "type": "edge",
                        "source": "input-node",
                        "target": "vector-store"
                    },
                    {
                        "id": "edge-2", 
                        "type": "edge",
                        "source": "vector-store",
                        "target": "llm-node"
                    }
                ],
                "execution_stats": {
                    "total_executions": 150,
                    "last_execution": "2024-01-20T14:25:00Z",
                    "success_rate": 95.5
                },
                "flow_data": {
                    "nodes": [
                        {"id": "input-node", "type": "input", "data": {"input_type": "text"}},
                        {"id": "vector-store", "type": "vectorstore", "data": {"store_type": "chroma"}},
                        {"id": "llm-node", "type": "llm", "data": {"model": "gpt-3.5-turbo"}}
                    ],
                    "edges": [
                        {"source": "input-node", "target": "vector-store"},
                        {"source": "vector-store", "target": "llm-node"}
                    ]
                }
            },
            {
                "flow_id": "example-vectorization-flow",
                "name": "ë¬¸ì„œ ë²¡í„°í™” Flow",
                "description": "PDF ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” Flow",
                "created_at": "2024-01-10T09:00:00Z",
                "updated_at": "2024-01-18T16:45:00Z",
                "is_active": True,
                "components": [
                    {
                        "id": "file-input",
                        "type": "node",
                        "name": "íŒŒì¼ ì…ë ¥",
                        "position": {"x": 100, "y": 150},
                        "data": {"file_type": "pdf"}
                    },
                    {
                        "id": "text-splitter",
                        "type": "node",
                        "name": "í…ìŠ¤íŠ¸ ë¶„í• ",
                        "position": {"x": 300, "y": 150},
                        "data": {"chunk_size": 1000}
                    },
                    {
                        "id": "embedding",
                        "type": "node",
                        "name": "ì„ë² ë”© ìƒì„±",
                        "position": {"x": 500, "y": 150},
                        "data": {"model": "text-embedding-ada-002"}
                    },
                    {
                        "id": "edge-3",
                        "type": "edge",
                        "source": "file-input",
                        "target": "text-splitter"
                    },
                    {
                        "id": "edge-4", 
                        "type": "edge",
                        "source": "text-splitter",
                        "target": "embedding"
                    }
                ],
                "execution_stats": {
                    "total_executions": 75,
                    "last_execution": "2024-01-19T11:20:00Z",
                    "success_rate": 88.0
                },
                "flow_data": {
                    "nodes": [
                        {"id": "file-input", "type": "fileinput", "data": {"file_type": "pdf"}},
                        {"id": "text-splitter", "type": "textsplitter", "data": {"chunk_size": 1000}},
                        {"id": "embedding", "type": "embedding", "data": {"model": "text-embedding-ada-002"}}
                    ],
                    "edges": [
                        {"source": "file-input", "target": "text-splitter"},
                        {"source": "text-splitter", "target": "embedding"}
                    ]
                }
            }
        ]
    
    @property
    def flow_service(self):
        """Flow ì„œë¹„ìŠ¤ì˜ ì§€ì—° ë¡œë”© í”„ë¡œí¼í‹°"""
        if self._flow_service is None:
            from .flow_service import FlowService
            self._flow_service = FlowService()
        return self._flow_service
    
    @property
    def file_service(self):
        """íŒŒì¼ ì„œë¹„ìŠ¤ì˜ ì§€ì—° ë¡œë”© í”„ë¡œí¼í‹°"""
        if self._file_service is None:
            from .file_service import FileService
            self._file_service = FileService()
        return self._file_service
    
    async def process_file_with_flow(self, file_id: str, flow_id: str, file_info: Any = None) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì¼ì„ Langflowë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            # "files"ëŠ” ì˜ëª»ëœ Flow IDì´ë¯€ë¡œ ì¦‰ì‹œ ì—ëŸ¬ ë°˜í™˜
            if flow_id == "files":
                print(f"ì˜ëª»ëœ Flow ID ìš”ì²­: {flow_id} - ë¬´ì‹œë¨")
                return {
                    "file_id": file_id,
                    "status": "error",
                    "error": f"ì˜ëª»ëœ Flow ID: {flow_id}"
                }
                
            # íŒŒì¼ ì •ë³´ê°€ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš° None ì²˜ë¦¬
            if not file_info:
                return {
                    "file_id": file_id,
                    "status": "error",
                    "error": "íŒŒì¼ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }
            
            # Flow ì‹¤í–‰ì„ ìœ„í•œ ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œ ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì‚¬ìš©)
            flow_input = {
                "file_id": file_id,
                "filename": file_info.filename,
                "category_id": file_info.category_id,
                "category_name": file_info.category_name
            }
            
            # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì„¤ì • - ë°˜ë“œì‹œ í•„ìš”
            if hasattr(file_info, 'file_path') and file_info.file_path:
                flow_input["file_path"] = file_info.file_path
                print(f"ì‹¤ì œ ì—…ë¡œë“œ íŒŒì¼ ê²½ë¡œ ì „ë‹¬: {file_info.file_path}")
                
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if os.path.exists(file_info.file_path):
                    file_size = os.path.getsize(file_info.file_path)
                    print(f"íŒŒì¼ í™•ì¸ë¨ - í¬ê¸°: {file_size} bytes")
                else:
                    print(f"ê²½ê³ : íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_info.file_path}")
            else:
                print(f"ì˜¤ë¥˜: íŒŒì¼ ê²½ë¡œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ - file_info: {file_info}")
                # íŒŒì¼ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ íŒŒì¼ ê²½ë¡œ ë‹¤ì‹œ ì¡°íšŒ
                actual_file_path = await self.file_service.get_file_path(file_id)
                if actual_file_path:
                    flow_input["file_path"] = actual_file_path
                    print(f"íŒŒì¼ ì„œë¹„ìŠ¤ì—ì„œ ê²½ë¡œ ì¡°íšŒ ì„±ê³µ: {actual_file_path}")
                else:
                    print(f"ì˜¤ë¥˜: íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
            
            # Flow ì‹¤í–‰
            result = await self.flow_service.execute_flow(flow_id, flow_input)
            
            # Flow ì‹¤í–‰ì´ ì„±ê³µí•œ ê²½ìš° ë²¡í„° ë°ì´í„° ì €ì¥ ë° íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸
            if result.get("status") == "completed":
                try:
                    # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    file_path = await self.file_service.get_file_path(file_id)
                    if file_path and os.path.exists(file_path):
                        # PDF íŒŒì¼ì¸ ê²½ìš° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if file_path.lower().endswith('.pdf'):
                            text = await self.file_service.extract_text_from_pdf(file_path)
                        else:
                            # ë‹¤ë¥¸ íŒŒì¼ í˜•ì‹ì€ ê°„ë‹¨íˆ ì½ê¸°
                            with open(file_path, 'r', encoding='utf-8') as f:
                                text = f.read()
                        
                        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                        chunks = await self.file_service.chunk_text(text)
                        
                        # ë²¡í„° ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                        metadata = {
                            "file_id": file_id,
                            "filename": file_info.filename,
                            "category_id": file_info.category_id,
                            "category_name": file_info.category_name,
                            "flow_id": flow_id,
                            "vectorization_method": "langflow",
                            "chunk_count": len(chunks),
                            "file_size": os.path.getsize(file_path) if os.path.exists(file_path) else 0
                        }
                        
                        # VectorServiceë¥¼ í†µí•´ ë²¡í„° ë°ì´í„° ì €ì¥
                        from .vector_service import VectorService
                        vector_service = VectorService()
                        save_success = await vector_service.add_document_chunks(file_id, chunks, metadata)
                        
                        if save_success:
                            print(f"ë²¡í„° ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_id}, ì²­í¬ ìˆ˜: {len(chunks)}")
                            result["vectorized_chunks"] = len(chunks)
                            
                            # íŒŒì¼ ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ - ì„±ê³µ
                            await self.file_service.update_file_vectorization_status(
                                file_id=file_id,
                                vectorized=True,
                                error_message=None
                            )
                            print(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {file_id} -> vectorized=True")
                        else:
                            print(f"ë²¡í„° ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {file_id}")
                            result["status"] = "error"
                            result["error"] = "ë²¡í„° ë°ì´í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                            
                            # íŒŒì¼ ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ - ì‹¤íŒ¨
                            await self.file_service.update_file_vectorization_status(
                                file_id=file_id,
                                vectorized=False,
                                error_message="ë²¡í„° ë°ì´í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                            )
                    
                except Exception as e:
                    print(f"ë²¡í„° ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    result["status"] = "error"
                    result["error"] = f"ë²¡í„° ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                    
                    # íŒŒì¼ ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ - ì˜¤ë¥˜
                    await self.file_service.update_file_vectorization_status(
                        file_id=file_id,
                        vectorized=False,
                        error_message=f"ë²¡í„° ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                    )
            else:
                # Flow ì‹¤í–‰ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸
                error_msg = result.get("error", "Flow ì‹¤í–‰ ì‹¤íŒ¨")
                await self.file_service.update_file_vectorization_status(
                    file_id=file_id,
                    vectorized=False,
                    error_message=error_msg
                )
            
            # ê²°ê³¼ì— íŒŒì¼ ì •ë³´ ì¶”ê°€
            result["file_id"] = file_id
            result["filename"] = file_info.filename
            
            return result
            
        except Exception as e:
            print(f"Langflow íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "file_id": file_id,
                "status": "error",
                "error": str(e)
            }
    
    async def get_flows(self) -> List[Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë“  LangFlow Flow ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            flows = []
            flows_dir = os.path.join(settings.BASE_DIR, "langflow", "flows")
            
            # flows ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if not os.path.exists(flows_dir):
                print(f"Flows ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {flows_dir}")
                # ìƒëŒ€ ê²½ë¡œë¡œë„ ì‹œë„
                flows_dir = "langflow/flows"
                if not os.path.exists(flows_dir):
                    print(f"ìƒëŒ€ ê²½ë¡œë„ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {flows_dir}")
                    return []
            
            # JSON íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ Flow ì •ë³´ ìƒì„±
            for filename in os.listdir(flows_dir):
                if filename.endswith('.json') and not filename.startswith('.'):
                    file_path = os.path.join(flows_dir, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            flow_data = json.load(f)
                        
                        # Flow IDëŠ” íŒŒì¼ëª…ì—ì„œ í™•ì¥ìë¥¼ ì œê±°í•˜ê³  ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
                        flow_id = filename.replace('.json', '').replace(' ', '_').lower()
                        
                        # Flow ì •ë³´ êµ¬ì„±
                        flow_info = {
                            "flow_id": flow_id,
                            "name": flow_data.get("name", flow_id),
                            "description": flow_data.get("description", f"{flow_id} Flow"),
                            "created_at": flow_data.get("created_at", "2024-01-01T00:00:00Z"),
                            "updated_at": flow_data.get("updated_at", "2024-01-01T00:00:00Z"),
                            "is_active": flow_data.get("is_active", True),
                            "components": flow_data.get("components", []),
                            "flow_data": flow_data,
                            "original_filename": filename  # ì›ë³¸ íŒŒì¼ëª… ì €ì¥
                        }
                        
                        flows.append(flow_info)
                        print(f"Flow ë¡œë“œë¨: {flow_id} (íŒŒì¼: {filename})")
                        
                    except Exception as e:
                        print(f"Flow íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({filename}): {str(e)}")
                        continue
            
            # ìƒì„± ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
            flows.sort(key=lambda x: x["created_at"], reverse=True)
            
            print(f"ì´ {len(flows)}ê°œì˜ Flowë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return flows
            
        except Exception as e:
            print(f"Flow ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def get_flow_details(self, flow_id: str) -> Dict[str, Any]:
        """íŠ¹ì • Flowì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            # "files"ëŠ” ì˜ëª»ëœ Flow IDì´ë¯€ë¡œ ì¦‰ì‹œ None ë°˜í™˜
            if flow_id == "files":
                print(f"ğŸš¨ ì˜ëª»ëœ Flow ID ìš”ì²­: {flow_id}")
                import traceback
                print("=== í˜¸ì¶œ ìŠ¤íƒ ì¶”ì  ===")
                traceback.print_stack()
                print("======================")
                return None
                
            # ì‹¤ì œ Flow ëª©ë¡ì—ì„œ í•´ë‹¹ Flow ì°¾ê¸°
            flows = await self.get_flows()
            target_flow = None
            
            for flow in flows:
                if flow["flow_id"] == flow_id:
                    target_flow = flow
                    break
            
            if not target_flow:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return None
            
            # Flow íŒŒì¼ ê²½ë¡œ (ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©)
            flows_dir = os.path.join(settings.BASE_DIR, "langflow", "flows")
            if not os.path.exists(flows_dir):
                flows_dir = "langflow/flows"
            
            flow_file = os.path.join(flows_dir, target_flow["original_filename"])
            
            if not os.path.exists(flow_file):
                print(f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_file}")
                return None
            
            # Flow íŒŒì¼ ì½ê¸°
            with open(flow_file, 'r', encoding='utf-8') as f:
                flow_data = json.load(f)
            
            # Flow ìƒì„¸ ì •ë³´ êµ¬ì„±
            flow_details = {
                "flow_id": flow_id,
                "name": flow_data.get("name", flow_id),
                "description": flow_data.get("description", f"{flow_id} Flow"),
                "created_at": flow_data.get("created_at", "2024-01-01T00:00:00Z"),
                "updated_at": flow_data.get("updated_at", "2024-01-01T00:00:00Z"),
                "is_active": flow_data.get("is_active", True),
                "components": flow_data.get("components", []),
                "execution_stats": flow_data.get("execution_stats", {
                    "total_executions": 0,
                    "last_execution": None,
                    "success_rate": 0.0
                }),
                "flow_data": flow_data
            }
            
            return flow_details
            
        except Exception as e:
            print(f"Flow ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def test_flow(self, flow_id: str, test_data: Dict[str, Any]) -> Dict[str, Any]:
        """Flowë¥¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # Flow ì‹¤í–‰
            result = await self.flow_service.execute_flow(flow_id, test_data)
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ í¬í•¨
            result["flow_id"] = flow_id
            result["test_data"] = test_data
            
            return result
            
        except Exception as e:
            print(f"Flow í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "flow_id": flow_id,
                "status": "error",
                "error": str(e),
                "test_data": test_data
            }
    
    async def toggle_flow_status(self, flow_id: str) -> bool:
        """Flowì˜ í™œì„±/ë¹„í™œì„± ìƒíƒœë¥¼ í† ê¸€í•©ë‹ˆë‹¤."""
        try:
            # "files"ëŠ” ì˜ëª»ëœ Flow IDì´ë¯€ë¡œ ì¦‰ì‹œ False ë°˜í™˜
            if flow_id == "files":
                print(f"ì˜ëª»ëœ Flow ID ìš”ì²­: {flow_id} - ë¬´ì‹œë¨")
                return False
                
            # ì‹¤ì œ Flow ëª©ë¡ì—ì„œ í•´ë‹¹ Flow ì°¾ê¸°
            flows = await self.get_flows()
            target_flow = None
            
            for flow in flows:
                if flow["flow_id"] == flow_id:
                    target_flow = flow
                    break
            
            if not target_flow:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return False
            
            # Flow íŒŒì¼ ê²½ë¡œ (ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©)
            flows_dir = os.path.join(settings.BASE_DIR, "langflow", "flows")
            if not os.path.exists(flows_dir):
                flows_dir = "langflow/flows"
            
            flow_file = os.path.join(flows_dir, target_flow["original_filename"])
            
            if not os.path.exists(flow_file):
                print(f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_file}")
                return False
            
            # Flow íŒŒì¼ ì½ê¸°
            with open(flow_file, 'r', encoding='utf-8') as f:
                flow_data = json.load(f)
            
            # ìƒíƒœ í† ê¸€
            current_status = flow_data.get("is_active", True)
            flow_data["is_active"] = not current_status
            
            # íŒŒì¼ì— ë‹¤ì‹œ ì €ì¥
            with open(flow_file, 'w', encoding='utf-8') as f:
                json.dump(flow_data, f, indent=2, ensure_ascii=False)
            
            print(f"Flow ìƒíƒœ ë³€ê²½: {flow_id} -> {'í™œì„±' if flow_data['is_active'] else 'ë¹„í™œì„±'}")
            return True
            
        except Exception as e:
            print(f"Flow ìƒíƒœ í† ê¸€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def set_default_vectorization_flow(self, flow_id: str) -> bool:
        """Flowë¥¼ ê¸°ë³¸ ë²¡í„°í™” Flowë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            # "files"ëŠ” ì˜ëª»ëœ Flow IDì´ë¯€ë¡œ ì¦‰ì‹œ False ë°˜í™˜
            if flow_id == "files":
                print(f"ì˜ëª»ëœ Flow ID ìš”ì²­: {flow_id} - ë¬´ì‹œë¨")
                return False
                
            # ì‹¤ì œ Flow ëª©ë¡ì—ì„œ í•´ë‹¹ Flow ì°¾ê¸°
            flows = await self.get_flows()
            target_flow = None
            
            for flow in flows:
                if flow["flow_id"] == flow_id:
                    target_flow = flow
                    break
            
            if not target_flow:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return False
            
            # ëª¨ë“  Flow íŒŒì¼ì—ì„œ ê¸°ë³¸ ë²¡í„°í™” ì„¤ì •ì„ í•´ì œ
            flows_dir = os.path.join(settings.BASE_DIR, "langflow", "flows")
            if not os.path.exists(flows_dir):
                flows_dir = "langflow/flows"
            
            for flow in flows:
                flow_file = os.path.join(flows_dir, flow["original_filename"])
                if os.path.exists(flow_file):
                    try:
                        with open(flow_file, 'r', encoding='utf-8') as f:
                            flow_data = json.load(f)
                        
                        # ê¸°ë³¸ ë²¡í„°í™” ì„¤ì • í•´ì œ
                        if "is_default_vectorization" in flow_data:
                            flow_data["is_default_vectorization"] = False
                            
                        with open(flow_file, 'w', encoding='utf-8') as f:
                            json.dump(flow_data, f, indent=2, ensure_ascii=False)
                    except Exception as e:
                        print(f"Flow íŒŒì¼ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ ({flow['flow_id']}): {str(e)}")
            
            # ëŒ€ìƒ Flowë¥¼ ê¸°ë³¸ ë²¡í„°í™” Flowë¡œ ì„¤ì •
            target_flow_file = os.path.join(flows_dir, target_flow["original_filename"])
            if os.path.exists(target_flow_file):
                with open(target_flow_file, 'r', encoding='utf-8') as f:
                    flow_data = json.load(f)
                
                flow_data["is_default_vectorization"] = True
                
                with open(target_flow_file, 'w', encoding='utf-8') as f:
                    json.dump(flow_data, f, indent=2, ensure_ascii=False)
            
            # ì„¤ì • íŒŒì¼ì— ê¸°ë³¸ Flow ID ì €ì¥ (ì„ì‹œë¡œ íŒŒì¼ì— ì €ì¥)
            config_file = os.path.join(settings.BASE_DIR, "langflow", "config.json")
            config_dir = os.path.dirname(config_file)
            os.makedirs(config_dir, exist_ok=True)
            
            config_data = {
                "default_vectorization_flow_id": flow_id,
                "updated_at": datetime.now().isoformat()
            }
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            
            print(f"ê¸°ë³¸ ë²¡í„°í™” Flow ì„¤ì •: {flow_id} ({target_flow['name']})")
            return True
            
        except Exception as e:
            print(f"ê¸°ë³¸ ë²¡í„°í™” Flow ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def set_search_flow(self, flow_id: str) -> bool:
        """ê²€ìƒ‰ Flow ì„¤ì •"""
        try:
            # Flow ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            flows = await self.get_flows()
            flow_exists = any(flow["flow_id"] == flow_id for flow in flows)
            
            if not flow_exists:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return False
            
            # ì„¤ì • íŒŒì¼ì— ê²€ìƒ‰ Flow ID ì €ì¥
            config_file = os.path.join(settings.DATA_DIR, "langflow_config.json")
            config = {}
            
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                except Exception as e:
                    print(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    config = {}
            
            config["default_search_flow_id"] = flow_id
            
            try:
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
                print(f"ê²€ìƒ‰ Flowê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {flow_id}")
                return True
            except Exception as e:
                print(f"ì„¤ì • íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                return False
                
        except Exception as e:
            print(f"ê²€ìƒ‰ Flow ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            return False

    async def delete_flow(self, flow_id: str) -> bool:
        """Flow ì‚­ì œ"""
        try:
            flows = await self.get_flows() # Get all flows to find the original filename
            target_flow = next((flow for flow in flows if flow["flow_id"] == flow_id), None)

            if not target_flow:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return False

            flow_file_path = os.path.join(settings.BASE_DIR, "langflow", "flows", target_flow["original_filename"])
            
            # Flow ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(flow_file_path):
                print(f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_file_path}")
                return False
            
            # Flowê°€ ê¸°ë³¸ ë²¡í„°í™” Flowì¸ì§€ í™•ì¸
            config_file = os.path.join(settings.DATA_DIR, "langflow_config.json")
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    
                    # ê¸°ë³¸ ë²¡í„°í™” Flowì¸ ê²½ìš° ì„¤ì • ì œê±°
                    if config.get("default_vectorization_flow_id") == flow_id:
                        config["default_vectorization_flow_id"] = None
                        with open(config_file, 'w', encoding='utf-8') as f:
                            json.dump(config, f, ensure_ascii=False, indent=2)
                        print(f"ê¸°ë³¸ ë²¡í„°í™” Flow ì„¤ì •ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤: {flow_id}")
                    
                    # ê¸°ë³¸ ê²€ìƒ‰ Flowì¸ ê²½ìš° ì„¤ì • ì œê±°
                    if config.get("default_search_flow_id") == flow_id:
                        config["default_search_flow_id"] = None
                        with open(config_file, 'w', encoding='utf-8') as f:
                            json.dump(config, f, ensure_ascii=False, indent=2)
                        print(f"ê¸°ë³¸ ê²€ìƒ‰ Flow ì„¤ì •ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤: {flow_id}")
                        
                except Exception as e:
                    print(f"ì„¤ì • íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            
            # Flow íŒŒì¼ ì‚­ì œ
            try:
                os.remove(flow_file_path)
                print(f"Flow íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {flow_file_path}")
                return True
            except Exception as e:
                print(f"Flow íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
                return False
                
        except Exception as e:
            print(f"Flow ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def vectorize_files_by_category(self, category_id: str, vectorization_flow_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ë²¡í„°í™”í•©ë‹ˆë‹¤."""
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
            files = await self.file_service.list_files(category_id=category_id)
            
            results = []
            for file_info in files:
                if not file_info.vectorized:  # ì•„ì§ ë²¡í„°í™”ë˜ì§€ ì•Šì€ íŒŒì¼ë§Œ
                    result = await self.process_file_with_flow(
                        file_info.file_id, 
                        vectorization_flow_id,
                        file_info
                    )
                    results.append(result)
            
            return results
            
        except Exception as e:
            print(f"ì¹´í…Œê³ ë¦¬ë³„ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def search_with_flow(self, query: str, search_flow_id: str, category_ids: List[str] = None, top_k: int = 10) -> Dict[str, Any]:
        """Langflowë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # LangFlow Flow ëŒ€ì‹  ì§ì ‘ ChromaDB ê²€ìƒ‰ ì‚¬ìš©
            from .vector_service import VectorService
            
            print(f"ì§ì ‘ ChromaDB ê²€ìƒ‰ ì‹¤í–‰: {search_flow_id}")
            print(f"ì¿¼ë¦¬: {query}")
            print(f"ì¹´í…Œê³ ë¦¬: {category_ids}")
            print(f"ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {top_k}ê°œ")
            
            vector_service = VectorService()
            search_results = await vector_service.search_similar_chunks(
                query=query,
                top_k=top_k,
                category_ids=category_ids
            )
            
            print(f"ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ë°œê²¬")
            
            if search_results:
                return {
                    "status": "success",
                    "results": search_results,
                    "flow_id": search_flow_id,
                    "search_method": "direct_chromadb",
                    "query": query,
                    "category_ids": category_ids,
                    "top_k": top_k
                }
            else:
                return {
                    "status": "success",
                    "results": [],
                    "flow_id": search_flow_id,
                    "search_method": "direct_chromadb",
                    "query": query,
                    "category_ids": category_ids,
                    "top_k": top_k
                }
                
        except Exception as e:
            print(f"ì§ì ‘ ChromaDB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "query": query,
                "status": "error",
                "error": str(e),
                "response": "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
    
    async def get_available_flows_by_type(self, flow_type: str) -> List[Dict[str, Any]]:
        """íƒ€ì…ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ Flow ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            all_flows = await self.get_flows()
            
            # Flow ì´ë¦„ì´ë‚˜ ì„¤ëª…ì—ì„œ íƒ€ì… í•„í„°ë§ (ê°„ë‹¨í•œ êµ¬í˜„)
            type_keywords = {
                "vectorization": ["ë²¡í„°", "vector", "embed", "chunk"],
                "search": ["ê²€ìƒ‰", "search", "rag", "retrieval"],
                "chat": ["ì±„íŒ…", "chat", "conversation"]
            }
            
            keywords = type_keywords.get(flow_type, [])
            filtered_flows = []
            
            for flow in all_flows:
                flow_dict = {
                    "flow_id": flow["flow_id"],
                    "name": flow["name"],
                    "created_at": flow["created_at"]
                }
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                if any(keyword in flow["name"].lower() for keyword in keywords):
                    flow_dict["recommended"] = True
                else:
                    flow_dict["recommended"] = False
                
                filtered_flows.append(flow_dict)
            
            return filtered_flows
            
        except Exception as e:
            print(f"Flow ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def get_vectorization_status(self) -> Dict[str, Any]:
        """ì „ì²´ ë²¡í„°í™” ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            all_files = await self.file_service.list_files()
            
            total_files = len(all_files)
            vectorized_files = len([f for f in all_files if f.vectorized])
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_stats = {}
            for file_info in all_files:
                category_name = file_info.category_name or "ë¯¸ë¶„ë¥˜"
                if category_name not in category_stats:
                    category_stats[category_name] = {"total": 0, "vectorized": 0}
                
                category_stats[category_name]["total"] += 1
                if file_info.vectorized:
                    category_stats[category_name]["vectorized"] += 1
            
            return {
                "total_files": total_files,
                "vectorized_files": vectorized_files,
                "vectorization_rate": round(vectorized_files / total_files * 100, 1) if total_files > 0 else 0,
                "category_stats": category_stats
            }
            
        except Exception as e:
            print(f"ë²¡í„°í™” ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "total_files": 0,
                "vectorized_files": 0,
                "vectorization_rate": 0,
                "category_stats": {}
            }