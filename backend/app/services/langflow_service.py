import os
import json
import time
from typing import Dict, Any, List, Optional
from ..core.config import settings
from datetime import datetime

class LangflowService:
    """Langflowì™€ì˜ ì—°ë™ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # Flow ì„œë¹„ìŠ¤ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬ (ìˆœí™˜ import ë°©ì§€)
        self._flow_service = None
        # íŒŒì¼ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ ì§€ì—° ë¡œë”©)
        self._file_service = None
        # ì˜ˆì‹œ Flow ë°ì´í„° (ì‹¤ì œë¡œëŠ” LangFlow APIì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        self.example_flows = [
            {
                "flow_id": "example-rag-flow",
                "name": "RAG ê²€ìƒ‰ Flow",
                "description": "ë¬¸ì„œ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µì„ ìœ„í•œ RAG Flow",
                "created_at": "2024-01-15T10:00:00Z",
                "updated_at": "2024-01-20T14:30:00Z",
                "is_active": True,
                "components": [
                    {
                        "id": "input-node",
                        "type": "node",
                        "name": "ì‚¬ìš©ì ì…ë ¥",
                        "position": {"x": 100, "y": 100},
                        "data": {"input_type": "text"}
                    },
                    {
                        "id": "vector-store",
                        "type": "node", 
                        "name": "ë²¡í„° ì €ì¥ì†Œ",
                        "position": {"x": 300, "y": 100},
                        "data": {"store_type": "chroma"}
                    },
                    {
                        "id": "llm-node",
                        "type": "node",
                        "name": "LLM ì²˜ë¦¬",
                        "position": {"x": 500, "y": 100},
                        "data": {"model": "gpt-3.5-turbo"}
                    },
                    {
                        "id": "edge-1",
                        "type": "edge",
                        "source": "input-node",
                        "target": "vector-store"
                    },
                    {
                        "id": "edge-2", 
                        "type": "edge",
                        "source": "vector-store",
                        "target": "llm-node"
                    }
                ],
                "execution_stats": {
                    "total_executions": 150,
                    "last_execution": "2024-01-20T14:25:00Z",
                    "success_rate": 95.5
                },
                "flow_data": {
                    "nodes": [
                        {"id": "input-node", "type": "input", "data": {"input_type": "text"}},
                        {"id": "vector-store", "type": "vectorstore", "data": {"store_type": "chroma"}},
                        {"id": "llm-node", "type": "llm", "data": {"model": "gpt-3.5-turbo"}}
                    ],
                    "edges": [
                        {"source": "input-node", "target": "vector-store"},
                        {"source": "vector-store", "target": "llm-node"}
                    ]
                }
            },
            {
                "flow_id": "example-vectorization-flow",
                "name": "ë¬¸ì„œ ë²¡í„°í™” Flow",
                "description": "PDF ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” Flow",
                "created_at": "2024-01-10T09:00:00Z",
                "updated_at": "2024-01-18T16:45:00Z",
                "is_active": True,
                "components": [
                    {
                        "id": "file-input",
                        "type": "node",
                        "name": "íŒŒì¼ ì…ë ¥",
                        "position": {"x": 100, "y": 150},
                        "data": {"file_type": "pdf"}
                    },
                    {
                        "id": "text-splitter",
                        "type": "node",
                        "name": "í…ìŠ¤íŠ¸ ë¶„í• ",
                        "position": {"x": 300, "y": 150},
                        "data": {"chunk_size": 1000}
                    },
                    {
                        "id": "embedding",
                        "type": "node",
                        "name": "ì„ë² ë”© ìƒì„±",
                        "position": {"x": 500, "y": 150},
                        "data": {"model": "text-embedding-ada-002"}
                    },
                    {
                        "id": "edge-3",
                        "type": "edge",
                        "source": "file-input",
                        "target": "text-splitter"
                    },
                    {
                        "id": "edge-4", 
                        "type": "edge",
                        "source": "text-splitter",
                        "target": "embedding"
                    }
                ],
                "execution_stats": {
                    "total_executions": 75,
                    "last_execution": "2024-01-19T11:20:00Z",
                    "success_rate": 88.0
                },
                "flow_data": {
                    "nodes": [
                        {"id": "file-input", "type": "fileinput", "data": {"file_type": "pdf"}},
                        {"id": "text-splitter", "type": "textsplitter", "data": {"chunk_size": 1000}},
                        {"id": "embedding", "type": "embedding", "data": {"model": "text-embedding-ada-002"}}
                    ],
                    "edges": [
                        {"source": "file-input", "target": "text-splitter"},
                        {"source": "text-splitter", "target": "embedding"}
                    ]
                }
            }
        ]
    
    @property
    def flow_service(self):
        """Flow ì„œë¹„ìŠ¤ì˜ ì§€ì—° ë¡œë”© í”„ë¡œí¼í‹°"""
        if self._flow_service is None:
            from .flow_service import FlowService
            self._flow_service = FlowService()
        return self._flow_service
    
    @property
    def file_service(self):
        """íŒŒì¼ ì„œë¹„ìŠ¤ì˜ ì§€ì—° ë¡œë”© í”„ë¡œí¼í‹°"""
        if self._file_service is None:
            from .file_service import FileService
            self._file_service = FileService()
        return self._file_service
    
    async def process_file_with_flow(self, file_id: str, flow_id: str, file_info: Any = None) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì¼ì„ Langflowë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            # "files"ëŠ” ì˜ëª»ëœ Flow IDì´ë¯€ë¡œ ì¦‰ì‹œ ì—ëŸ¬ ë°˜í™˜
            if flow_id == "files":
                print(f"ì˜ëª»ëœ Flow ID ìš”ì²­: {flow_id} - ë¬´ì‹œë¨")
                return {
                    "file_id": file_id,
                    "status": "error",
                    "error": f"ì˜ëª»ëœ Flow ID: {flow_id}"
                }
                
            # íŒŒì¼ ì •ë³´ê°€ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš° None ì²˜ë¦¬
            if not file_info:
                return {
                    "file_id": file_id,
                    "status": "error",
                    "error": "íŒŒì¼ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }
            
            # Flow ì‹¤í–‰ì„ ìœ„í•œ ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œ ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì‚¬ìš©)
            flow_input = {
                "file_id": file_id,
                "filename": file_info.filename,
                "category_id": file_info.category_id,
                "category_name": file_info.category_name
            }
            
            # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì„¤ì • - ë°˜ë“œì‹œ í•„ìš”
            if hasattr(file_info, 'file_path') and file_info.file_path:
                flow_input["file_path"] = file_info.file_path
                print(f"ì‹¤ì œ ì—…ë¡œë“œ íŒŒì¼ ê²½ë¡œ ì „ë‹¬: {file_info.file_path}")
                
                # íŒŒì¼ ì¡´ì¬ í™•ì¸
                if os.path.exists(file_info.file_path):
                    file_size = os.path.getsize(file_info.file_path)
                    print(f"íŒŒì¼ í™•ì¸ë¨ - í¬ê¸°: {file_size} bytes")
                else:
                    print(f"ê²½ê³ : íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_info.file_path}")
            else:
                print(f"ì˜¤ë¥˜: íŒŒì¼ ê²½ë¡œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ - file_info: {file_info}")
                # íŒŒì¼ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ íŒŒì¼ ê²½ë¡œ ë‹¤ì‹œ ì¡°íšŒ
                actual_file_path = await self.file_service.get_file_path(file_id)
                if actual_file_path:
                    flow_input["file_path"] = actual_file_path
                    print(f"íŒŒì¼ ì„œë¹„ìŠ¤ì—ì„œ ê²½ë¡œ ì¡°íšŒ ì„±ê³µ: {actual_file_path}")
                else:
                    print(f"ì˜¤ë¥˜: íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
            
            # Flow ì‹¤í–‰
            result = await self.flow_service.execute_flow(flow_id, flow_input)
            
            # Flow ì‹¤í–‰ì´ ì„±ê³µí•œ ê²½ìš° ë²¡í„° ë°ì´í„° ì €ì¥ ë° íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸
            if result.get("status") == "completed":
                try:
                    # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    file_path = await self.file_service.get_file_path(file_id)
                    if file_path and os.path.exists(file_path):
                        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        text = await self.file_service.extract_text_from_file(file_path)
                        
                        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                        chunks = await self.file_service.chunk_text(text)
                        
                        # ë²¡í„° ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                        metadata = {
                            "file_id": file_id,
                            "filename": file_info.filename,
                            "category_id": file_info.category_id,
                            "category_name": file_info.category_name,
                            "flow_id": flow_id,
                            "vectorization_method": "langflow",
                            "chunk_count": len(chunks),
                            "file_size": os.path.getsize(file_path) if os.path.exists(file_path) else 0
                        }
                        
                        # VectorServiceë¥¼ í†µí•´ ë²¡í„° ë°ì´í„° ì €ì¥
                        from .vector_service import VectorService
                        vector_service = VectorService()
                        save_success = await vector_service.add_document_chunks(file_id, chunks, metadata)
                        
                        if save_success:
                            print(f"ë²¡í„° ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_id}, ì²­í¬ ìˆ˜: {len(chunks)}")
                            result["vectorized_chunks"] = len(chunks)
                            
                            # íŒŒì¼ ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ - ì„±ê³µ
                            await self.file_service.update_file_vectorization_status(
                                file_id=file_id,
                                vectorized=True,
                                error_message=None
                            )
                            print(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {file_id} -> vectorized=True")
                        else:
                            print(f"ë²¡í„° ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {file_id}")
                            result["status"] = "error"
                            result["error"] = "ë²¡í„° ë°ì´í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                            
                            # íŒŒì¼ ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ - ì‹¤íŒ¨
                            await self.file_service.update_file_vectorization_status(
                                file_id=file_id,
                                vectorized=False,
                                error_message="ë²¡í„° ë°ì´í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                            )
                    
                except Exception as e:
                    print(f"ë²¡í„° ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    result["status"] = "error"
                    result["error"] = f"ë²¡í„° ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                    
                    # íŒŒì¼ ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ - ì˜¤ë¥˜
                    await self.file_service.update_file_vectorization_status(
                        file_id=file_id,
                        vectorized=False,
                        error_message=f"ë²¡í„° ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                    )
            else:
                # Flow ì‹¤í–‰ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸
                error_msg = result.get("error", "Flow ì‹¤í–‰ ì‹¤íŒ¨")
                await self.file_service.update_file_vectorization_status(
                    file_id=file_id,
                    vectorized=False,
                    error_message=error_msg
                )
            
            # ê²°ê³¼ì— íŒŒì¼ ì •ë³´ ì¶”ê°€
            result["file_id"] = file_id
            result["filename"] = file_info.filename
            
            return result
            
        except Exception as e:
            print(f"Langflow íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "file_id": file_id,
                "status": "error",
                "error": str(e)
            }
    
    async def get_flows(self) -> List[Dict[str, Any]]:
        """ë“±ë¡ëœ ëª¨ë“  LangFlow Flow ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            flows = []
            flows_dir = os.path.join(settings.BASE_DIR, "langflow", "flows")
            
            # flows ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if not os.path.exists(flows_dir):
                print(f"Flows ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {flows_dir}")
                # ìƒëŒ€ ê²½ë¡œë¡œë„ ì‹œë„
                flows_dir = "langflow/flows"
                if not os.path.exists(flows_dir):
                    print(f"ìƒëŒ€ ê²½ë¡œë„ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {flows_dir}")
                    return []
            
            # JSON íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ Flow ì •ë³´ ìƒì„±
            for filename in os.listdir(flows_dir):
                if filename.endswith('.json') and not filename.startswith('.'):
                    file_path = os.path.join(flows_dir, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            flow_data = json.load(f)
                        
                        # Flow IDëŠ” íŒŒì¼ëª…ì—ì„œ í™•ì¥ìë¥¼ ì œê±°í•˜ê³  ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
                        flow_id = filename.replace('.json', '').replace(' ', '_').lower()
                        
                        # Flow ì •ë³´ êµ¬ì„±
                        flow_info = {
                            "flow_id": flow_id,
                            "name": flow_data.get("name", flow_id),
                            "description": flow_data.get("description", f"{flow_id} Flow"),
                            "created_at": flow_data.get("created_at", "2024-01-01T00:00:00Z"),
                            "updated_at": flow_data.get("updated_at", "2024-01-01T00:00:00Z"),
                            "is_active": flow_data.get("is_active", True),
                            "components": flow_data.get("components", []),
                            "flow_data": flow_data,
                            "original_filename": filename  # ì›ë³¸ íŒŒì¼ëª… ì €ì¥
                        }
                        
                        flows.append(flow_info)
                        print(f"Flow ë¡œë“œë¨: {flow_id} (íŒŒì¼: {filename})")
                        
                    except Exception as e:
                        print(f"Flow íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({filename}): {str(e)}")
                        continue
            
            # ìƒì„± ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
            flows.sort(key=lambda x: x["created_at"], reverse=True)
            
            print(f"ì´ {len(flows)}ê°œì˜ Flowë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return flows
            
        except Exception as e:
            print(f"Flow ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def get_flow_details(self, flow_id: str) -> Dict[str, Any]:
        """íŠ¹ì • Flowì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            # "files"ëŠ” ì˜ëª»ëœ Flow IDì´ë¯€ë¡œ ì¦‰ì‹œ None ë°˜í™˜
            if flow_id == "files":
                print(f"ğŸš¨ ì˜ëª»ëœ Flow ID ìš”ì²­: {flow_id}")
                import traceback
                print("=== í˜¸ì¶œ ìŠ¤íƒ ì¶”ì  ===")
                traceback.print_stack()
                print("======================")
                return None
                
            # ì‹¤ì œ Flow ëª©ë¡ì—ì„œ í•´ë‹¹ Flow ì°¾ê¸°
            flows = await self.get_flows()
            target_flow = None
            
            for flow in flows:
                if flow["flow_id"] == flow_id:
                    target_flow = flow
                    break
            
            if not target_flow:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return None
            
            # Flow íŒŒì¼ ê²½ë¡œ (ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©)
            flows_dir = os.path.join(settings.BASE_DIR, "langflow", "flows")
            if not os.path.exists(flows_dir):
                flows_dir = "langflow/flows"
            
            flow_file = os.path.join(flows_dir, target_flow["original_filename"])
            
            if not os.path.exists(flow_file):
                print(f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_file}")
                return None
            
            # Flow íŒŒì¼ ì½ê¸°
            with open(flow_file, 'r', encoding='utf-8') as f:
                flow_data = json.load(f)
            
            # Flow ìƒì„¸ ì •ë³´ êµ¬ì„±
            flow_details = {
                "flow_id": flow_id,
                "name": flow_data.get("name", flow_id),
                "description": flow_data.get("description", f"{flow_id} Flow"),
                "created_at": flow_data.get("created_at", "2024-01-01T00:00:00Z"),
                "updated_at": flow_data.get("updated_at", "2024-01-01T00:00:00Z"),
                "is_active": flow_data.get("is_active", True),
                "components": flow_data.get("components", []),
                "execution_stats": flow_data.get("execution_stats", {
                    "total_executions": 0,
                    "last_execution": None,
                    "success_rate": 0.0
                }),
                "flow_data": flow_data
            }
            
            return flow_details
            
        except Exception as e:
            print(f"Flow ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def test_flow(self, flow_id: str, test_data: Dict[str, Any]) -> Dict[str, Any]:
        """Flowë¥¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # Flow ì‹¤í–‰
            result = await self.flow_service.execute_flow(flow_id, test_data)
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ í¬í•¨
            result["flow_id"] = flow_id
            result["test_data"] = test_data
            
            return result
            
        except Exception as e:
            print(f"Flow í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "flow_id": flow_id,
                "status": "error",
                "error": str(e),
                "test_data": test_data
            }
    
    async def toggle_flow_status(self, flow_id: str) -> bool:
        """Flowì˜ í™œì„±/ë¹„í™œì„± ìƒíƒœë¥¼ í† ê¸€í•©ë‹ˆë‹¤."""
        try:
            # "files"ëŠ” ì˜ëª»ëœ Flow IDì´ë¯€ë¡œ ì¦‰ì‹œ False ë°˜í™˜
            if flow_id == "files":
                print(f"ì˜ëª»ëœ Flow ID ìš”ì²­: {flow_id} - ë¬´ì‹œë¨")
                return False
                
            # ì‹¤ì œ Flow ëª©ë¡ì—ì„œ í•´ë‹¹ Flow ì°¾ê¸°
            flows = await self.get_flows()
            target_flow = None
            
            for flow in flows:
                if flow["flow_id"] == flow_id:
                    target_flow = flow
                    break
            
            if not target_flow:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return False
            
            # Flow íŒŒì¼ ê²½ë¡œ (ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©)
            flows_dir = os.path.join(settings.BASE_DIR, "langflow", "flows")
            if not os.path.exists(flows_dir):
                flows_dir = "langflow/flows"
            
            flow_file = os.path.join(flows_dir, target_flow["original_filename"])
            
            if not os.path.exists(flow_file):
                print(f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_file}")
                return False
            
            # Flow íŒŒì¼ ì½ê¸°
            with open(flow_file, 'r', encoding='utf-8') as f:
                flow_data = json.load(f)
            
            # ìƒíƒœ í† ê¸€
            current_status = flow_data.get("is_active", True)
            flow_data["is_active"] = not current_status
            
            # íŒŒì¼ì— ë‹¤ì‹œ ì €ì¥
            with open(flow_file, 'w', encoding='utf-8') as f:
                json.dump(flow_data, f, indent=2, ensure_ascii=False)
            
            print(f"Flow ìƒíƒœ ë³€ê²½: {flow_id} -> {'í™œì„±' if flow_data['is_active'] else 'ë¹„í™œì„±'}")
            return True
            
        except Exception as e:
            print(f"Flow ìƒíƒœ í† ê¸€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def set_default_vectorization_flow(self, flow_id: str) -> bool:
        """Flowë¥¼ ê¸°ë³¸ ë²¡í„°í™” Flowë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            # "files"ëŠ” ì˜ëª»ëœ Flow IDì´ë¯€ë¡œ ì¦‰ì‹œ False ë°˜í™˜
            if flow_id == "files":
                print(f"ì˜ëª»ëœ Flow ID ìš”ì²­: {flow_id} - ë¬´ì‹œë¨")
                return False
                
            # ì‹¤ì œ Flow ëª©ë¡ì—ì„œ í•´ë‹¹ Flow ì°¾ê¸°
            flows = await self.get_flows()
            target_flow = None
            
            for flow in flows:
                if flow["flow_id"] == flow_id:
                    target_flow = flow
                    break
            
            if not target_flow:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return False
            
            # ëª¨ë“  Flow íŒŒì¼ì—ì„œ ê¸°ë³¸ ë²¡í„°í™” ì„¤ì •ì„ í•´ì œ
            flows_dir = os.path.join(settings.BASE_DIR, "langflow", "flows")
            if not os.path.exists(flows_dir):
                flows_dir = "langflow/flows"
            
            for flow in flows:
                flow_file = os.path.join(flows_dir, flow["original_filename"])
                if os.path.exists(flow_file):
                    try:
                        with open(flow_file, 'r', encoding='utf-8') as f:
                            flow_data = json.load(f)
                        
                        # ê¸°ë³¸ ë²¡í„°í™” ì„¤ì • í•´ì œ
                        if "is_default_vectorization" in flow_data:
                            flow_data["is_default_vectorization"] = False
                            
                        with open(flow_file, 'w', encoding='utf-8') as f:
                            json.dump(flow_data, f, indent=2, ensure_ascii=False)
                    except Exception as e:
                        print(f"Flow íŒŒì¼ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ ({flow['flow_id']}): {str(e)}")
            
            # ëŒ€ìƒ Flowë¥¼ ê¸°ë³¸ ë²¡í„°í™” Flowë¡œ ì„¤ì •
            target_flow_file = os.path.join(flows_dir, target_flow["original_filename"])
            if os.path.exists(target_flow_file):
                with open(target_flow_file, 'r', encoding='utf-8') as f:
                    flow_data = json.load(f)
                
                flow_data["is_default_vectorization"] = True
                
                with open(target_flow_file, 'w', encoding='utf-8') as f:
                    json.dump(flow_data, f, indent=2, ensure_ascii=False)
            
            # ì„¤ì • íŒŒì¼ì— ê¸°ë³¸ Flow ID ì €ì¥ (ì„ì‹œë¡œ íŒŒì¼ì— ì €ì¥)
            config_file = os.path.join(settings.BASE_DIR, "langflow", "config.json")
            config_dir = os.path.dirname(config_file)
            os.makedirs(config_dir, exist_ok=True)
            
            config_data = {
                "default_vectorization_flow_id": flow_id,
                "updated_at": datetime.now().isoformat()
            }
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            
            print(f"ê¸°ë³¸ ë²¡í„°í™” Flow ì„¤ì •: {flow_id} ({target_flow['name']})")
            return True
            
        except Exception as e:
            print(f"ê¸°ë³¸ ë²¡í„°í™” Flow ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def set_search_flow(self, flow_id: str) -> bool:
        """ê²€ìƒ‰ Flow ì„¤ì •"""
        try:
            # Flow ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            flows = await self.get_flows()
            flow_exists = any(flow["flow_id"] == flow_id for flow in flows)
            
            if not flow_exists:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return False
            
            # ì„¤ì • íŒŒì¼ì— ê²€ìƒ‰ Flow ID ì €ì¥
            config_file = os.path.join(settings.DATA_DIR, "langflow_config.json")
            config = {}
            
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                except Exception as e:
                    print(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    config = {}
            
            config["default_search_flow_id"] = flow_id
            
            try:
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
                print(f"ê²€ìƒ‰ Flowê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {flow_id}")
                return True
            except Exception as e:
                print(f"ì„¤ì • íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                return False
                
        except Exception as e:
            print(f"ê²€ìƒ‰ Flow ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            return False

    async def delete_flow(self, flow_id: str) -> bool:
        """Flow ì‚­ì œ"""
        try:
            flows = await self.get_flows() # Get all flows to find the original filename
            target_flow = next((flow for flow in flows if flow["flow_id"] == flow_id), None)

            if not target_flow:
                print(f"Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}")
                return False

            flow_file_path = os.path.join(settings.BASE_DIR, "langflow", "flows", target_flow["original_filename"])
            
            # Flow ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(flow_file_path):
                print(f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_file_path}")
                return False
            
            # Flowê°€ ê¸°ë³¸ ë²¡í„°í™” Flowì¸ì§€ í™•ì¸
            config_file = os.path.join(settings.DATA_DIR, "langflow_config.json")
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    
                    # ê¸°ë³¸ ë²¡í„°í™” Flowì¸ ê²½ìš° ì„¤ì • ì œê±°
                    if config.get("default_vectorization_flow_id") == flow_id:
                        config["default_vectorization_flow_id"] = None
                        with open(config_file, 'w', encoding='utf-8') as f:
                            json.dump(config, f, ensure_ascii=False, indent=2)
                        print(f"ê¸°ë³¸ ë²¡í„°í™” Flow ì„¤ì •ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤: {flow_id}")
                    
                    # ê¸°ë³¸ ê²€ìƒ‰ Flowì¸ ê²½ìš° ì„¤ì • ì œê±°
                    if config.get("default_search_flow_id") == flow_id:
                        config["default_search_flow_id"] = None
                        with open(config_file, 'w', encoding='utf-8') as f:
                            json.dump(config, f, ensure_ascii=False, indent=2)
                        print(f"ê¸°ë³¸ ê²€ìƒ‰ Flow ì„¤ì •ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤: {flow_id}")
                        
                except Exception as e:
                    print(f"ì„¤ì • íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            
            # Flow íŒŒì¼ ì‚­ì œ
            try:
                os.remove(flow_file_path)
                print(f"Flow íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {flow_file_path}")
                return True
            except Exception as e:
                print(f"Flow íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
                return False
                
        except Exception as e:
            print(f"Flow ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def vectorize_files_by_category(self, category_id: str, vectorization_flow_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ë²¡í„°í™”í•©ë‹ˆë‹¤."""
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
            files = await self.file_service.list_files(category_id=category_id)
            
            results = []
            for file_info in files:
                if not file_info.vectorized:  # ì•„ì§ ë²¡í„°í™”ë˜ì§€ ì•Šì€ íŒŒì¼ë§Œ
                    result = await self.process_file_with_flow(
                        file_info.file_id, 
                        vectorization_flow_id,
                        file_info
                    )
                    results.append(result)
            
            return results
            
        except Exception as e:
            print(f"ì¹´í…Œê³ ë¦¬ë³„ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def search_with_flow(self, query: str, search_flow_id: str, category_ids: List[str] = None, top_k: int = 10) -> Dict[str, Any]:
        """Langflowë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # LangFlow Flow ëŒ€ì‹  ì§ì ‘ ChromaDB ê²€ìƒ‰ ì‚¬ìš©
            from .vector_service import VectorService
            
            print(f"ì§ì ‘ ChromaDB ê²€ìƒ‰ ì‹¤í–‰: {search_flow_id}")
            print(f"ì¿¼ë¦¬: {query}")
            print(f"ì¹´í…Œê³ ë¦¬: {category_ids}")
            print(f"ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {top_k}ê°œ")
            
            vector_service = VectorService()
            search_results = await vector_service.search_similar_chunks(
                query=query,
                top_k=top_k,
                category_ids=category_ids
            )
            
            print(f"ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ë°œê²¬")
            
            if search_results:
                return {
                    "status": "success",
                    "results": search_results,
                    "flow_id": search_flow_id,
                    "search_method": "direct_chromadb",
                    "query": query,
                    "category_ids": category_ids,
                    "top_k": top_k
                }
            else:
                return {
                    "status": "success",
                    "results": [],
                    "flow_id": search_flow_id,
                    "search_method": "direct_chromadb",
                    "query": query,
                    "category_ids": category_ids,
                    "top_k": top_k
                }
                
        except Exception as e:
            print(f"ì§ì ‘ ChromaDB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "query": query,
                "status": "error",
                "error": str(e),
                "response": "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
    
    async def execute_flow_with_llm(self, flow_id: str, prompt: str, system_message: str = None, model_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """LangFlowë¥¼ í†µí•´ LLM ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            print(f"LangFlow LLM ì‹¤í–‰: {flow_id}")
            
            # Flow JSON íŒŒì¼ ë¡œë“œ
            flow_file_path = os.path.join(settings.BASE_DIR, "langflow", "flows", f"{flow_id.replace('_', ' ').title()}.json")
            
            # íŒŒì¼ëª… ë³€í™˜ (ì˜ˆ: vector_store_search -> Vector Store Search.json)
            if not os.path.exists(flow_file_path):
                flow_file_path = os.path.join(settings.BASE_DIR, "langflow", "flows", "Vector Store Search.json")
            
            if not os.path.exists(flow_file_path):
                print(f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_file_path}")
                return {
                    "status": "error",
                    "error": f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}",
                    "response": "Flow ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # Flow íŒŒì¼ ë¡œë“œ
            
            # Flow JSONì—ì„œ LLM ì„¤ì • ì¶”ì¶œ
            with open(flow_file_path, 'r', encoding='utf-8') as f:
                flow_data = json.load(f)
            
            # LanguageModelComponent ë…¸ë“œ ì°¾ê¸°
            llm_node = None
            nodes = flow_data.get("data", {}).get("nodes", [])
            
            for node in nodes:
                if "LanguageModelComponent" in node.get("id", ""):
                    llm_node = node
                    break
            
            if not llm_node:
                print("LanguageModelComponent ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {
                    "status": "error", 
                    "error": "LanguageModelComponent ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "response": "LLM ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # LLM ì„¤ì • ì¶”ì¶œ
            llm_data = llm_node.get("data", {})
            node_data = llm_data.get("node", {})
            template_data = node_data.get("template", {})
            
            api_key = None
            # ì‚¬ìš©ì ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©ì ì„¤ì •ì„ ì™„ì „ ìš°ì„ ì‹œ
            if model_config and model_config.get("llm", {}).get("model"):
                provider = model_config["llm"].get("provider", "openai").title()
                model_name = model_config["llm"]["model"]
                temperature = model_config["llm"].get("temperature", 0.7)
                api_key = model_config["llm"].get("api_key")
                print(f"ì‚¬ìš©ì ì„¤ì • ì‚¬ìš©: {provider} {model_name} (temp: {temperature})")
            elif template_data:
                # ì‚¬ìš©ì ì„¤ì •ì´ ì—†ìœ¼ë©´ Flow ì„¤ì • ì‚¬ìš©
                # Provider ì¶”ì¶œ
                provider_data = template_data.get("provider", {})
                provider = provider_data.get("value", "OpenAI")
                
                # Model Name ì¶”ì¶œ  
                model_name_data = template_data.get("model_name", {})
                model_name = model_name_data.get("value", "gpt-4o-mini")
                
                # Temperature ì¶”ì¶œ
                temperature_data = template_data.get("temperature", {})
                temperature = temperature_data.get("value", 0.7)
                print(f"Flow ì„¤ì • ì‚¬ìš©: {provider} {model_name} (temp: {temperature})")
            else:
                # ì‚¬ìš©ì ì„¤ì •ë„ Flow ì„¤ì •ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                if model_config and model_config.get("llm", {}).get("model"):
                    provider = model_config["llm"].get("provider", "openai").title()
                    model_name = model_config["llm"]["model"]
                    temperature = model_config["llm"].get("temperature", 0.7)
                    api_key = model_config["llm"].get("api_key")
                    print(f"ì‚¬ìš©ì ì„¤ì • ê¸°ë³¸ê°’ ì‚¬ìš©: {provider} {model_name} (temp: {temperature})")
                else:
                    provider = "OpenAI"
                    model_name = "gpt-4o-mini"
                    temperature = 0.7
                    print(f"ì‹œìŠ¤í…œ ê¸°ë³¸ê°’ ì‚¬ìš©: {provider} {model_name} (temp: {temperature})")
            
            print(f"LLM ì‹¤í–‰: {provider} {model_name} (temp: {temperature})")
            
            # Providerë³„ LLM ì‹¤í–‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            if provider.lower() == "google":
                response_text = await self._execute_google_llm(model_name, prompt, system_message, temperature, api_key=api_key)
            elif provider.lower() == "openai":
                response_text = await self._execute_openai_llm(model_name, prompt, system_message, temperature, api_key=api_key)
            elif provider.lower() == "anthropic":
                response_text = await self._execute_anthropic_llm(model_name, prompt, system_message, temperature, api_key=api_key)
            else:
                print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” Provider: {provider}")
                return {
                    "status": "error",
                    "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” Provider: {provider}",
                    "response": f"{provider} ProviderëŠ” ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                }
            
            print(f"=== LLM ì‹¤í–‰ ì™„ë£Œ ===")
            print(f"ì‘ë‹µ ê¸¸ì´: {len(response_text)} ê¸€ì")
            
            return {
                "status": "success",
                "response": response_text,
                "provider": provider,
                "model": model_name,
                "flow_id": flow_id
            }
            
        except Exception as e:
            print(f"LangFlow LLM ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "status": "error",
                "error": str(e),
                "response": f"LLM ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    async def _execute_google_llm(self, model_name: str, prompt: str, system_message: str = None, temperature: float = 0.1, api_key: str = None) -> str:
        """Google Gemini ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # langchain_google_genai ì‚¬ìš©
            from langchain_google_genai import ChatGoogleGenerativeAI
            from ..core.config import settings
            
            # Google API í‚¤ í™•ì¸
            final_api_key = api_key or settings.GOOGLE_API_KEY or settings.GEMINI_API_KEY
            if not final_api_key:
                raise ValueError("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_API_KEY ë˜ëŠ” GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            
            # ChatGoogleGenerativeAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            llm = ChatGoogleGenerativeAI(
                model=model_name,
                temperature=temperature,
                google_api_key=final_api_key
            )
            
            # ë©”ì‹œì§€ êµ¬ì„± - Geminiì˜ ê²½ìš° system ë©”ì‹œì§€ë¥¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
            if system_message:
                # system ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì§ì ‘ í¬í•¨
                combined_prompt = f"ë‹¤ìŒì€ ì‹œìŠ¤í…œ ì§€ì¹¨ì…ë‹ˆë‹¤:\n{system_message}\n\nì‚¬ìš©ì ì§ˆë¬¸:\n{prompt}"
                messages = [("human", combined_prompt)]
            else:
                messages = [("human", prompt)]
            
            start_time = time.time()
            
            # LLM ì‹¤í–‰
            response = llm.invoke(messages)
            
            api_time = time.time() - start_time
            print(f"Gemini ì‘ë‹µ ì™„ë£Œ ({api_time:.2f}ì´ˆ)")
            
            # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
            content = response.content if hasattr(response, 'content') else str(response)
            
            # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
            if not content or content.strip() == "":
                if hasattr(response, 'response_metadata'):
                    metadata = response.response_metadata
                    prompt_feedback = metadata.get('prompt_feedback', {})
                    
                    # ì•ˆì „ í•„í„°ê°€ ì‘ë™í•œ ê²½ìš°
                    if prompt_feedback.get('block_reason'):
                        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì•ˆì „ ì •ì±…ìœ¼ë¡œ ì¸í•´ ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
                    
                    # ê¸°íƒ€ ì´ìœ ë¡œ ë¹ˆ ì‘ë‹µì¸ ê²½ìš°
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. Geminiì—ì„œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                else:
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. Geminiì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            
            return content
            
        except Exception as e:
            print(f"Google Gemini ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            raise e
    
    async def _execute_openai_llm(self, model_name: str, prompt: str, system_message: str = None, temperature: float = 0.1, api_key: str = None) -> str:
        """OpenAI ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # OpenAI ëª¨ë¸ ì‹¤í–‰
            
            from langchain_openai import ChatOpenAI
            from ..core.config import settings
            
            final_api_key = api_key or settings.OPENAI_API_KEY
            if not final_api_key:
                raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            llm = ChatOpenAI(
                model=model_name,
                temperature=temperature,
                openai_api_key=final_api_key
            )
            
            messages = []
            if system_message:
                messages.append(("system", system_message))
            messages.append(("human", prompt))
            
            start_time = time.time()
            response = llm.invoke(messages)
            api_time = time.time() - start_time
            print(f"OpenAI ì‘ë‹µ ì™„ë£Œ ({api_time:.2f}ì´ˆ)")
            
            return response.content
            
        except Exception as e:
            print(f"OpenAI ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise e
    
    async def _execute_anthropic_llm(self, model_name: str, prompt: str, system_message: str = None, temperature: float = 0.1, api_key: str = None) -> str:
        """Anthropic Claude ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # Anthropic Claude ëª¨ë¸ ì‹¤í–‰
            
            from langchain_anthropic import ChatAnthropic
            from ..core.config import settings
            
            final_api_key = api_key or settings.ANTHROPIC_API_KEY
            if not final_api_key:
                raise ValueError("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            llm = ChatAnthropic(
                model=model_name,
                temperature=temperature,
                anthropic_api_key=final_api_key
            )
            
            messages = []
            if system_message:
                messages.append(("system", system_message))
            messages.append(("human", prompt))
            
            start_time = time.time()
            response = llm.invoke(messages)
            api_time = time.time() - start_time
            print(f"Anthropic ì‘ë‹µ ì™„ë£Œ ({api_time:.2f}ì´ˆ)")
            
            return response.content
            
        except Exception as e:
            print(f"Anthropic ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise e
    
    async def get_available_flows_by_type(self, flow_type: str) -> List[Dict[str, Any]]:
        """íƒ€ì…ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ Flow ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            all_flows = await self.get_flows()
            
            # Flow ì´ë¦„ì´ë‚˜ ì„¤ëª…ì—ì„œ íƒ€ì… í•„í„°ë§ (ê°„ë‹¨í•œ êµ¬í˜„)
            type_keywords = {
                "vectorization": ["ë²¡í„°", "vector", "embed", "chunk"],
                "search": ["ê²€ìƒ‰", "search", "rag", "retrieval"],
                "chat": ["ì±„íŒ…", "chat", "conversation"]
            }
            
            keywords = type_keywords.get(flow_type, [])
            filtered_flows = []
            
            for flow in all_flows:
                flow_dict = {
                    "flow_id": flow["flow_id"],
                    "name": flow["name"],
                    "created_at": flow["created_at"]
                }
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                if any(keyword in flow["name"].lower() for keyword in keywords):
                    flow_dict["recommended"] = True
                else:
                    flow_dict["recommended"] = False
                
                filtered_flows.append(flow_dict)
            
            return filtered_flows
            
        except Exception as e:
            print(f"Flow ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def get_vectorization_status(self) -> Dict[str, Any]:
        """ì „ì²´ ë²¡í„°í™” ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            all_files = await self.file_service.list_files()
            
            total_files = len(all_files)
            vectorized_files = len([f for f in all_files if f.vectorized])
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_stats = {}
            for file_info in all_files:
                category_name = file_info.category_name or "ë¯¸ë¶„ë¥˜"
                if category_name not in category_stats:
                    category_stats[category_name] = {"total": 0, "vectorized": 0}
                
                category_stats[category_name]["total"] += 1
                if file_info.vectorized:
                    category_stats[category_name]["vectorized"] += 1
            
            return {
                "total_files": total_files,
                "vectorized_files": vectorized_files,
                "vectorization_rate": round(vectorized_files / total_files * 100, 1) if total_files > 0 else 0,
                "category_stats": category_stats
            }
            
        except Exception as e:
            print(f"ë²¡í„°í™” ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "total_files": 0,
                "vectorized_files": 0,
                "vectorization_rate": 0,
                "category_stats": {}
            }
    
    async def execute_multimodal_flow_with_llm(self, flow_id: str, prompt: str, images: List[str], system_message: str = None, model_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ LangFlowë¥¼ í†µí•´ ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ì—¬ LLM ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            print(f"ë©€í‹°ëª¨ë‹¬ LangFlow ì‹¤í–‰: {flow_id} (ì´ë¯¸ì§€ {len(images)}ê°œ)")
            
            # Flow JSON íŒŒì¼ ë¡œë“œ
            flow_file_path = os.path.join(settings.BASE_DIR, "langflow", "flows", f"{flow_id.replace('_', ' ').title()}.json")
            
            # íŒŒì¼ëª… ë³€í™˜ (ì˜ˆ: vector_store_search -> Vector Store Search.json)
            if not os.path.exists(flow_file_path):
                flow_file_path = os.path.join(settings.BASE_DIR, "langflow", "flows", "Vector Store Search.json")
            
            if not os.path.exists(flow_file_path):
                print(f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_file_path}")
                return {
                    "status": "error",
                    "error": f"Flow íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {flow_id}",
                    "response": "Flow ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ë©€í‹°ëª¨ë‹¬ Flow íŒŒì¼ ë¡œë“œ
            
            # Flow JSONì—ì„œ LLM ì„¤ì • ì¶”ì¶œ
            with open(flow_file_path, 'r', encoding='utf-8') as f:
                flow_data = json.load(f)
            
            # LanguageModelComponent ë…¸ë“œ ì°¾ê¸°
            llm_node = None
            nodes = flow_data.get("data", {}).get("nodes", [])
            
            for node in nodes:
                if "LanguageModelComponent" in node.get("id", ""):
                    llm_node = node
                    break
            
            if not llm_node:
                print("LanguageModelComponent ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {
                    "status": "error", 
                    "error": "LanguageModelComponent ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "response": "LLM ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # LLM ì„¤ì • ì¶”ì¶œ
            llm_data = llm_node.get("data", {})
            node_data = llm_data.get("node", {})
            template_data = node_data.get("template", {})
            
            api_key = None
            # ë©€í‹°ëª¨ë‹¬: ì‚¬ìš©ì ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©ì ì„¤ì •ì„ ì™„ì „ ìš°ì„ ì‹œ
            if model_config and model_config.get("llm", {}).get("model"):
                provider = model_config["llm"].get("provider", "openai").title()
                model_name = model_config["llm"]["model"]
                temperature = model_config["llm"].get("temperature", 0.7)
                api_key = model_config["llm"].get("api_key")
                print(f"ë©€í‹°ëª¨ë‹¬ ì‚¬ìš©ì ì„¤ì • ì‚¬ìš©: {provider} {model_name} (temp: {temperature})")
            elif template_data:
                # ì‚¬ìš©ì ì„¤ì •ì´ ì—†ìœ¼ë©´ Flow ì„¤ì • ì‚¬ìš©
                provider_data = template_data.get("provider", {})
                provider = provider_data.get("value", "OpenAI")
                
                model_name_data = template_data.get("model_name", {})
                model_name = model_name_data.get("value", "gpt-4o")  # ë©€í‹°ëª¨ë‹¬ ê¸°ë³¸ê°’
                
                temperature_data = template_data.get("temperature", {})
                temperature = temperature_data.get("value", 0.7)
                print(f"ë©€í‹°ëª¨ë‹¬ Flow ì„¤ì • ì‚¬ìš©: {provider} {model_name} (temp: {temperature})")
            else:
                # ë©€í‹°ëª¨ë‹¬: ì‚¬ìš©ì ì„¤ì •ë„ Flow ì„¤ì •ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                if model_config and model_config.get("llm", {}).get("model"):
                    provider = model_config["llm"].get("provider", "openai").title()
                    model_name = model_config["llm"]["model"]
                    temperature = model_config["llm"].get("temperature", 0.7)
                    api_key = model_config["llm"].get("api_key")
                    print(f"ë©€í‹°ëª¨ë‹¬ ì‚¬ìš©ì ì„¤ì • ê¸°ë³¸ê°’ ì‚¬ìš©: {provider} {model_name} (temp: {temperature})")
                else:
                    # ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ ê¸°ë³¸ ì„¤ì •
                    provider = "OpenAI"
                    model_name = "gpt-4o"  # ë©€í‹°ëª¨ë‹¬ì„ ìœ„í•œ ê¸°ë³¸ê°’
                    temperature = 0.7
                    print(f"ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ ê¸°ë³¸ê°’ ì‚¬ìš©: {provider} {model_name} (temp: {temperature})")
            
            print(f"ë©€í‹°ëª¨ë‹¬ LLM ì‹¤í–‰: {provider} {model_name} (temp: {temperature}, ì´ë¯¸ì§€: {len(images)}ê°œ)")
            
            # Providerë³„ ë©€í‹°ëª¨ë‹¬ LLM ì‹¤í–‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            if provider.lower() == "google":
                response_text = await self._execute_google_multimodal_llm(model_name, prompt, images, system_message, temperature, api_key=api_key)
            elif provider.lower() == "openai":
                response_text = await self._execute_openai_multimodal_llm(model_name, prompt, images, system_message, temperature, api_key=api_key)
            elif provider.lower() == "anthropic":
                response_text = await self._execute_anthropic_multimodal_llm(model_name, prompt, images, system_message, temperature, api_key=api_key)
            else:
                print(f"ë©€í‹°ëª¨ë‹¬ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” Provider: {provider}")
                # Fallback: í…ìŠ¤íŠ¸ ì „ìš© ì²˜ë¦¬
                return await self.execute_flow_with_llm(flow_id, prompt, system_message, model_config)
            
            print(f"=== ë©€í‹°ëª¨ë‹¬ LLM ì‹¤í–‰ ì™„ë£Œ ===")
            print(f"ì‘ë‹µ ê¸¸ì´: {len(response_text)} ê¸€ì")
            
            return {
                "status": "success",
                "response": response_text,
                "provider": provider,
                "model": model_name,
                "flow_id": flow_id,
                "multimodal": True
            }
            
        except Exception as e:
            print(f"ë©€í‹°ëª¨ë‹¬ LangFlow LLM ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "status": "error",
                "error": str(e),
                "response": f"ë©€í‹°ëª¨ë‹¬ LLM ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    async def _execute_google_multimodal_llm(self, model_name: str, prompt: str, images: List[str], system_message: str = None, temperature: float = 0.1, api_key: str = None) -> str:
        """Google Gemini ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            print(f"=== Google Gemini ë©€í‹°ëª¨ë‹¬ ì‹¤í–‰ ===")
            print(f"ëª¨ë¸: {model_name}")
            print(f"ì´ë¯¸ì§€ ìˆ˜: {len(images)}")
            
            # langchain_google_genai ì‚¬ìš©
            from langchain_google_genai import ChatGoogleGenerativeAI
            from langchain_core.messages import HumanMessage
            from ..core.config import settings
            import base64
            
            # Google API í‚¤ í™•ì¸
            final_api_key = api_key or settings.GOOGLE_API_KEY or settings.GEMINI_API_KEY
            if not final_api_key:
                raise ValueError("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_API_KEY ë˜ëŠ” GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            
            print(f"Google API í‚¤ í™•ì¸ë¨: {final_api_key[:10]}...")
            
            # ChatGoogleGenerativeAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            llm = ChatGoogleGenerativeAI(
                model=model_name,
                temperature=temperature,
                google_api_key=final_api_key
            )
            
            # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ êµ¬ì„±
            message_content = []
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ì— í¬í•¨
            if system_message:
                combined_text = f"ë‹¤ìŒì€ ì‹œìŠ¤í…œ ì§€ì¹¨ì…ë‹ˆë‹¤:\n{system_message}\n\nì‚¬ìš©ì ì§ˆë¬¸:\n{prompt}"
            else:
                combined_text = prompt
            
            # í…ìŠ¤íŠ¸ ì¶”ê°€
            message_content.append({
                "type": "text",
                "text": combined_text
            })
            
            # ì´ë¯¸ì§€ë“¤ ì¶”ê°€
            for i, image_data in enumerate(images):
                try:
                    # Base64 ë°ì´í„°ì—ì„œ MIME íƒ€ì…ê³¼ ë°ì´í„° ë¶„ë¦¬
                    if image_data.startswith('data:'):
                        # data:image/jpeg;base64,/9j/4AAQ... í˜•íƒœ
                        mime_type = image_data.split(';')[0].split(':')[1]
                        base64_data = image_data.split(',')[1]
                    else:
                        # ìˆœìˆ˜ base64 ë°ì´í„°ì¸ ê²½ìš°
                        mime_type = "image/jpeg"  # ê¸°ë³¸ê°’
                        base64_data = image_data
                    
                    print(f"ì´ë¯¸ì§€ {i+1}: {mime_type}, ë°ì´í„° ê¸¸ì´: {len(base64_data)}")
                    
                    message_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime_type};base64,{base64_data}"
                        }
                    })
                    
                except Exception as img_error:
                    print(f"ì´ë¯¸ì§€ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(img_error)}")
                    continue
            
            # HumanMessage ìƒì„±
            message = HumanMessage(content=message_content)
            
            print(f"Gemini ë©€í‹°ëª¨ë‹¬ API í˜¸ì¶œ ì‹œì‘...")
            print(f"ë©”ì‹œì§€ ì»¨í…ì¸  í•­ëª© ìˆ˜: {len(message_content)}")
            start_time = time.time()
            
            # LLM ì‹¤í–‰
            response = llm.invoke([message])
            
            api_time = time.time() - start_time
            print(f"Gemini ë©€í‹°ëª¨ë‹¬ API í˜¸ì¶œ ì™„ë£Œ: {api_time:.2f}ì´ˆ")
            
            # ì‘ë‹µ ë‚´ìš© ë””ë²„ê¹…
            print(f"=== Gemini ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ë””ë²„ê¹… ===")
            print(f"ì‘ë‹µ íƒ€ì…: {type(response)}")
            print(f"ì‘ë‹µ content: '{response.content}'")
            print(f"ì‘ë‹µ content ê¸¸ì´: {len(response.content) if hasattr(response, 'content') else 'N/A'}")
            
            # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
            content = response.content if hasattr(response, 'content') else str(response)
            
            if not content or content.strip() == "":
                return "ì£„ì†¡í•©ë‹ˆë‹¤. Geminiì—ì„œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë¶„ì„í•œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            
            print(f"ìµœì¢… ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ: '{content[:100]}...' ({len(content)} ê¸€ì)")
            return content
            
        except Exception as e:
            print(f"Google Gemini ë©€í‹°ëª¨ë‹¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            # ë©€í‹°ëª¨ë‹¬ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì „ìš©ìœ¼ë¡œ í´ë°±
            print("ë©€í‹°ëª¨ë‹¬ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ì „ìš©ìœ¼ë¡œ í´ë°± ì‹œë„...")
            try:
                return await self._execute_google_llm(model_name, f"{prompt}\n\n(ì°¸ê³ : ì´ë¯¸ì§€ {len(images)}ê°œê°€ í¬í•¨ë˜ì—ˆìœ¼ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ì–´ í…ìŠ¤íŠ¸ë§Œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.)", system_message, temperature)
            except:
                raise e
    
    async def _execute_openai_multimodal_llm(self, model_name: str, prompt: str, images: List[str], system_message: str = None, temperature: float = 0.1, api_key: str = None) -> str:
        """OpenAI ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            print(f"=== OpenAI ë©€í‹°ëª¨ë‹¬ ì‹¤í–‰ ===")
            print(f"ëª¨ë¸: {model_name}")
            print(f"ì´ë¯¸ì§€ ìˆ˜: {len(images)}")
            
            from langchain_openai import ChatOpenAI
            from langchain_core.messages import HumanMessage, SystemMessage
            from ..core.config import settings
            
            final_api_key = api_key or settings.OPENAI_API_KEY
            if not final_api_key:
                raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # OpenAI ë¹„ì „ ëª¨ë¸ í™•ì¸ (gpt-4o, gpt-4-turbo, gpt-4-vision-preview ë“±)
            if "gpt-4" not in model_name.lower() and "vision" not in model_name.lower():
                print(f"ê²½ê³ : {model_name}ì€ ë¹„ì „ ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                # gpt-4oë¡œ ê°•ì œ ë³€ê²½
                model_name = "gpt-4o"
                print(f"ëª¨ë¸ì„ {model_name}ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")
            
            llm = ChatOpenAI(
                model=model_name,
                temperature=temperature,
                openai_api_key=final_api_key
            )
            
            messages = []
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
            if system_message:
                messages.append(SystemMessage(content=system_message))
            
            # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ êµ¬ì„±
            message_content = []
            
            # í…ìŠ¤íŠ¸ ì¶”ê°€
            message_content.append({
                "type": "text",
                "text": prompt
            })
            
            # ì´ë¯¸ì§€ë“¤ ì¶”ê°€
            for i, image_data in enumerate(images):
                try:
                    # Base64 ë°ì´í„° ì²˜ë¦¬
                    if image_data.startswith('data:'):
                        # data:image/jpeg;base64,/9j/4AAQ... í˜•íƒœ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        image_url = image_data
                    else:
                        # ìˆœìˆ˜ base64 ë°ì´í„°ì¸ ê²½ìš° data URL í˜•íƒœë¡œ ë³€ê²½
                        image_url = f"data:image/jpeg;base64,{image_data}"
                    
                    print(f"ì´ë¯¸ì§€ {i+1}: ë°ì´í„° ê¸¸ì´: {len(image_data)}")
                    
                    message_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": image_url,
                            "detail": "auto"  # OpenAI íŠ¹ìœ ì˜ detail íŒŒë¼ë¯¸í„°
                        }
                    })
                    
                except Exception as img_error:
                    print(f"ì´ë¯¸ì§€ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(img_error)}")
                    continue
            
            # HumanMessage ìƒì„±
            messages.append(HumanMessage(content=message_content))
            
            print(f"OpenAI ë©€í‹°ëª¨ë‹¬ API í˜¸ì¶œ ì‹œì‘...")
            print(f"ë©”ì‹œì§€ ì»¨í…ì¸  í•­ëª© ìˆ˜: {len(message_content)}")
            start_time = time.time()
            
            # LLM ì‹¤í–‰
            response = llm.invoke(messages)
            
            api_time = time.time() - start_time
            print(f"OpenAI ë©€í‹°ëª¨ë‹¬ API í˜¸ì¶œ ì™„ë£Œ: {api_time:.2f}ì´ˆ")
            
            print(f"OpenAI ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ê¸¸ì´: {len(response.content)} ê¸€ì")
            return response.content
            
        except Exception as e:
            print(f"OpenAI ë©€í‹°ëª¨ë‹¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            # ë©€í‹°ëª¨ë‹¬ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì „ìš©ìœ¼ë¡œ í´ë°±
            print("ë©€í‹°ëª¨ë‹¬ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ì „ìš©ìœ¼ë¡œ í´ë°± ì‹œë„...")
            try:
                return await self._execute_openai_llm(model_name, f"{prompt}\n\n(ì°¸ê³ : ì´ë¯¸ì§€ {len(images)}ê°œê°€ í¬í•¨ë˜ì—ˆìœ¼ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ì–´ í…ìŠ¤íŠ¸ë§Œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.)", system_message, temperature)
            except:
                raise e
    
    async def _execute_anthropic_multimodal_llm(self, model_name: str, prompt: str, images: List[str], system_message: str = None, temperature: float = 0.1, api_key: str = None) -> str:
        """Anthropic Claude ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            print(f"=== Anthropic Claude ë©€í‹°ëª¨ë‹¬ ì‹¤í–‰ ===")
            print(f"ëª¨ë¸: {model_name}")
            print(f"ì´ë¯¸ì§€ ìˆ˜: {len(images)}")
            
            from langchain_anthropic import ChatAnthropic
            from langchain_core.messages import HumanMessage, SystemMessage
            from ..core.config import settings
            import base64
            
            final_api_key = api_key or settings.ANTHROPIC_API_KEY
            if not final_api_key:
                raise ValueError("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Claude ëª¨ë¸ì—ì„œ ë¹„ì „ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
            vision_models = ["claude-3", "claude-3.5"]
            supports_vision = any(vm in model_name.lower() for vm in vision_models)
            
            if not supports_vision:
                print(f"ê²½ê³ : {model_name}ì€ ë¹„ì „ ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                # Claude 3.5 Sonnetìœ¼ë¡œ ê°•ì œ ë³€ê²½
                model_name = "claude-3-5-sonnet-20241022"
                print(f"ëª¨ë¸ì„ {model_name}ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")
            
            llm = ChatAnthropic(
                model=model_name,
                temperature=temperature,
                anthropic_api_key=final_api_key
            )
            
            messages = []
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
            if system_message:
                messages.append(SystemMessage(content=system_message))
            
            # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ êµ¬ì„±
            message_content = []
            
            # í…ìŠ¤íŠ¸ ì¶”ê°€
            message_content.append({
                "type": "text",
                "text": prompt
            })
            
            # ì´ë¯¸ì§€ë“¤ ì¶”ê°€
            for i, image_data in enumerate(images):
                try:
                    # Base64 ë°ì´í„°ì—ì„œ MIME íƒ€ì…ê³¼ ë°ì´í„° ë¶„ë¦¬
                    if image_data.startswith('data:'):
                        # data:image/jpeg;base64,/9j/4AAQ... í˜•íƒœ
                        mime_type = image_data.split(';')[0].split(':')[1]
                        base64_data = image_data.split(',')[1]
                    else:
                        # ìˆœìˆ˜ base64 ë°ì´í„°ì¸ ê²½ìš°
                        mime_type = "image/jpeg"  # ê¸°ë³¸ê°’
                        base64_data = image_data
                    
                    print(f"ì´ë¯¸ì§€ {i+1}: {mime_type}, ë°ì´í„° ê¸¸ì´: {len(base64_data)}")
                    
                    # Anthropic API í˜•ì‹ì— ë§ê²Œ ì´ë¯¸ì§€ ë°ì´í„° êµ¬ì„±
                    message_content.append({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": mime_type,
                            "data": base64_data
                        }
                    })
                    
                except Exception as img_error:
                    print(f"ì´ë¯¸ì§€ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(img_error)}")
                    continue
            
            # HumanMessage ìƒì„±
            messages.append(HumanMessage(content=message_content))
            
            print(f"Anthropic ë©€í‹°ëª¨ë‹¬ API í˜¸ì¶œ ì‹œì‘...")
            print(f"ë©”ì‹œì§€ ì»¨í…ì¸  í•­ëª© ìˆ˜: {len(message_content)}")
            start_time = time.time()
            
            # LLM ì‹¤í–‰
            response = llm.invoke(messages)
            
            api_time = time.time() - start_time
            print(f"Anthropic ë©€í‹°ëª¨ë‹¬ API í˜¸ì¶œ ì™„ë£Œ: {api_time:.2f}ì´ˆ")
            
            print(f"Anthropic ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ê¸¸ì´: {len(response.content)} ê¸€ì")
            return response.content
            
        except Exception as e:
            print(f"Anthropic Claude ë©€í‹°ëª¨ë‹¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            # ë©€í‹°ëª¨ë‹¬ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì „ìš©ìœ¼ë¡œ í´ë°±
            print("ë©€í‹°ëª¨ë‹¬ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ì „ìš©ìœ¼ë¡œ í´ë°± ì‹œë„...")
            try:
                return await self._execute_anthropic_llm(model_name, f"{prompt}\n\n(ì°¸ê³ : ì´ë¯¸ì§€ {len(images)}ê°œê°€ í¬í•¨ë˜ì—ˆìœ¼ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ì–´ í…ìŠ¤íŠ¸ë§Œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.)", system_message, temperature)
            except:
                raise e