"""
PRD3: AI ê¸°ë°˜ ì²­í‚¹ ì„œë¹„ìŠ¤
LLMì—ê²Œ ë¬¸ì„œë¥¼ ë³´ë‚´ì„œ JSON í˜•íƒœë¡œ ì²­í‚¹ ê¸°ì¤€ì„ ë°›ì•„ ì§€ëŠ¥í˜• ì²­í‚¹ ìˆ˜í–‰
"""

import os
import json
import re
import uuid
import time
import logging
import asyncio
import base64
from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
from enum import Enum

from .chunking_service import ChunkProposal, ChunkingRules, QualityWarning, ChunkQualityIssue
from .settings_service import settings_service
from ..core.logger import get_console_logger

logger = get_console_logger()


class AIProvider(str, Enum):
    """AI ì œê³µì—…ì²´"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    UPSTAGE = "upstage"
    GOOGLE = "google"


@dataclass
class AIChunkingOptions:
    """AI ì²­í‚¹ ì˜µì…˜"""
    provider: AIProvider = AIProvider.OPENAI
    model: str = "gpt-4o-mini"
    max_tokens: int = 800
    min_tokens: int = 200
    overlap_tokens: int = 80
    respect_headings: bool = True
    snap_to_sentence: bool = True
    hard_sentence_max_tokens: int = 400
    temperature: float = 0.1
    max_retries: int = 2
    use_multimodal: bool = False
    pdf_file_path: Optional[str] = None


@dataclass
class AIChunkItem:
    """AIê°€ ì œì•ˆí•œ ì²­í¬ ì•„ì´í…œ"""
    order: int
    text: str
    heading_path: Optional[List[str]] = None
    reasoning: Optional[str] = None  # AIì˜ ì²­í‚¹ ì´ìœ 


class AIChunkingService:
    """AI ê¸°ë°˜ ì²­í‚¹ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.logger = get_console_logger()
        # í•˜ë“œì½”ë”©ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œê±° - ëª¨ë¸ í”„ë¡œí•„ì—ì„œ ê´€ë¦¬
    
    def _convert_pdf_to_images(self, pdf_path: str, max_pages: int = 10) -> List[str]:
        """PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ base64 ì¸ì½”ë”©ëœ ë¬¸ìì—´ ëª©ë¡ ë°˜í™˜"""
        try:
            import fitz  # PyMuPDF
            
            if not os.path.exists(pdf_path):
                raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
            
            doc = fitz.open(pdf_path)
            images = []
            
            # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì œí•œ (í† í° ë° ë¹„ìš© ì œí•œ)
            num_pages = min(len(doc), max_pages)
            
            for page_num in range(num_pages):
                page = doc[page_num]
                
                # ê³ í•´ìƒë„ë¡œ ë Œë”ë§ (DPI 150)
                mat = fitz.Matrix(150/72, 150/72)  # 150 DPI
                pix = page.get_pixmap(matrix=mat)
                
                # PNG ë°”ì´íŠ¸ë¡œ ë³€í™˜
                img_data = pix.tobytes("png")
                
                # base64 ì¸ì½”ë”©
                img_base64 = base64.b64encode(img_data).decode('utf-8')
                images.append(img_base64)
                
                self.logger.info(f"PDF í˜ì´ì§€ {page_num + 1}/{num_pages} ë³€í™˜ ì™„ë£Œ")
            
            doc.close()
            
            self.logger.info(f"PDF ë³€í™˜ ì™„ë£Œ: {len(images)}ê°œ í˜ì´ì§€")
            return images
            
        except ImportError:
            raise RuntimeError("PyMuPDF (fitz) íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install PyMuPDF'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        except Exception as e:
            self.logger.error(f"PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"PDF ë³€í™˜ ì‹¤íŒ¨: {e}")

    def _get_user_prompt(self, text: str, options: AIChunkingOptions) -> str:
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""Analyze this document and create optimal chunks for RAG retrieval.

CONSTRAINTS:
- max_tokens per chunk: {options.max_tokens}
- min_tokens per chunk: {options.min_tokens}
- overlap_tokens between chunks: {options.overlap_tokens}
- respect_headings: {options.respect_headings}
- snap_to_sentence: {options.snap_to_sentence}
- hard_sentence_max_tokens: {options.hard_sentence_max_tokens}

If a single sentence exceeds hard_sentence_max_tokens, split it safely.
Add overlap by including the tail of previous chunk in the next chunk.
Focus on semantic coherence and retrieval quality.

DOCUMENT:
\"\"\"
{text[:20000]}  # í† í° ì œí•œì„ ìœ„í•´ ì²˜ìŒ 20k ë¬¸ìë§Œ
\"\"\""""

    def _get_multimodal_user_prompt(self, text: str, options: AIChunkingOptions) -> str:
        """ë©€í‹°ëª¨ë‹¬ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""Analyze both the visual document pages (images) and the extracted text below to create optimal chunks for RAG retrieval.

MULTIMODAL ANALYSIS INSTRUCTIONS:
- Examine the visual layout, structure, and formatting from the images
- Cross-reference with the extracted text for accurate content understanding
- Identify visual elements like headers, sections, tables, figures, and their hierarchy
- Preserve semantic relationships between visual and textual elements
- Consider page boundaries and visual flow when creating chunks

CONSTRAINTS:
- max_tokens per chunk: {options.max_tokens}
- min_tokens per chunk: {options.min_tokens}
- overlap_tokens between chunks: {options.overlap_tokens}
- respect_headings: {options.respect_headings}
- snap_to_sentence: {options.snap_to_sentence}
- hard_sentence_max_tokens: {options.hard_sentence_max_tokens}

Use the visual context to enhance chunking decisions. If images show clear structural divisions, respect them.
Focus on creating chunks that maintain both visual and semantic coherence for optimal retrieval.

EXTRACTED TEXT:
\"\"\"
{text[:20000]}  # í† í° ì œí•œì„ ìœ„í•´ ì²˜ìŒ 20k ë¬¸ìë§Œ
\"\"\""""

    async def _call_llm(self, provider: AIProvider, model: str, system_prompt: str, user_prompt: str, temperature: float = 0.1, api_key: str = None, images: Optional[List[str]] = None) -> str:
        """LLM API í˜¸ì¶œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)"""
        try:
            if provider == AIProvider.OPENAI:
                return await self._call_openai(model, system_prompt, user_prompt, temperature, api_key, images)
            elif provider == AIProvider.ANTHROPIC:
                return await self._call_anthropic(model, system_prompt, user_prompt, temperature, api_key, images)
            elif provider == AIProvider.UPSTAGE:
                return await self._call_upstage(model, system_prompt, user_prompt, temperature, api_key, images)
            elif provider == AIProvider.GOOGLE:
                return await self._call_google(model, system_prompt, user_prompt, temperature, api_key, images)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ì œê³µì—…ì²´: {provider}")
        except Exception as e:
            self.logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨ ({provider}): {e}")
            raise

    async def _call_openai(self, model: str, system_prompt: str, user_prompt: str, temperature: float, api_key: str = None, images: Optional[List[str]] = None) -> str:
        """OpenAI API í˜¸ì¶œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)"""
        try:
            from openai import AsyncOpenAI
            
            # API í‚¤ ì‚¬ìš© (ëª¨ë¸ í”„ë¡œí•„ì—ì„œ ì „ë‹¬ëœ í‚¤ ìš°ì„  ì‚¬ìš©)
            if not api_key:
                # í´ë°±: ê¸°ì¡´ ì„¤ì • ë°©ì‹
                system_settings = settings_service.get_section_settings("system")
                api_key = system_settings.get("openai_api_key") or os.getenv("OPENAI_API_KEY")
            
            if not api_key:
                raise RuntimeError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ í”„ë¡œí•„ì— API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì‹œìŠ¤í…œ ì„¤ì •ì—ì„œ openai_api_keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            
            client = AsyncOpenAI(api_key=api_key)
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": system_prompt}]
            
            if images and len(images) > 0:
                # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)
                content = []
                
                # ì´ë¯¸ì§€ë“¤ ì¶”ê°€
                for i, img_base64 in enumerate(images[:5]):  # ìµœëŒ€ 5ê°œ ì´ë¯¸ì§€ë¡œ ì œí•œ
                    content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{img_base64}",
                            "detail": "high"  # ê³ í•´ìƒë„ ë¶„ì„
                        }
                    })
                    self.logger.info(f"ì´ë¯¸ì§€ {i+1}/{len(images[:5])} ì¶”ê°€ë¨")
                
                # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                content.append({
                    "type": "text",
                    "text": user_prompt
                })
                
                messages.append({
                    "role": "user", 
                    "content": content
                })
            else:
                # í…ìŠ¤íŠ¸ë§Œ
                messages.append({"role": "user", "content": user_prompt})
            
            response = await client.chat.completions.create(
                model=model,
                temperature=temperature,
                messages=messages,
                max_tokens=4000
            )
            
            return response.choices[0].message.content or ""
            
        except Exception as e:
            self.logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    async def _call_anthropic(self, model: str, system_prompt: str, user_prompt: str, temperature: float, api_key: str = None, images: Optional[List[str]] = None) -> str:
        """Anthropic (Claude) API í˜¸ì¶œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)"""
        try:
            import anthropic
            
            # API í‚¤ ì‚¬ìš© (ëª¨ë¸ í”„ë¡œí•„ì—ì„œ ì „ë‹¬ëœ í‚¤ ìš°ì„  ì‚¬ìš©)
            if not api_key:
                # í´ë°±: ê¸°ì¡´ ì„¤ì • ë°©ì‹
                system_settings = settings_service.get_section_settings("system")
                api_key = system_settings.get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")
            
            if not api_key:
                raise RuntimeError("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ í”„ë¡œí•„ì— API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì‹œìŠ¤í…œ ì„¤ì •ì—ì„œ anthropic_api_keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            
            client = anthropic.AsyncAnthropic(api_key=api_key)
            
            if images and len(images) > 0:
                # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ êµ¬ì„±
                content = []
                
                # ì´ë¯¸ì§€ë“¤ ì¶”ê°€ (ClaudeëŠ” ìµœëŒ€ 20ê°œ ì´ë¯¸ì§€ ì§€ì›)
                for i, img_base64 in enumerate(images[:10]):  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
                    content.append({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": "image/png",
                            "data": img_base64
                        }
                    })
                    self.logger.info(f"Claude ì´ë¯¸ì§€ {i+1}/{len(images[:10])} ì¶”ê°€ë¨")
                
                # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                content.append({
                    "type": "text",
                    "text": user_prompt
                })
                
                response = await client.messages.create(
                    model=model,
                    max_tokens=4000,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": content}
                    ]
                )
            else:
                # í…ìŠ¤íŠ¸ë§Œ
                response = await client.messages.create(
                    model=model,
                    max_tokens=4000,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ]
                )
            
            return response.content[0].text
            
        except Exception as e:
            self.logger.error(f"Anthropic API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Anthropic API í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    async def _call_upstage(self, model: str, system_prompt: str, user_prompt: str, temperature: float, api_key: str = None, images: Optional[List[str]] = None) -> str:
        """Upstage API í˜¸ì¶œ (ë©€í‹°ëª¨ë‹¬ ì œí•œì  ì§€ì›)"""
        try:
            import httpx
            
            # ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ê²½ê³ 
            if images and len(images) > 0:
                self.logger.warning("Upstage Solar ëª¨ë¸ì€ í˜„ì¬ ì´ë¯¸ì§€ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
            
            # API í‚¤ ì‚¬ìš© (ëª¨ë¸ í”„ë¡œí•„ì—ì„œ ì „ë‹¬ëœ í‚¤ ìš°ì„  ì‚¬ìš©)
            if not api_key:
                # í´ë°±: ê¸°ì¡´ ì„¤ì • ë°©ì‹
                system_settings = settings_service.get_section_settings("system")
                api_key = system_settings.get("upstage_api_key") or os.getenv("UPSTAGE_API_KEY")
            
            if not api_key:
                raise RuntimeError("Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ í”„ë¡œí•„ì— API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì‹œìŠ¤í…œ ì„¤ì •ì—ì„œ upstage_api_keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "https://api.upstage.ai/v1/solar/chat/completions",
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": model,
                        "temperature": temperature,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ]
                    },
                    timeout=60.0
                )
                
                if response.status_code != 200:
                    raise RuntimeError(f"Upstage API HTTP {response.status_code}: {response.text}")
                
                data = response.json()
                return data["choices"][0]["message"]["content"]
                
        except Exception as e:
            self.logger.error(f"Upstage API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Upstage API í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    async def _call_google(self, model: str, system_prompt: str, user_prompt: str, temperature: float, api_key: str = None, images: Optional[List[str]] = None) -> str:
        """Google Gemini API í˜¸ì¶œ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)"""
        try:
            import google.generativeai as genai
            from PIL import Image
            import io
            
            # API í‚¤ ì‚¬ìš© (ëª¨ë¸ í”„ë¡œí•„ì—ì„œ ì „ë‹¬ëœ í‚¤ ìš°ì„  ì‚¬ìš©)
            if not api_key:
                # í´ë°±: ê¸°ì¡´ ì„¤ì • ë°©ì‹
                system_settings = settings_service.get_section_settings("system")
                api_key = system_settings.get("google_api_key") or os.getenv("GOOGLE_API_KEY")
            
            if not api_key:
                raise RuntimeError("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ í”„ë¡œí•„ì— API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì‹œìŠ¤í…œ ì„¤ì •ì—ì„œ google_api_keyë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            
            # Gemini API ì„¤ì •
            genai.configure(api_key=api_key)
            model_instance = genai.GenerativeModel(model)
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê²°í•©
            combined_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            if images and len(images) > 0:
                # ë©€í‹°ëª¨ë‹¬ ì»¨í…ì¸  êµ¬ì„±
                content_parts = [combined_prompt]
                
                # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¶”ê°€
                for i, img_base64 in enumerate(images[:8]):  # GeminiëŠ” ìµœëŒ€ 20ê°œ ì´ë¯¸ì§€ ì§€ì›, ë¹„ìš© ê³ ë ¤í•´ì„œ 8ê°œë¡œ ì œí•œ
                    try:
                        # base64ë¥¼ PIL Imageë¡œ ë³€í™˜
                        img_bytes = base64.b64decode(img_base64)
                        img = Image.open(io.BytesIO(img_bytes))
                        
                        content_parts.append(img)
                        self.logger.info(f"Gemini ì´ë¯¸ì§€ {i+1}/{len(images[:8])} ì¶”ê°€ë¨")
                    except Exception as img_error:
                        self.logger.warning(f"ì´ë¯¸ì§€ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {img_error}")
                        continue
                
                response = await model_instance.generate_content_async(
                    content_parts,
                    generation_config=genai.types.GenerationConfig(
                        temperature=temperature,
                        max_output_tokens=8192,  # ë” ê¸´ ì‘ë‹µ í—ˆìš©
                    )
                )
            else:
                # í…ìŠ¤íŠ¸ë§Œ
                response = await model_instance.generate_content_async(
                    combined_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=temperature,
                        max_output_tokens=8192,  # ë” ê¸´ ì‘ë‹µ í—ˆìš©
                    )
                )
            
            # ì‘ë‹µ ì•ˆì „ ì²˜ë¦¬
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                self.logger.info(f"Gemini ì‘ë‹µ ìƒíƒœ: finish_reason={candidate.finish_reason}")
                
                # finish_reason í™•ì¸
                if candidate.finish_reason == 1:  # STOP
                    if hasattr(candidate, 'content') and candidate.content and candidate.content.parts:
                        # partsê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_parts.append(part.text)
                        if text_parts:
                            full_text = ''.join(text_parts)
                            self.logger.warning(f"Gemini ì‘ë‹µì´ ì˜ë ¸ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸¸ì´: {len(full_text)} ë¬¸ì")
                            return full_text
                    
                    # partsê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°
                    self.logger.error("Gemini ì‘ë‹µì´ ì™„ì „íˆ ë¹„ì–´ìˆìŠµë‹ˆë‹¤ - í† í° ì œí•œ ì´ˆê³¼ ê°€ëŠ¥ì„±")
                    raise RuntimeError("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì‘ë‹µì´ í† í° ì œí•œì„ ì´ˆê³¼í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                # ì •ìƒ ì‘ë‹µ ì²˜ë¦¬
                return response.text if response.text else ""
            
            # ì‘ë‹µ ìì²´ê°€ ì—†ëŠ” ê²½ìš°
            self.logger.error("Gemini APIì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤")
            raise RuntimeError("Gemini API ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            
        except ImportError as ie:
            if "PIL" in str(ie):
                raise RuntimeError("ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ Pillow íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install Pillow'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            raise
        except Exception as e:
            self.logger.error(f"Google API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Google API í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    def _safe_json_parse(self, text: str) -> Dict[str, Any]:
        """JSON ì•ˆì „ íŒŒì‹± (LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ)"""
        try:
            # ë¨¼ì € ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„
            return json.loads(text.strip())
        except json.JSONDecodeError:
            # JSON ë¸”ë¡ ì°¾ê¸° ì‹œë„
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except json.JSONDecodeError:
                    pass
            
            # { ... } íŒ¨í„´ ì°¾ê¸°
            brace_match = re.search(r'\{.*\}', text, re.DOTALL)
            if brace_match:
                try:
                    return json.loads(brace_match.group())
                except json.JSONDecodeError:
                    pass
            
            raise ValueError(f"ìœ íš¨í•œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {text[:200]}...")

    def _validate_and_fix_chunks(self, chunks_data: List[Dict], options: AIChunkingOptions) -> List[AIChunkItem]:
        """ì²­í¬ ë°ì´í„° ê²€ì¦ ë° ìˆ˜ì •"""
        validated_chunks = []
        
        for i, chunk_data in enumerate(chunks_data):
            try:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                text = chunk_data.get("text", "").strip()
                if not text:
                    self.logger.warning(f"ì²­í¬ {i+1}: ë¹ˆ í…ìŠ¤íŠ¸, ê±´ë„ˆëœ€")
                    continue
                
                order = chunk_data.get("order", i + 1)
                heading_path = chunk_data.get("heading_path")
                reasoning = chunk_data.get("reasoning")
                
                # í—¤ë”© ê²½ë¡œ ì •ë¦¬
                if heading_path and isinstance(heading_path, list):
                    heading_path = [str(h).strip() for h in heading_path if str(h).strip()]
                    if not heading_path:
                        heading_path = None
                else:
                    heading_path = None
                
                validated_chunks.append(AIChunkItem(
                    order=order,
                    text=text,
                    heading_path=heading_path,
                    reasoning=reasoning
                ))
                
            except Exception as e:
                self.logger.warning(f"ì²­í¬ {i+1} ê²€ì¦ ì‹¤íŒ¨: {e}")
                continue
        
        # ìˆœì„œ ì¬ì •ë ¬
        for i, chunk in enumerate(validated_chunks, 1):
            chunk.order = i
        
        return validated_chunks

    def _apply_overlap(self, chunks: List[AIChunkItem], overlap_tokens: int) -> List[AIChunkItem]:
        """ì˜¤ë²„ë© ì ìš©"""
        if overlap_tokens <= 0 or len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = []
        
        for i, chunk in enumerate(chunks):
            if i == 0:
                overlapped_chunks.append(chunk)
                continue
            
            # ì´ì „ ì²­í¬ì˜ ëë¶€ë¶„ ì¶”ì¶œ
            prev_chunk = chunks[i - 1]
            prev_words = prev_chunk.text.split()
            
            # ëŒ€ëµì ì¸ ì˜¤ë²„ë© ë‹¨ì–´ ìˆ˜ ê³„ì‚°
            overlap_words = min(overlap_tokens, len(prev_words) // 2)
            if overlap_words > 0:
                overlap_text = " ".join(prev_words[-overlap_words:])
                
                # í˜„ì¬ ì²­í¬ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                if not chunk.text.startswith(overlap_text[:50]):
                    chunk.text = f"{overlap_text} {chunk.text}"
            
            overlapped_chunks.append(chunk)
        
        return overlapped_chunks

    def _convert_to_chunk_proposals(self, ai_chunks: List[AIChunkItem]) -> List[ChunkProposal]:
        """AI ì²­í¬ë¥¼ ChunkProposalë¡œ ë³€í™˜"""
        proposals = []
        
        for ai_chunk in ai_chunks:
            # í† í° ìˆ˜ ê³„ì‚°
            from .chunking_service import TokenCounter
            token_counter = TokenCounter()
            token_estimate = token_counter.count_tokens(ai_chunk.text)
            
            # ê¸°ë³¸ í’ˆì§ˆ ê²½ê³  ìƒì„±
            warnings = []
            if len(ai_chunk.text.strip()) < 10:
                warnings.append(QualityWarning(
                    issue_type=ChunkQualityIssue.TOO_SHORT,
                    severity="warning",
                    message="ì²­í¬ê°€ ë§¤ìš° ì§§ìŠµë‹ˆë‹¤",
                    suggestion="ë‹¤ë¥¸ ì²­í¬ì™€ ë³‘í•©ì„ ê³ ë ¤í•˜ì„¸ìš”"
                ))
            
            proposal = ChunkProposal(
                chunk_id=str(uuid.uuid4()),
                order=ai_chunk.order,
                text=ai_chunk.text,
                token_estimate=token_estimate,
                heading_path=ai_chunk.heading_path,
                quality_warnings=warnings,
                created_at=datetime.now().isoformat()
            )
            
            # AI ì¶”ë¡  ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì €ì¥
            if hasattr(proposal, 'metadata'):
                proposal.metadata = proposal.metadata or {}
                proposal.metadata['ai_reasoning'] = ai_chunk.reasoning
                proposal.metadata['ai_generated'] = True
            
            proposals.append(proposal)
        
        return proposals

    async def propose_chunks_with_ai(self, text: str, options: AIChunkingOptions, api_key: str = None, system_message: str = None) -> List[ChunkProposal]:
        """AIë¥¼ ì‚¬ìš©í•œ ì²­í‚¹ ì œì•ˆ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)"""
        mode = "ë©€í‹°ëª¨ë‹¬" if options.use_multimodal else "í…ìŠ¤íŠ¸"
        self.logger.info(f"ğŸš€ AI ì²­í‚¹ ì‹œì‘ ({mode}) - ì œê³µì—…ì²´: {options.provider}, ëª¨ë¸: {options.model}")
        self.logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text):,} ë¬¸ì, ì˜µì…˜: max_tokens={options.max_tokens}, min_tokens={options.min_tokens}")
        
        start_time = time.time()
        
        # í”„ë¡¬í”„íŠ¸ ì„ íƒ (ë©€í‹°ëª¨ë‹¬ vs í…ìŠ¤íŠ¸)
        self.logger.info(f"ğŸ“‹ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘... (ëª¨ë“œ: {mode})")
        if options.use_multimodal:
            user_prompt = self._get_multimodal_user_prompt(text, options)
        else:
            user_prompt = self._get_user_prompt(text, options)
        
        # PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ (ë©€í‹°ëª¨ë‹¬ ëª¨ë“œ)
        images = None
        if options.use_multimodal and options.pdf_file_path:
            try:
                self.logger.info(f"ğŸ–¼ï¸ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì‹œì‘: {options.pdf_file_path}")
                images = self._convert_pdf_to_images(options.pdf_file_path)
                self.logger.info(f"âœ… PDF ë³€í™˜ ì™„ë£Œ: {len(images)}ê°œ ì´ë¯¸ì§€ ìƒì„±ë¨")
            except Exception as img_error:
                self.logger.error(f"âŒ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {img_error}")
                # ë©€í‹°ëª¨ë‹¬ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ í´ë°±
                self.logger.warning("âš ï¸ ë©€í‹°ëª¨ë‹¬ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ í´ë°±")
                user_prompt = self._get_user_prompt(text, options)
                images = None
        
        # ì¬ì‹œë„ ë¡œì§
        last_error = None
        self.logger.info(f"ğŸ”„ AI ì²­í‚¹ ì¬ì‹œë„ ì„¤ì •: ìµœëŒ€ {options.max_retries + 1}íšŒ ì‹œë„")
        
        for attempt in range(options.max_retries + 1):
            try:
                self.logger.info(f"ğŸ¤– LLM í˜¸ì¶œ ì‹œë„ {attempt + 1}/{options.max_retries + 1}")
                
                # LLM í˜¸ì¶œ (API í‚¤, ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì´ë¯¸ì§€ í¬í•¨)
                if not system_message:
                    raise RuntimeError("AI ì²­í‚¹ì„ ìœ„í•œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ í”„ë¡œí•„ì—ì„œ 'AI ì²­í‚¹ ì‹œìŠ¤í…œ ë©”ì‹œì§€'ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
                
                effective_system_prompt = system_message
                self.logger.info(f"ğŸ“¡ LLM API í˜¸ì¶œ ì¤‘... (ëª¨ë¸: {options.model}, ì˜¨ë„: {options.temperature})")
                
                response = await self._call_llm(
                    options.provider,
                    options.model,
                    effective_system_prompt,
                    user_prompt,
                    options.temperature,
                    api_key,
                    images  # ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ ì¶”ê°€
                )
                
                self.logger.info(f"âœ… LLM ì‘ë‹µ ë°›ìŒ (ê¸¸ì´: {len(response)} ë¬¸ì)")
                
                # JSON íŒŒì‹±
                self.logger.info("ğŸ” JSON ì‘ë‹µ íŒŒì‹± ì¤‘...")
                response_data = self._safe_json_parse(response)
                chunks_data = response_data.get("chunks", [])
                
                if not chunks_data:
                    raise ValueError("AIê°€ ì²­í¬ë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
                self.logger.info(f"ğŸ“Š AIê°€ {len(chunks_data)}ê°œ ì²­í¬ ì œì•ˆí•¨")
                
                # ì²­í¬ ê²€ì¦ ë° ë³€í™˜
                self.logger.info("âœ… ì²­í¬ ê²€ì¦ ë° ìˆ˜ì • ì¤‘...")
                ai_chunks = self._validate_and_fix_chunks(chunks_data, options)
                if not ai_chunks:
                    raise ValueError("ìœ íš¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                
                self.logger.info(f"ğŸ“ ê²€ì¦ í›„ {len(ai_chunks)}ê°œ ì²­í¬ ìœ íš¨í•¨")
                
                # ì˜¤ë²„ë© ì ìš©
                if options.overlap_tokens > 0:
                    self.logger.info(f"ğŸ”— ì˜¤ë²„ë© ì ìš© ì¤‘... ({options.overlap_tokens} í† í°)")
                    ai_chunks = self._apply_overlap(ai_chunks, options.overlap_tokens)
                
                # ChunkProposalë¡œ ë³€í™˜
                self.logger.info("ğŸ”„ ChunkProposal í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
                proposals = self._convert_to_chunk_proposals(ai_chunks)
                
                elapsed = time.time() - start_time
                self.logger.info(f"ğŸ‰ AI ì²­í‚¹ ì„±ê³µ! {len(proposals)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ, ì‹œë„: {attempt + 1}/{options.max_retries + 1})")
                return proposals
                
            except Exception as e:
                last_error = e
                self.logger.warning(f"âš ï¸ AI ì²­í‚¹ ì‹œë„ {attempt + 1}/{options.max_retries + 1} ì‹¤íŒ¨: {e}")
                
                if attempt < options.max_retries:
                    self.logger.info(f"â° {1.0}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(1.0)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    continue
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°±
        elapsed = time.time() - start_time
        self.logger.error(f"âŒ AI ì²­í‚¹ ì™„ì „ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ): {last_error}")
        raise RuntimeError(f"AI ì²­í‚¹ ì‹¤íŒ¨: {last_error}")

    async def propose_chunks_with_fallback(self, text: str, options: AIChunkingOptions, api_key: str = None, system_message: str = None) -> Tuple[List[ChunkProposal], bool]:
        """AI ì²­í‚¹ + í´ë°± (ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜)"""
        try:
            # 1ì°¨: AI ì²­í‚¹ ì‹œë„
            proposals = await self.propose_chunks_with_ai(text, options, api_key, system_message)
            return proposals, False  # AI ì„±ê³µ
            
        except Exception as ai_error:
            self.logger.warning(f"AI ì²­í‚¹ ì‹¤íŒ¨, í´ë°± ëª¨ë“œë¡œ ì „í™˜: {ai_error}")
            
            # 2ì°¨: ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ í´ë°±
            try:
                from .chunking_service import chunking_service, ChunkingRules
                
                rules = ChunkingRules(
                    max_tokens=options.max_tokens,
                    min_tokens=options.min_tokens,
                    overlap_tokens=options.overlap_tokens,
                    respect_headings=options.respect_headings,
                    snap_to_sentence=options.snap_to_sentence,
                    hard_sentence_max_tokens=options.hard_sentence_max_tokens
                )
                
                proposals = chunking_service.propose_chunks(text, rules, use_hierarchical=True)
                
                # í´ë°± í‘œì‹œë¥¼ ìœ„í•œ ê²½ê³  ì¶”ê°€
                for proposal in proposals:
                    if not proposal.quality_warnings:
                        proposal.quality_warnings = []
                    proposal.quality_warnings.append(QualityWarning(
                        issue_type=ChunkQualityIssue.NO_CONTENT,  # ì„ì‹œ íƒ€ì…
                        severity="info",
                        message="AI ì²­í‚¹ ì‹¤íŒ¨ë¡œ ì•Œê³ ë¦¬ì¦˜ í´ë°± ì‚¬ìš©",
                        suggestion="AI ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”"
                    ))
                
                self.logger.info(f"í´ë°± ì²­í‚¹ ì„±ê³µ - {len(proposals)}ê°œ ì²­í¬ ìƒì„±")
                return proposals, True  # í´ë°± ì„±ê³µ
                
            except Exception as fallback_error:
                self.logger.error(f"í´ë°± ì²­í‚¹ë„ ì‹¤íŒ¨: {fallback_error}")
                raise RuntimeError(f"AI ì²­í‚¹ ë° í´ë°± ëª¨ë‘ ì‹¤íŒ¨: AI={ai_error}, Fallback={fallback_error}")


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
ai_chunking_service = AIChunkingService()