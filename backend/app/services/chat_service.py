import time
import os
import json
from typing import Dict, Any, List
from ..models.schemas import ChatRequest, ChatResponse
from ..core.config import settings
from .file_service import FileService
from .langflow_service import LangflowService
from .persona_service import PersonaService
from .system_settings_service import SystemSettingsService
from .model_settings_service import get_current_model_config
from ..utils.image_utils import extract_image_path_from_chunk, is_image_chunk, create_vision_image_content
import openai
from datetime import datetime

class ChatService:
    def __init__(self):
        self.file_service = FileService()
        self.langflow_service = LangflowService()
        self.persona_service = PersonaService()
        self.system_settings_service = SystemSettingsService()
    
    async def _get_llm_client(self):
        """í˜„ì¬ ëª¨ë¸ ì„¤ì •ì— ë”°ë¼ LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            model_config = await get_current_model_config()
            llm_config = model_config.get("llm", {})
            
            provider = llm_config.get("provider", "openai")
            api_key = llm_config.get("api_key", "")
            
            if provider == "openai" and api_key:
                import openai
                client = openai.OpenAI(api_key=api_key)
                return client, provider
            elif provider == "anthropic" and api_key:
                try:
                    import anthropic
                    client = anthropic.Anthropic(api_key=api_key)
                    return client, provider
                except ImportError:
                    print("Anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install anthropicìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
                    return None, None
            elif provider == "google" and api_key:
                try:
                    import google.generativeai as genai
                    genai.configure(api_key=api_key)
                    return genai, provider
                except ImportError:
                    print("Google AI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-generativeaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
                    return None, None
            elif provider == "groq" and api_key:
                try:
                    import groq
                    client = groq.Groq(api_key=api_key)
                    return client, provider
                except ImportError:
                    print("Groq íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install groqë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
                    return None, None
            else:
                print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì—…ì²´ì´ê±°ë‚˜ API í‚¤ê°€ ì—†ìŒ: {provider}")
                return None, None
                
        except Exception as e:
            print(f"LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None, None
    
    async def _call_vision_llm(self, messages: List[Dict[str, Any]], images: List[str] = None) -> str:
        """Vision ëª¨ë¸ë¡œ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
        try:
            model_config = await get_current_model_config()
            llm_config = model_config.get("llm", {})
            
            llm_client, provider = await self._get_llm_client()
            if not llm_client:
                return "LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            model = llm_config.get("model", "gpt-4o")  # Vision ëª¨ë¸ ê¸°ë³¸ê°’
            temperature = llm_config.get("temperature", 0.7)
            max_tokens = llm_config.get("max_tokens", 4096)
            
            print(f"Vision LLM í˜¸ì¶œ: {provider} {model} (ì´ë¯¸ì§€: {len(images) if images else 0}ê°œ)")
            
            if provider == "openai":
                # OpenAI Vision ëª¨ë¸ ì§€ì›
                if images:
                    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— ì´ë¯¸ì§€ ì¶”ê°€
                    if messages and messages[-1]["role"] == "user":
                        content = [{"type": "text", "text": messages[-1]["content"]}]
                        
                        # ì´ë¯¸ì§€ ê²½ë¡œë“¤ì„ Vision ì½˜í…ì¸ ë¡œ ë³€í™˜
                        for image_path in images:
                            image_content = create_vision_image_content(image_path)
                            if image_content:
                                content.append(image_content)
                                print(f"ğŸ–¼ï¸ Vision ì´ë¯¸ì§€ ì¶”ê°€: {image_path}")
                        
                        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
                        messages[-1]["content"] = content
                
                response = llm_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content
            
            else:
                # ë‹¤ë¥¸ ì œê³µì—…ì²´ëŠ” ì¼ë°˜ LLMìœ¼ë¡œ í´ë°±
                print(f"âš ï¸ {provider}ëŠ” Vision ëª¨ë¸ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¼ë°˜ LLMìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                return await self._call_llm(messages)
                
        except Exception as e:
            print(f"Vision LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì¼ë°˜ LLMìœ¼ë¡œ í´ë°±
            return await self._call_llm(messages)
    
    async def _call_llm(self, messages: List[Dict[str, Any]]) -> str:
        """ì„¤ì •ëœ LLMìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
        try:
            model_config = await get_current_model_config()
            llm_config = model_config.get("llm", {})
            
            llm_client, provider = await self._get_llm_client()
            if not llm_client:
                return "LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            model = llm_config.get("model", "gpt-4o-mini")  # ì„¤ì •ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            temperature = llm_config.get("temperature", 0.7)
            max_tokens = llm_config.get("max_tokens", 4096)
            
            print(f"LLM í˜¸ì¶œ: {provider} {model} (temp: {temperature})")
            
            if provider == "openai":
                response = llm_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content
                
            elif provider == "anthropic":
                # Anthropic ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
                system_message = ""
                user_messages = []
                
                for msg in messages:
                    if msg["role"] == "system":
                        system_message = msg["content"]
                    else:
                        user_messages.append(msg)
                
                response = llm_client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    system=system_message,
                    messages=user_messages
                )
                return response.content[0].text
                
            elif provider == "google":
                # Google Gemini í˜¸ì¶œ
                model_instance = llm_client.GenerativeModel(model)
                
                # ë©”ì‹œì§€ë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ê²°í•©
                prompt_parts = []
                for msg in messages:
                    if msg["role"] == "system":
                        prompt_parts.append(f"System: {msg['content']}")
                    elif msg["role"] == "user":
                        prompt_parts.append(f"User: {msg['content']}")
                
                prompt = "\n\n".join(prompt_parts)
                
                response = model_instance.generate_content(
                    prompt,
                    generation_config={
                        "temperature": temperature,
                        "max_output_tokens": max_tokens,
                    }
                )
                return response.text
                
            elif provider == "groq":
                response = llm_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content
                
            else:
                return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì—…ì²´: {provider}"
                
        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def process_chat(self, request: ChatRequest) -> ChatResponse:
        """ì±„íŒ… ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        start_time = time.time()
        
        try:
            print(f"=== ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ ì‹œì‘ ===")
            print(f"ì‚¬ìš©ì ID: {request.user_id}")
            print(f"ë©”ì‹œì§€: {request.message}")
            print(f"ì¹´í…Œê³ ë¦¬ IDs: {request.category_ids}")
            print(f"Flow ID: {request.flow_id}")
            print(f"í˜ë¥´ì†Œë‚˜ ID: {request.persona_id}")
            print(f"ì‹œìŠ¤í…œ ë©”ì‹œì§€: {request.system_message}")
            print(f"ì²¨ë¶€ëœ ì´ë¯¸ì§€ ìˆ˜: {len(request.images) if request.images else 0}")
            
            # ìµœì¢… ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„± (í˜ë¥´ì†Œë‚˜ + ì„¤ì • ì¡°í•©)
            final_system_message = await self._build_system_message(request.system_message, request.persona_id)
            print(f"ìµœì¢… ì‹œìŠ¤í…œ ë©”ì‹œì§€: {final_system_message}")
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            if request.user_id:
                await self._save_chat_message(
                    user_id=request.user_id,
                    message=request.message,
                    role="user",
                    category_ids=request.category_ids
                )
            
            # ê¸°ë³¸ ê²€ìƒ‰ Flow ID í™•ì¸
            search_flow_id = request.flow_id or await self._get_default_search_flow()
            
            if search_flow_id:
                print(f"LangFlow ê²€ìƒ‰ Flow ì‚¬ìš©: {search_flow_id}")
                # LangFlow ê²€ìƒ‰ í”Œë¡œìš° ì‹¤í–‰
                langflow_result = await self.langflow_service.search_with_flow(
                    request.message,
                    search_flow_id,
                    request.category_ids,
                    top_k=request.top_k  # ìš”ì²­ì—ì„œ top_k ì„¤ì • ì‚¬ìš©
                )
                
                print(f"LangFlow ê²°ê³¼: {langflow_result}")
                
                # LangFlow ê²°ê³¼ì—ì„œ ì‘ë‹µê³¼ ì†ŒìŠ¤ ì¶”ì¶œ
                if langflow_result.get("status") == "success":
                    # ì§ì ‘ ChromaDB ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
                    search_results = langflow_result.get("results", [])
                    print(f"ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ë°œê²¬")
                    
                    if search_results:
                        # ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡° í™•ì¸ì„ ìœ„í•œ ë””ë²„ê·¸
                        print(f"ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡°: {search_results[0] if search_results else 'None'}")
                        
                        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        relevant_documents = []
                        for i, result in enumerate(search_results):
                            metadata = result.get("metadata", {})
                            filename = metadata.get("filename", "") or metadata.get("file_name", "") or metadata.get("source", "")
                            content = result.get("text", "") or result.get("content", "")
                            
                            doc = {
                                "file_id": metadata.get("file_id", ""),
                                "filename": filename,
                                "category_id": metadata.get("category_id", ""),
                                "category_name": metadata.get("category_name", ""),
                                "content": content,
                                "score": result.get("score", 1.0),
                                "distance": result.get("distance", 1.0)
                            }
                            
                            # ì´ë¯¸ì§€ ì²­í¬ ê°ì§€ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            if is_image_chunk(content):
                                image_path = extract_image_path_from_chunk(content)
                                if image_path:
                                    doc["image_path"] = image_path
                                    doc["is_image_chunk"] = True
                                    print(f"ğŸ–¼ï¸ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ì²­í¬ ë°œê²¬: {image_path}")
                                else:
                                    doc["is_image_chunk"] = False
                            else:
                                doc["is_image_chunk"] = False
                            
                            relevant_documents.append(doc)
                            
                            print(f"ë³€í™˜ëœ ë¬¸ì„œ {i+1}: file_id={doc['file_id']}, filename='{doc['filename']}', score={doc['score']:.3f}, distance={doc['distance']:.3f}, ì´ë¯¸ì§€={doc.get('is_image_chunk', False)}")
                        
                        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ê°€ ë¨¼ì €)
                        relevant_documents.sort(key=lambda x: x['score'], reverse=True)
                        print(f"ì ìˆ˜ìˆœ ì •ë ¬ í›„ ì²« 3ê°œ ë¬¸ì„œ:")
                        for i, doc in enumerate(relevant_documents[:3]):
                            print(f"  {i+1}ìœ„: {doc['filename']} (ì ìˆ˜: {doc['score']:.3f})")
                        
                        # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                        has_image_chunks = any(doc.get("is_image_chunk", False) for doc in relevant_documents)
                        
                        # LangFlowë¥¼ í†µí•´ ì‘ë‹µ ìƒì„± (Flow ê¸°ë°˜ ëª¨ë¸ ì‚¬ìš©)
                        if request.images and len(request.images) > 0:
                            # ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸
                            print(f"ë©€í‹°ëª¨ë‹¬ ëª¨ë“œ: ì´ë¯¸ì§€ {len(request.images)}ê°œì™€ í…ìŠ¤íŠ¸ ì²˜ë¦¬")
                            response_text = await self.generate_multimodal_response_with_flow(
                                request.message, 
                                request.images,
                                relevant_documents, 
                                final_system_message,
                                search_flow_id
                            )
                        elif has_image_chunks:
                            # ê²€ìƒ‰ ê²°ê³¼ì— ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ê²½ìš° Vision ëª¨ë¸ ì‚¬ìš©
                            print(f"ğŸ–¼ï¸ ê²€ìƒ‰ ê²°ê³¼ì— ì´ë¯¸ì§€ê°€ í¬í•¨ë˜ì–´ Vision ëª¨ë¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                            response_text = await self.generate_response_with_vision(
                                request.message, 
                                relevant_documents, 
                                final_system_message
                            )
                        else:
                            # ê¸°ì¡´ í…ìŠ¤íŠ¸ ì „ìš© ì²˜ë¦¬
                            response_text = await self.generate_response_with_flow(
                                request.message, 
                                relevant_documents, 
                                final_system_message,
                                search_flow_id
                            )
                    else:
                        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        response_text = "ì£„ì†¡í•©ë‹ˆë‹¤, ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        relevant_documents = []
                else:
                    print(f"LangFlow ì‹¤í–‰ ì‹¤íŒ¨: {langflow_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    # LangFlow ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                    relevant_documents = await self.search_documents(
                        request.message, 
                        request.category_ids,
                        request.categories
                    )
                    response_text = await self.generate_response_with_flow(
                        request.message, 
                        relevant_documents, 
                        final_system_message,
                        None  # fallbackì¼ ë•ŒëŠ” ê¸°ë³¸ flow ì‚¬ìš©
                    )
            else:
                print("ê²€ìƒ‰ Flowê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                # ê¸°ë³¸ ê²€ìƒ‰ (fallback)
                relevant_documents = await self.search_documents(
                    request.message, 
                    request.category_ids,
                    request.categories
                )
                response_text = await self.generate_response_with_flow(
                    request.message, 
                    relevant_documents, 
                    final_system_message,
                    None  # ê¸°ë³¸ flow ì‚¬ìš©
                )
            
            processing_time = time.time() - start_time
            print(f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            # ì†ŒìŠ¤ ë¬¸ì„œ: íŒŒì¼ ë‹¨ìœ„ë¡œ ì¤‘ë³µ ì œê±°(ë™ì¼ ë¬¸ì„œëŠ” 1ê°œë§Œ)
            print(f"ì¤‘ë³µ ì œê±° ì „ relevant_documents: {[doc.get('filename', 'NO_NAME') for doc in relevant_documents]}")
            sources_for_response = self._unique_sources(relevant_documents)
            print(f"ì¤‘ë³µ ì œê±° í›„ sources_for_response: {[src.get('filename', 'NO_NAME') for src in sources_for_response]}")

            # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì €ì¥
            if request.user_id:
                await self._save_chat_message(
                    user_id=request.user_id,
                    message=response_text,
                    role="assistant",
                    category_ids=request.category_ids,
                    sources=sources_for_response
                )
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ë‹¨ìˆœí™”)
            confidence = 0.7  # ê³ ì •ê°’ìœ¼ë¡œ ì„¤ì • (ì„ì‹œ)
            if relevant_documents:
                print(f"ë¬¸ì„œ {len(relevant_documents)}ê°œ ë°œê²¬, ì‹ ë¢°ë„: {confidence}")
            else:
                confidence = 0.3
                print(f"ë¬¸ì„œ ì—†ìŒ, ì‹ ë¢°ë„: {confidence}")
            
            print(f"ê³„ì‚°ëœ ì‹ ë¢°ë„: {confidence:.3f} (ì›ë³¸ ë¬¸ì„œ {len(relevant_documents)}ê°œ, ìœ ë‹ˆí¬ ì†ŒìŠ¤ {len(sources_for_response)}ê°œ)")
            
            return ChatResponse(
                response=response_text,
                sources=sources_for_response,
                confidence=confidence,
                processing_time=processing_time,
                categories=request.categories,
                flow_id=search_flow_id,
                user_id=request.user_id
            )
            
        except Exception as e:
            print(f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
            processing_time = time.time() - start_time
            return ChatResponse(
                response=f"ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                sources=[],
                confidence=0.0,
                processing_time=processing_time,
                categories=request.categories or [],
                flow_id=request.flow_id,
                user_id=request.user_id
            )
    
    async def execute_langflow_flow(self, flow_id: str, message: str, context: List[Dict[str, Any]] = None) -> str:
        """íŠ¹ì • Langflow Flowë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # TODO: ì‹¤ì œ Langflow Flow ì‹¤í–‰ ë¡œì§ êµ¬í˜„
            # 1. Flow IDë¡œ Flow JSON ë¡œë“œ
            # 2. ì»¨í…ìŠ¤íŠ¸ì™€ ë©”ì‹œì§€ë¥¼ Flowì— ì „ë‹¬
            # 3. Flow ì‹¤í–‰ ê²°ê³¼ ë°˜í™˜
            
            # ì„ì‹œ êµ¬í˜„
            context_text = ""
            if context:
                context_text = "\n".join([doc.get("content", "") for doc in context])
            
            return f"Flow {flow_id} ì‹¤í–‰ ê²°ê³¼: {message}\n\nì°¸ê³  ë¬¸ì„œ:\n{context_text}"
            
        except Exception as e:
            return f"Flow ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def search_documents(self, query: str, category_ids: List[str] = None, categories: List[str] = None) -> List[Dict[str, Any]]:
        """ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ í•„í„°ë§
            if category_ids or categories:
                files = await self.file_service.get_files_by_categories(category_ids, categories)
            else:
                files = await self.file_service.list_files()
            
            # ë²¡í„°í™”ëœ ë¬¸ì„œì—ì„œ ê²€ìƒ‰
            documents = []
            for file_info in files[:5]:  # ìµœëŒ€ 5ê°œ íŒŒì¼
                if file_info.vectorized:
                    # ë²¡í„° ë°ì´í„° ë¡œë“œ
                    vector_content = await self._load_vector_data(file_info.file_id)
                    if vector_content:
                        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°)
                        relevant_chunks = await self._search_chunks(query, vector_content.get("chunks", []))
                        
                        for chunk in relevant_chunks[:3]:  # íŒŒì¼ë‹¹ ìµœëŒ€ 3ê°œ ì²­í¬
                            # ì´ë¯¸ì§€ ì²­í¬ì¸ì§€ í™•ì¸
                            chunk_data = {
                                "file_id": file_info.file_id,
                                "filename": file_info.filename,
                                "category_id": file_info.category_id,
                                "category_name": file_info.category_name,
                                "content": chunk,
                                "score": 0.8
                            }
                            
                            # ì´ë¯¸ì§€ ìº¡ì…˜ ì²­í¬ì¸ ê²½ìš° ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ
                            if is_image_chunk(chunk):
                                image_path = extract_image_path_from_chunk(chunk)
                                if image_path:
                                    chunk_data["image_path"] = image_path
                                    chunk_data["is_image_chunk"] = True
                                    print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²­í¬ ê°ì§€: {image_path}")
                                else:
                                    chunk_data["is_image_chunk"] = False
                            else:
                                chunk_data["is_image_chunk"] = False
                            
                            documents.append(chunk_data)
                else:
                    # ë²¡í„°í™”ë˜ì§€ ì•Šì€ íŒŒì¼ì€ ë©”íƒ€ë°ì´í„°ë§Œ ì œê³µ
                    documents.append({
                        "file_id": file_info.file_id,
                        "filename": file_info.filename,
                        "category_id": file_info.category_id,
                        "category_name": file_info.category_name,
                        "content": f"{file_info.filename} (ì•„ì§ ë²¡í„°í™”ë˜ì§€ ì•ŠìŒ)",
                        "score": 0.3
                    })
            
            return documents
            
        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def generate_response_with_vision(self, query: str, context: List[Dict[str, Any]], system_message: str = None) -> str:
        """ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ Vision ëª¨ë¸ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            context_sections = []
            sources_info = []
            image_paths = []
            
            print(f"=== Vision ëª¨ë¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì‹œì‘ ===")
            print(f"ì „ë‹¬ë°›ì€ ë¬¸ì„œ ìˆ˜: {len(context)}")
            
            for i, doc in enumerate(context, 1):
                source_name = doc.get("filename", f"ë¬¸ì„œ{i}")
                content = doc.get("content", "")
                
                # ì´ë¯¸ì§€ ì²­í¬ì¸ì§€ í™•ì¸
                if doc.get("is_image_chunk", False) and doc.get("image_path"):
                    image_path = doc.get("image_path")
                    image_paths.append(image_path)
                    
                    # ì´ë¯¸ì§€ ìº¡ì…˜ì—ì„œ ê²½ë¡œ ë¶€ë¶„ ì œê±°í•œ ìˆœìˆ˜ ìº¡ì…˜ë§Œ ì¶”ì¶œ
                    clean_caption = content
                    if content.startswith("[ì´ë¯¸ì§€:") and "]" in content:
                        clean_caption = content[content.find("]") + 1:].strip()
                    
                    context_sections.append(f"=== ì´ë¯¸ì§€ {i}: {source_name} ===\nì´ë¯¸ì§€ ì„¤ëª…: {clean_caption}\n")
                    sources_info.append(f"[{i}] {source_name} (ì´ë¯¸ì§€)")
                    print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¬¸ì„œ {i}: {source_name} -> {image_path}")
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ
                    context_sections.append(f"=== ë¬¸ì„œ {i}: {source_name} ===\n{content}\n")
                    sources_info.append(f"[{i}] {source_name}")
                    print(f"ğŸ“„ í…ìŠ¤íŠ¸ ë¬¸ì„œ {i}: {source_name} (ê¸¸ì´: {len(content)} ê¸€ì)")
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text = "\n".join(context_sections)
            sources_text = "\n".join(sources_info) if sources_info else "ì°¸ê³  ë¬¸ì„œ ì—†ìŒ"
            
            print(f"=== Vision ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ ===")
            print(f"ì´ë¯¸ì§€ ìˆ˜: {len(image_paths)}")
            print(f"ì†ŒìŠ¤ ì •ë³´: {sources_text}")
            print(f"ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context_text)} ê¸€ì")
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
            if not system_message:
                system_message = "ë‹¹ì‹ ì€ ë¬¸ì„œì™€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            
            vision_system_message = f"""{system_message}

ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œì™€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
2. ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ê²½ìš°, ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ìì„¸íˆ ë¶„ì„í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”.
3. ë‹µë³€ì— ì‚¬ìš©í•œ ì†ŒìŠ¤ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

ì°¸ê³  ìë£Œ:
{sources_text}

ì»¨í…ìŠ¤íŠ¸:
{context_text}"""
            
            # Vision ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±
            messages = [
                {"role": "system", "content": vision_system_message},
                {"role": "user", "content": f"ì§ˆë¬¸: {query}"}
            ]
            
            # Vision ëª¨ë¸ í˜¸ì¶œ
            if image_paths:
                response = await self._call_vision_llm(messages, image_paths)
            else:
                response = await self._call_llm(messages)
            
            print(f"âœ… Vision ëª¨ë¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            return response
            
        except Exception as e:
            print(f"âŒ Vision ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë¸ë¡œ í´ë°±
            return await self.generate_response_with_flow(query, context, system_message, None)
    
    async def generate_response_with_flow(self, query: str, context: List[Dict[str, Any]], system_message: str = None, flow_id: str = None) -> str:
        """LangFlowë¥¼ í†µí•´ ë™ì ìœ¼ë¡œ ì„ íƒëœ LLM ëª¨ë¸ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ ë” ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
            context_sections = []
            sources_info = []
            
            print(f"=== LangFlow ê¸°ë°˜ LLM ì‘ë‹µ ìƒì„± ì‹œì‘ ===")
            print(f"ì‚¬ìš© Flow ID: {flow_id}")
            print(f"ì „ë‹¬ë°›ì€ ë¬¸ì„œ ìˆ˜: {len(context)}")
            
            for i, doc in enumerate(context, 1):
                source_name = doc.get("filename", f"ë¬¸ì„œ{i}")
                content = doc.get("content", "")
                
                # ê° ë¬¸ì„œë¥¼ ëª…í™•íˆ êµ¬ë¶„
                context_sections.append(f"=== ë¬¸ì„œ {i}: {source_name} ===\n{content}\n")
                sources_info.append(f"[{i}] {source_name}")
                
                print(f"ë¬¸ì„œ {i}: {source_name} (ê¸¸ì´: {len(content)} ê¸€ì)")
            
            # ëª¨ë“  ë¬¸ì„œ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            context_text = "\n".join(context_sections)
            sources_text = "\n".join(sources_info) if sources_info else "ì°¸ê³  ë¬¸ì„œ ì—†ìŒ"
            
            print(f"=== ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ ===")
            print(f"ì†ŒìŠ¤ ì •ë³´: {sources_text}")
            print(f"ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context_text)} ê¸€ì")
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ ë° ì¶•ì†Œ
            if len(context_text) > 8000:  # 8000ì ì œí•œ
                print(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ {len(context_text)}ì, ì¶•ì†Œ í•„ìš”")
                # ê° ë¬¸ì„œë¥¼ 500ìë¡œ ì œí•œ
                shortened_sections = []
                for i, doc in enumerate(context, 1):
                    source_name = doc.get("filename", f"ë¬¸ì„œ{i}")
                    content = doc.get("content", "")[:500] + ("..." if len(doc.get("content", "")) > 500 else "")
                    shortened_sections.append(f"=== ë¬¸ì„œ {i}: {source_name} ===\n{content}\n")
                context_text = "\n".join(shortened_sections)
                print(f"ì¶•ì†Œ í›„ ê¸¸ì´: {len(context_text)}ì")

            # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì¶œì²˜ë¥¼ [1], [2] í˜•íƒœë¡œ í‘œì‹œí•˜ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{sources_text}

ë‚´ìš©:
{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

            # LangFlowë¥¼ í†µí•´ LLM ì‹¤í–‰
            flow_id_to_use = flow_id or await self._get_default_search_flow()
            if flow_id_to_use:
                print(f"LangFlow ì‹¤í–‰: {flow_id_to_use}")
                model_config = await get_current_model_config()
                langflow_result = await self.langflow_service.execute_flow_with_llm(
                    flow_id_to_use,
                    prompt,
                    system_message,
                    model_config=model_config
                )
                
                if langflow_result.get("status") == "success":
                    response_text = langflow_result.get("response", "LangFlowì—ì„œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    print(f"LangFlow ì‘ë‹µ ì„±ê³µ: {len(response_text)} ê¸€ì")
                    return response_text
                else:
                    print(f"LangFlow ì‹¤í–‰ ì‹¤íŒ¨: {langflow_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    # Fallback to original method
                    return await self.generate_response_fallback(query, context, system_message)
            else:
                print("Flow IDê°€ ì—†ìŠµë‹ˆë‹¤. Fallback ì‚¬ìš©")
                return await self.generate_response_fallback(query, context, system_message)
                
        except Exception as e:
            print(f"LangFlow ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            # Fallback to original method
            return await self.generate_response_fallback(query, context, system_message)

    async def generate_response_fallback(self, query: str, context: List[Dict[str, Any]], system_message: str = None) -> str:
        """Fallback: ê¸°ì¡´ OpenAI ì§ì ‘ í˜¸ì¶œ ë°©ì‹"""
        print("=== Fallback: OpenAI ì§ì ‘ í˜¸ì¶œ ì‚¬ìš© ===")
        return await self.generate_response(query, context, system_message)

    async def generate_response(self, query: str, context: List[Dict[str, Any]], system_message: str = None) -> str:
        """ì„¤ì •ëœ LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        llm_client, provider = await self._get_llm_client()
        if not llm_client:
            return "LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì—…ì²´ì…ë‹ˆë‹¤."
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ ë” ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
            context_sections = []
            sources_info = []
            
            print(f"=== LLM ì „ë‹¬ìš© ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹œì‘ ===")
            print(f"ì „ë‹¬ë°›ì€ ë¬¸ì„œ ìˆ˜: {len(context)}")
            
            for i, doc in enumerate(context, 1):
                source_name = doc.get("filename", f"ë¬¸ì„œ{i}")
                content = doc.get("content", "")
                
                # ê° ë¬¸ì„œë¥¼ ëª…í™•íˆ êµ¬ë¶„
                context_sections.append(f"=== ë¬¸ì„œ {i}: {source_name} ===\n{content}\n")
                sources_info.append(f"[{i}] {source_name}")
                
                print(f"ë¬¸ì„œ {i}: {source_name} (ê¸¸ì´: {len(content)} ê¸€ì)")
            
            # ëª¨ë“  ë¬¸ì„œ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            context_text = "\n".join(context_sections)
            sources_text = "\n".join(sources_info) if sources_info else "ì°¸ê³  ë¬¸ì„œ ì—†ìŒ"
            
            print(f"=== ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ ===")
            print(f"ì†ŒìŠ¤ ì •ë³´: {sources_text}")
            print(f"ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context_text)} ê¸€ì")
            
            prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€í•  ë•Œ ê´€ë ¨ëœ ì¶œì²˜ë¥¼ [1], [2] í˜•íƒœë¡œ ì¸ë¼ì¸ì— í‘œì‹œí•´ì£¼ì„¸ìš”.

ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œ ëª©ë¡:
{sources_text}

ëª¨ë“  ë¬¸ì„œ ë‚´ìš©:
{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€ ê·œì¹™:
1. ëª¨ë“  ê´€ë ¨ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ë‹µë³€ ë‚´ìš©ì— ê´€ë ¨ëœ ì¶œì²˜ë¥¼ [1], [2] í˜•íƒœë¡œ í‘œì‹œí•˜ì„¸ìš”  
3. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì–»ì€ ì •ë³´ëŠ” ëª¨ë‘ í™œìš©í•˜ì„¸ìš”"""
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
            if not system_message:
                system_message = "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
            
            # ë™ì  LLM í˜¸ì¶œ
            print(f"ì„¤ì •ëœ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„± ì‹œì‘")
            response = await self._call_llm([
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ])
            
            print(f"LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            return response
            
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _load_vector_data(self, file_id: str) -> Dict[str, Any]:
        """ë²¡í„° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (DEPRECATED: ChromaDBì™€ SQLite ë©”íƒ€ë°ì´í„° ì‚¬ìš©)"""
        # ë ˆê±°ì‹œ í•¨ìˆ˜ - ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
        print(f"ê²½ê³ : _load_vector_dataëŠ” deprecated í•¨ìˆ˜ì…ë‹ˆë‹¤. ChromaDBë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì„¸ìš”.")
        return None
    
    async def _search_chunks(self, query: str, chunks: List[str]) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì²­í¬ ê²€ìƒ‰ (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°)"""
        try:
            query_lower = query.lower()
            relevant_chunks = []
            
            for chunk in chunks:
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
                if any(keyword in chunk.lower() for keyword in query_lower.split()):
                    relevant_chunks.append(chunk)
            
            # ìµœëŒ€ 5ê°œ ì²­í¬ ë°˜í™˜
            return relevant_chunks[:5]
            
        except Exception as e:
            print(f"ì²­í¬ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def _get_default_search_flow(self) -> str:
        """ê¸°ë³¸ ê²€ìƒ‰ Flow IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # ì„¤ì • íŒŒì¼ì—ì„œ ê¸°ë³¸ ê²€ìƒ‰ Flow ID ì½ê¸°
            config_file = os.path.join(settings.BASE_DIR, "langflow", "config.json")
            
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                return config_data.get("default_search_flow_id")
            
            return None
            
        except Exception as e:
            print(f"ê¸°ë³¸ ê²€ìƒ‰ Flow ID ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def _extract_sources_from_langflow_result(self, langflow_result: Dict[str, Any], category_ids: List[str] = None) -> List[Dict[str, Any]]:
        """LangFlow ê²°ê³¼ì—ì„œ ì†ŒìŠ¤ ë¬¸ì„œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            sources = []
            
            # LangFlow ê²°ê³¼ì—ì„œ ì‚¬ìš©ëœ íŒŒì¼ ì •ë³´ ì¶”ì¶œ (êµ¬í˜„ í•„ìš”)
            # í˜„ì¬ëŠ” ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ë°˜í™˜
            if category_ids:
                files = await self.file_service.get_files_by_categories(category_ids, None)
                for file_info in files[:3]:  # ìµœëŒ€ 3ê°œ íŒŒì¼
                    if file_info.vectorized:
                        sources.append({
                            "file_id": file_info.file_id,
                            "filename": file_info.filename,
                            "category_id": file_info.category_id,
                            "category_name": file_info.category_name,
                            "content": f"ğŸ“„ {file_info.filename}ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                            "score": 0.9
                        })
            
            return sources
            
        except Exception as e:
            print(f"ì†ŒìŠ¤ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return [] 

    def _unique_sources(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """íŒŒì¼ëª… ê¸°ì¤€ ì¤‘ë³µ ì œê±°: ë™ì¼ íŒŒì¼ëª…ì€ 1ê°œë§Œ ë°˜í™˜"""
        if not documents:
            print("=== ì¤‘ë³µ ì œê±°: ì…ë ¥ ë¬¸ì„œ ì—†ìŒ ===")
            return []
            
        print(f"=== ì¤‘ë³µ ì œê±° ì‹œì‘ ===")
        print(f"ì…ë ¥ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        
        # ì…ë ¥ ë¬¸ì„œë“¤ì˜ íŒŒì¼ëª… í™•ì¸
        filenames = [doc.get("filename", "EMPTY") for doc in documents]
        print(f"ëª¨ë“  íŒŒì¼ëª…: {filenames}")
        
        # ê°•ë ¥í•œ ì¤‘ë³µ ì œê±°: filenameì„ ì •ê·œí™”í•˜ê³  ì¤‘ë³µ ì œê±°
        seen_filenames = set()
        result = []
        
        for i, doc in enumerate(documents):
            filename = doc.get("filename", "").strip()
            if not filename:
                print(f"ë¬¸ì„œ {i}: filenameì´ ë¹„ì–´ìˆìŒ, ê±´ë„ˆëœ€")
                continue
                
            # íŒŒì¼ëª… ì •ê·œí™” (ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜)
            normalized_filename = filename.lower().strip()
            
            if normalized_filename not in seen_filenames:
                seen_filenames.add(normalized_filename)
                result.append({
                    "file_id": doc.get("file_id", ""),
                    "filename": filename,  # ì›ë³¸ íŒŒì¼ëª… ìœ ì§€
                    "category_id": doc.get("category_id", ""),
                    "category_name": doc.get("category_name", ""),
                })
                print(f"ë¬¸ì„œ {i}: filename='{filename}' ì¶”ê°€ë¨ (ì •ê·œí™”: {normalized_filename})")
            else:
                print(f"ë¬¸ì„œ {i}: filename='{filename}' ì¤‘ë³µ (ì •ê·œí™”: {normalized_filename}), ê±´ë„ˆëœ€")
        
        print(f"ìµœì¢… ìœ ë‹ˆí¬ ì†ŒìŠ¤ ìˆ˜: {len(result)}")
        print(f"ìœ ë‹ˆí¬ ì†ŒìŠ¤ë“¤: {[r['filename'] for r in result]}")
        print(f"=== ì¤‘ë³µ ì œê±° ì™„ë£Œ ===")
        return result

    async def get_chat_history(self, user_id: str, limit: int = 50) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ìë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            history_file = os.path.join(
                settings.DATA_DIR, 
                'chat_history.json'
            )
            
            if not os.path.exists(history_file):
                return []
            
            with open(history_file, 'r', encoding='utf-8') as f:
                all_history = json.load(f)
            
            # ì‚¬ìš©ìë³„ íˆìŠ¤í† ë¦¬ í•„í„°ë§
            user_history = all_history.get(user_id, [])
            
            # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  limitë§Œí¼ ë°˜í™˜
            sorted_history = sorted(user_history, key=lambda x: x.get('timestamp', ''), reverse=True)
            return sorted_history[:limit]
            
        except Exception as e:
            print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return []

    async def save_chat_history(self, user_id: str, user_message: dict, assistant_message: dict) -> bool:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            history_file = os.path.join(
                settings.DATA_DIR, 
                'chat_history.json'
            )
            
            # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ë¡œë“œ
            all_history = {}
            if os.path.exists(history_file):
                with open(history_file, 'r', encoding='utf-8') as f:
                    all_history = json.load(f)
            
            # ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            if user_id not in all_history:
                all_history[user_id] = []
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            user_msg = {
                "id": user_message.get("id", str(int(time.time() * 1000))),
                "message": user_message.get("content", ""),
                "role": "user",
                "timestamp": user_message.get("timestamp", datetime.now().isoformat()),
                "category_ids": user_message.get("categories", []),
                "sources": user_message.get("sources", [])
            }
            
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
            assistant_msg = {
                "id": assistant_message.get("id", str(int(time.time() * 1000) + 1)),
                "message": assistant_message.get("content", ""),
                "role": "assistant",
                "timestamp": assistant_message.get("timestamp", datetime.now().isoformat()),
                "category_ids": assistant_message.get("categories", []),
                "sources": assistant_message.get("sources", []),
                "source_details": assistant_message.get("sourceDetails", []),
                "processing_time": assistant_message.get("processingTime"),
                "confidence": assistant_message.get("confidence")
            }
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            all_history[user_id].extend([user_msg, assistant_msg])
            
            # ìµœëŒ€ 100ê°œ ë©”ì‹œì§€ë¡œ ì œí•œ (50ê°œ ëŒ€í™”)
            if len(all_history[user_id]) > 100:
                all_history[user_id] = all_history[user_id][-100:]
            
            # íŒŒì¼ì— ì €ì¥
            os.makedirs(os.path.dirname(history_file), exist_ok=True)
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(all_history, f, ensure_ascii=False, indent=2)
            
            print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {user_id}")
            return True
            
        except Exception as e:
            print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return False

    async def _save_chat_message(
        self, 
        user_id: str, 
        message: str, 
        role: str, 
        category_ids: List[str] = None,
        sources: List[Dict[str, Any]] = None
    ):
        """ì±„íŒ… ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            history_file = os.path.join(
                settings.DATA_DIR, 
                'chat_history.json'
            )
            
            # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ë¡œë“œ
            all_history = {}
            if os.path.exists(history_file):
                with open(history_file, 'r', encoding='utf-8') as f:
                    all_history = json.load(f)
            
            # ì‚¬ìš©ìë³„ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            if user_id not in all_history:
                all_history[user_id] = []
            
            # ìƒˆ ë©”ì‹œì§€ ì¶”ê°€
            chat_message = {
                "id": f"{user_id}_{int(time.time() * 1000)}",
                "user_id": user_id,
                "message": message,
                "role": role,
                "timestamp": datetime.now().isoformat(),
                "category_ids": category_ids or [],
                "sources": sources or []
            }
            
            all_history[user_id].append(chat_message)
            
            # ìµœëŒ€ 1000ê°œ ë©”ì‹œì§€ ìœ ì§€
            if len(all_history[user_id]) > 1000:
                all_history[user_id] = all_history[user_id][-1000:]
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(all_history, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"ì±„íŒ… ë©”ì‹œì§€ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def _build_system_message(self, custom_message: str = None, persona_id: str = None) -> str:
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤: ê¸°ë³¸/ì‚¬ìš©ì ì§€ì • ë©”ì‹œì§€ + (ì„ íƒ) í˜ë¥´ì†Œë‚˜ ë©”ì‹œì§€ + Chart.js ìƒì„± ì§€ì‹œì‚¬í•­ ê²°í•©."""
        try:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì„¤ì • or ì‚¬ìš©ì ì§€ì •)
            if custom_message:
                print("ì‚¬ìš©ì ì§€ì • ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì‚¬ìš©")
                base_message = custom_message
            else:
                base_message = await self.system_settings_service.get_default_system_message()
                print("ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì‚¬ìš©")

            # ì‚¬ìš©í•  í˜ë¥´ì†Œë‚˜ ê²°ì •: ìš”ì²­ > ì‹œìŠ¤í…œ ê¸°ë³¸
            chosen_persona_id = persona_id or await self.system_settings_service.get_default_persona_id()

            persona_text = None
            if chosen_persona_id:
                persona = await self.persona_service.get_persona(chosen_persona_id)
                if persona and persona.system_message:
                    print(f"í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ í¬í•¨: {persona.name}")
                    persona_text = persona.system_message
                else:
                    print(f"í˜ë¥´ì†Œë‚˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—†ìŒ: {chosen_persona_id}")


            # ìµœì¢… ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
            message_parts = [base_message]
            
            if persona_text:
                message_parts.append(persona_text)
            
            final_message = "\n\n".join(message_parts)
            print(f"ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„± ì™„ë£Œ (ê¸¸ì´: {len(final_message)}ì)")
            
            return final_message

        except Exception as e:
            print(f"ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ìµœì¢… í´ë°±: ê°„ë‹¨í•œ ê¸°ë³¸ê°’
            return "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ë‹µë³€í•  ë•Œ ê´€ë ¨ëœ ì¶œì²˜ë¥¼ [1], [2] í˜•íƒœë¡œ ì¸ë¼ì¸ì— í‘œì‹œí•´ì£¼ì„¸ìš”."
    
    async def generate_multimodal_response_with_flow(self, query: str, images: List[str], context: List[Dict[str, Any]], system_message: str = None, flow_id: str = None) -> str:
        """ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ìƒì„±: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            print(f"=== ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ìƒì„± ì‹œì‘ ===")
            print(f"í…ìŠ¤íŠ¸: {query[:100]}...")
            print(f"ì´ë¯¸ì§€ ìˆ˜: {len(images)}")
            print(f"ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜: {len(context)}")
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ê¸°ì¡´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì™€ ë™ì¼)
            context_sections = []
            sources_info = []
            
            for i, doc in enumerate(context, 1):
                source_name = doc.get("filename", f"ë¬¸ì„œ{i}")
                content = doc.get("content", "")
                
                context_sections.append(f"=== ë¬¸ì„œ {i}: {source_name} ===\n{content}\n")
                sources_info.append(f"[{i}] {source_name}")
            
            context_text = "\n".join(context_sections)
            sources_text = "\n".join(sources_info) if sources_info else "ì°¸ê³  ë¬¸ì„œ ì—†ìŒ"
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë©€í‹°ëª¨ë‹¬ì—ì„œëŠ” ë” ì§§ê²Œ)
            if len(context_text) > 5000:  # ì´ë¯¸ì§€ ë•Œë¬¸ì— ì»¨í…ìŠ¤íŠ¸ë¥¼ ë” ì¤„ì„
                print(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ {len(context_text)}ì, ì¶•ì†Œ í•„ìš”")
                shortened_sections = []
                for i, doc in enumerate(context, 1):
                    source_name = doc.get("filename", f"ë¬¸ì„œ{i}")
                    content = doc.get("content", "")[:300] + ("..." if len(doc.get("content", "")) > 300 else "")
                    shortened_sections.append(f"=== ë¬¸ì„œ {i}: {source_name} ===\n{content}\n")
                context_text = "\n".join(shortened_sections)
                print(f"ì¶•ì†Œ í›„ ê¸¸ì´: {len(context_text)}ì")

            # ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì´ë¯¸ì§€ ë¶„ì„ + ë¬¸ì„œ ì°¸ê³ )
            prompt = f"""ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ì°¸ê³  ë¬¸ì„œë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{sources_text}

ë¬¸ì„œ ë‚´ìš©:
{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€ ì‹œ ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”:
1. ì´ë¯¸ì§€ì—ì„œ ê´€ì°°ë˜ëŠ” ë‚´ìš© ë¶„ì„
2. ì°¸ê³  ë¬¸ì„œì˜ ê´€ë ¨ ì •ë³´ í™œìš©
3. ì´ë¯¸ì§€ì™€ ë¬¸ì„œ ì •ë³´ë¥¼ ì¢…í•©í•œ ìµœì¢… ë‹µë³€
4. ì¶œì²˜ëŠ” [1], [2] í˜•íƒœë¡œ í‘œì‹œ

ë‹µë³€:"""

            # LangFlowë¥¼ í†µí•´ ë©€í‹°ëª¨ë‹¬ LLM ì‹¤í–‰
            flow_id_to_use = flow_id or await self._get_default_search_flow()
            if flow_id_to_use:
                print(f"ë©€í‹°ëª¨ë‹¬ LangFlow ì‹¤í–‰: {flow_id_to_use}")
                model_config = await get_current_model_config()
                langflow_result = await self.langflow_service.execute_multimodal_flow_with_llm(
                    flow_id_to_use,
                    prompt,
                    images,
                    system_message,
                    model_config=model_config
                )
                
                if langflow_result.get("status") == "success":
                    response_text = langflow_result.get("response", "ë©€í‹°ëª¨ë‹¬ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    print(f"ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ì„±ê³µ: {len(response_text)} ê¸€ì")
                    return response_text
                else:
                    print(f"ë©€í‹°ëª¨ë‹¬ ì‹¤í–‰ ì‹¤íŒ¨: {langflow_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    # Fallback: ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
                    return await self.generate_response_with_flow(query, context, system_message, flow_id)
            else:
                print("Flow IDê°€ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì „ìš© Fallback ì‚¬ìš©")
                return await self.generate_response_with_flow(query, context, system_message, None)
                
        except Exception as e:
            print(f"ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            # Fallback: í…ìŠ¤íŠ¸ ì „ìš© ì²˜ë¦¬
            return await self.generate_response_with_flow(query, context, system_message, flow_id)