"""
PRD ë°©ì‹ ì‚¬ìš©ì ê°œì…í˜• ì²­í‚¹ ì„œë¹„ìŠ¤
ìë™ ì œì•ˆ â†’ ì‚¬ìš©ì í¸ì§‘/í™•ì • â†’ ì„ë² ë”©/ì €ì¥ íŒŒì´í”„ë¼ì¸
"""

import re
import uuid
import json
import math
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict, field
from datetime import datetime
from enum import Enum
import logging
from collections import Counter

# ì½˜ì†” ë¡œê±° ì‚¬ìš©ì„ ìœ„í•œ import ì¶”ê°€ ì‹œë„
try:
    from ..core.logger import get_console_logger
    logger = get_console_logger()
except ImportError:
    logger = logging.getLogger(__name__)


class ChunkQualityIssue(str, Enum):
    """ì²­í¬ í’ˆì§ˆ ì´ìŠˆ íƒ€ì…"""
    TOO_LONG = "too_long"           # ë„ˆë¬´ ê¸´ ì²­í¬
    TOO_SHORT = "too_short"         # ë„ˆë¬´ ì§§ì€ ì²­í¬
    HEADING_SPLIT = "heading_split"  # í—¤ë”©ì´ ë¶„í• ë¨
    HEADING_BOUNDARY = "heading_boundary"  # í—¤ë”© ê²½ê³„ ìœ„ë°˜
    TABLE_SPLIT = "table_split"     # í‘œê°€ ë¶„í• ë¨
    LIST_SPLIT = "list_split"       # ëª©ë¡ì´ ë¶„í• ë¨
    NO_CONTENT = "no_content"       # ë‚´ìš© ì—†ìŒ
    DUPLICATE_CONTENT = "duplicate_content"  # ì¤‘ë³µ ì˜ì‹¬ ì½˜í…ì¸ 
    ISOLATED_CAPTION = "isolated_caption"   # ê³ ë¦½ëœ ìº¡ì…˜


@dataclass
class ChunkingRules:
    """ì²­í‚¹ ê·œì¹™ ì„¤ì • (ì¤‘ì•™ ì§‘ì¤‘ ì„¤ì • ê¸°ë°˜)"""
    # ê³µí†µ ê·œì¹™
    max_tokens: int = 800
    min_tokens: int = 200
    overlap_tokens: int = 80
    hard_sentence_max_tokens: int = 1000  # ê°•ì œ ë¶„ì ˆ ì„ê³„ê°’
    respect_headings: bool = True
    preserve_tables: bool = True
    preserve_lists: bool = True
    drop_short_chunks: bool = False
    
    # ë¬¸ì¥ ë¶„í•  ë°©ë²• ì„ íƒ
    sentence_splitter: str = "kss"  # "kss", "kiwi", "regex", "recursive"
    
    # KSS ì „ìš© ì˜µì…˜ (Python KSS 6.0.5 í˜¸í™˜)
    kss_backend: str = "punct"  # ë¶„ì„ ë°±ì—”ë“œ: 'mecab', 'pecab', 'punct', 'fast' (pecab overflow ì´ìŠˆë¡œ punct ê¸°ë³¸)
    kss_num_workers: int = 1  # ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜
    kss_strip: bool = True  # ë¬¸ì¥ ì–‘ë ê³µë°± ì œê±°
    kss_return_morphemes: bool = False  # í˜•íƒœì†Œ ë°˜í™˜ ì—¬ë¶€ (Trueì‹œ ë³µì¡í•œ êµ¬ì¡° ë°˜í™˜)
    kss_ignores: List[str] = field(default_factory=list)  # ë¬´ì‹œí•  ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    
    # Kiwi ì „ìš© ì˜µì…˜
    kiwi_model_path: str = ""
    kiwi_integrate_allomorph: bool = True
    kiwi_load_default_dict: bool = True
    kiwi_max_unk_form_len: int = 8
    
    # ì •ê·œì‹ ì „ìš© ì˜µì…˜
    regex_sentence_endings: str = "[.!?]"
    regex_preserve_abbreviations: bool = True
    regex_custom_patterns: List[str] = field(default_factory=list)
    
    # RecursiveCharacterTextSplitter ì „ìš© ì˜µì…˜
    recursive_separators: List[str] = field(default_factory=lambda: ["\n\n", "\n", " ", ""])
    recursive_keep_separator: bool = False
    recursive_is_separator_regex: bool = False
    
    # í’ˆì§ˆ ë° ì¤‘ë³µ ê²€ì‚¬ ì„¤ì •
    enable_quality_check: bool = True
    enable_duplicate_check: bool = True
    similarity_threshold: float = 0.95
    word_overlap_threshold: float = 0.85
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì • (PDFìš©)
    enable_image_extraction: bool = False
    max_image_distance: float = 100.0
    max_images_per_chunk: int = 3
    
    # ê°ì‚¬ ë©”íƒ€ë°ì´í„°
    created_at: Optional[str] = None
    version: str = "2.0"
    
    @classmethod
    def from_settings(cls, settings: Dict[str, Any]) -> 'ChunkingRules':
        """ì¤‘ì•™ ì§‘ì¤‘ ì„¤ì •ì—ì„œ ì²­í‚¹ ê·œì¹™ ìƒì„±"""
        from ..services.settings_service import settings_service
        
        # manual_preprocessing ì„¤ì • ë¡œë“œ
        manual_settings = settings_service.get_section_settings("manual_preprocessing")
        
        # ì„¤ì •ê°’ë“¤ì„ ChunkingRulesë¡œ ë³€í™˜
        return cls(
            max_tokens=manual_settings.get("max_tokens", 800),
            min_tokens=manual_settings.get("min_tokens", 200), 
            overlap_tokens=manual_settings.get("overlap_tokens", 80),
            hard_sentence_max_tokens=manual_settings.get("hard_sentence_max_tokens", 1000),
            respect_headings=manual_settings.get("respect_headings", True),
            preserve_tables=manual_settings.get("preserve_tables", True),
            preserve_lists=manual_settings.get("preserve_lists", True),
            drop_short_chunks=manual_settings.get("drop_short_chunks", False),
            sentence_splitter=manual_settings.get("default_sentence_splitter", "kss"),
            kss_backend=manual_settings.get("kss_backend", "punct"),
            kss_num_workers=manual_settings.get("kss_num_workers", 1),
            kss_strip=manual_settings.get("kss_strip", True),
            kss_return_morphemes=manual_settings.get("kss_return_morphemes", False),
            kss_ignores=manual_settings.get("kss_ignores", []),
            kiwi_model_path=manual_settings.get("kiwi_model_path", ""),
            kiwi_integrate_allomorph=manual_settings.get("kiwi_integrate_allomorph", True),
            kiwi_load_default_dict=manual_settings.get("kiwi_load_default_dict", True),
            kiwi_max_unk_form_len=manual_settings.get("kiwi_max_unk_form_len", 8),
            regex_sentence_endings=manual_settings.get("regex_sentence_endings", "[.!?]"),
            regex_preserve_abbreviations=manual_settings.get("regex_preserve_abbreviations", True),
            regex_custom_patterns=manual_settings.get("regex_custom_patterns", []),
            recursive_separators=manual_settings.get("recursive_separators", ["\n\n", "\n", " ", ""]),
            recursive_keep_separator=manual_settings.get("recursive_keep_separator", False),
            recursive_is_separator_regex=manual_settings.get("recursive_is_separator_regex", False),
            enable_quality_check=manual_settings.get("enable_quality_check", True),
            enable_duplicate_check=manual_settings.get("enable_duplicate_check", True),
            similarity_threshold=manual_settings.get("similarity_threshold", 0.95),
            word_overlap_threshold=manual_settings.get("word_overlap_threshold", 0.85),
            enable_image_extraction=manual_settings.get("enable_image_extraction", False),
            max_image_distance=manual_settings.get("max_image_distance", 100.0),
            max_images_per_chunk=manual_settings.get("max_images_per_chunk", 3),
            created_at=datetime.now().isoformat(),
            version="2.1"
        )
    
    def to_audit_snapshot(self) -> Dict[str, Any]:
        """ê°ì‚¬ìš© ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        return {
            "rules": asdict(self),
            "timestamp": datetime.now().isoformat(),
            "version": self.version
        }


@dataclass
class QualityWarning:
    """í’ˆì§ˆ ê²½ê³ """
    issue_type: ChunkQualityIssue
    severity: str  # "warning", "error"
    message: str
    suggestion: Optional[str] = None


@dataclass
class BBox:
    """ë¬¸ì„œ ë‚´ ì˜ì—­ ì¢Œí‘œ (í”„ë¡œë•ì…˜ ê°œì„  - ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì—°ê´€ì„±)"""
    x0: float  # ì¢Œì¸¡ x ì¢Œí‘œ
    y0: float  # í•˜ë‹¨ y ì¢Œí‘œ (PDF ì¢Œí‘œê³„)
    x1: float  # ìš°ì¸¡ x ì¢Œí‘œ 
    y1: float  # ìƒë‹¨ y ì¢Œí‘œ (PDF ì¢Œí‘œê³„)
    page: int  # í˜ì´ì§€ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)
    
    @property
    def width(self) -> float:
        return self.x1 - self.x0
    
    @property
    def height(self) -> float:
        return self.y1 - self.y0
    
    @property
    def center_x(self) -> float:
        return (self.x0 + self.x1) / 2
    
    @property
    def center_y(self) -> float:
        return (self.y0 + self.y1) / 2
    
    def distance_to(self, other: 'BBox') -> float:
        """ë‹¤ë¥¸ bboxì™€ì˜ ì¤‘ì‹¬ì  ê±°ë¦¬"""
        if self.page != other.page:
            return float('inf')  # ë‹¤ë¥¸ í˜ì´ì§€ëŠ” ë¬´í•œëŒ€ ê±°ë¦¬
        
        dx = self.center_x - other.center_x
        dy = self.center_y - other.center_y
        return (dx * dx + dy * dy) ** 0.5
    
    def vertical_distance_to(self, other: 'BBox') -> float:
        """ì„¸ë¡œ ê±°ë¦¬ë§Œ ê³„ì‚° (ê°™ì€ í˜ì´ì§€ì¼ ë•Œ)"""
        if self.page != other.page:
            return float('inf')
        
        return abs(self.center_y - other.center_y)

@dataclass
class ImageRef:
    """ì´ë¯¸ì§€ ì°¸ì¡° ì •ë³´ (í”„ë¡œë•ì…˜ ê°œì„ )"""
    image_id: str  # ê³ ìœ  ì´ë¯¸ì§€ ID
    bbox: BBox     # ì´ë¯¸ì§€ ìœ„ì¹˜
    image_type: str = "unknown"  # ì´ë¯¸ì§€ íƒ€ì… (jpeg, png ë“±)
    description: Optional[str] = None  # ì´ë¯¸ì§€ ì„¤ëª… (OCR ë“±ìœ¼ë¡œ ì¶”ì¶œ ê°€ëŠ¥)
    distance_to_text: float = 0.0  # í…ìŠ¤íŠ¸ì™€ì˜ ê±°ë¦¬

@dataclass
class HeadingNode:
    """ê³„ì¸µì  í—¤ë”© ë…¸ë“œ (í”„ë¡œë•ì…˜ ê°œì„ )"""
    text: str
    level: int  # 1-6, HTML h1-h6ì™€ ë™ì¼
    start_index: int  # ë¬¸ì¥ ì‹œì‘ ì¸ë±ìŠ¤
    end_index: Optional[int] = None  # ë¬¸ì¥ ë ì¸ë±ìŠ¤ (ë‹¤ìŒ í—¤ë”© ì „ê¹Œì§€)
    children: List['HeadingNode'] = None
    parent: Optional['HeadingNode'] = None
    
    def __post_init__(self):
        if self.children is None:
            self.children = []

@dataclass
class SentenceInfo:
    """ë¬¸ì¥ ì •ë³´ (í”„ë¡œë•ì…˜ ê°œì„ )"""
    text: str
    tokens: int
    page: Optional[int] = None
    is_heading: bool = False
    is_list_item: bool = False
    is_table_content: bool = False
    index: int = 0
    heading_level: Optional[int] = None  # í—¤ë”©ì¸ ê²½ìš° ë ˆë²¨ (1-6)
    heading_path: Optional[List[str]] = None  # í—¤ë”© ê²½ë¡œ (ìƒìœ„ í—¤ë”©ë“¤)
    bbox: Optional[BBox] = None  # ë¬¸ì¥ì˜ ìœ„ì¹˜ ì •ë³´ (í”„ë¡œë•ì…˜ ê°œì„ )
    image_refs: List[ImageRef] = None  # ê·¼ì ‘í•œ ì´ë¯¸ì§€ë“¤ (í”„ë¡œë•ì…˜ ê°œì„ )
    
    def __post_init__(self):
        if self.image_refs is None:
            self.image_refs = []


@dataclass 
class ChunkEditLog:
    """ì²­í¬ í¸ì§‘ ë¡œê·¸ (ê°ì‚¬ ì¶”ì ìš©)"""
    timestamp: str
    user_id: Optional[str]
    action: str  # "create", "merge", "split", "edit_text", "delete"
    chunk_ids: List[str]
    details: Dict[str, Any]
    
    @classmethod
    def create_log(cls, action: str, chunk_ids: List[str], user_id: Optional[str] = None, **details):
        return cls(
            timestamp=datetime.now().isoformat(),
            user_id=user_id,
            action=action,
            chunk_ids=chunk_ids,
            details=details
        )


@dataclass
class ChunkProposal:
    """ì²­í¬ ì œì•ˆ (í”„ë¡œë•ì…˜ ê°ì‚¬ ì§€ì›)"""
    chunk_id: str
    order: int
    text: str
    token_estimate: int
    page_start: Optional[int] = None
    page_end: Optional[int] = None
    heading_path: Optional[List[str]] = None
    sentences: Optional[List[SentenceInfo]] = None
    quality_warnings: List[QualityWarning] = None
    image_refs: List[ImageRef] = None  # ì²­í¬ ë‚´ ì´ë¯¸ì§€ ì°¸ì¡°ë“¤ (í”„ë¡œë•ì…˜ ê°œì„ )
    
    # ê°ì‚¬ ë©”íƒ€ë°ì´í„°
    created_at: Optional[str] = None
    last_modified_at: Optional[str] = None
    edit_logs: List[ChunkEditLog] = None
    
    def __post_init__(self):
        if self.quality_warnings is None:
            self.quality_warnings = []
        if self.edit_logs is None:
            self.edit_logs = []
        if self.image_refs is None:
            self.image_refs = []
        if self.created_at is None:
            self.created_at = datetime.now().isoformat()
    
    def add_edit_log(self, action: str, user_id: Optional[str] = None, **details):
        """í¸ì§‘ ë¡œê·¸ ì¶”ê°€"""
        log = ChunkEditLog.create_log(action, [self.chunk_id], user_id, **details)
        self.edit_logs.append(log)
        self.last_modified_at = datetime.now().isoformat()


class TokenCounter:
    """í”„ë¡œë•ì…˜ ìˆ˜ì¤€ í† í° ì¹´ìš´í„° - ì„¤ì • ê¸°ë°˜ í´ë°± ì œì–´"""
    
    def __init__(self):
        self._tiktoken_encoder = None
        self._init_tiktoken()
    
    def _get_fallback_settings(self) -> Dict[str, Any]:
        """í´ë°± ì œì–´ ì„¤ì • ì¡°íšŒ"""
        try:
            from .settings_service import settings_service
            return settings_service.get_section_settings("fallback_control")
        except Exception:
            return {"enable_token_counter_fallback": False, "strict_mode": True}
    
    def _init_tiktoken(self):
        """tiktoken ì´ˆê¸°í™” (í”„ë¡œë•ì…˜ìš©)"""
        try:
            import tiktoken
            # OpenAI GPT-3.5/GPT-4 í˜¸í™˜ ì¸ì½”ë”©
            self._tiktoken_encoder = tiktoken.get_encoding("cl100k_base")
            logger.info("tiktoken ì´ˆê¸°í™” ì„±ê³µ - ì •í™•í•œ í† í° ê³„ì‚° ì‚¬ìš©")
        except ImportError:
            fallback_settings = self._get_fallback_settings()
            if fallback_settings.get("enable_token_counter_fallback", False):
                logger.warning("tiktoken ë¯¸ì„¤ì¹˜ - ì„¤ì •ì— ë”°ë¼ ì¶”ì • ëª¨ë“œ ì‚¬ìš©")
                self._tiktoken_encoder = None
            else:
                error_msg = "tiktoken íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install tiktokenìœ¼ë¡œ ì„¤ì¹˜í•˜ê±°ë‚˜ ì„¤ì •ì—ì„œ í´ë°±ì„ í™œì„±í™”í•˜ì„¸ìš”."
                logger.error(error_msg)
                if fallback_settings.get("strict_mode", True):
                    raise RuntimeError(error_msg)
                self._tiktoken_encoder = None
        except Exception as e:
            fallback_settings = self._get_fallback_settings()
            if fallback_settings.get("enable_token_counter_fallback", False):
                logger.error(f"tiktoken ì´ˆê¸°í™” ì‹¤íŒ¨: {e} - ì„¤ì •ì— ë”°ë¼ ì¶”ì • ëª¨ë“œ ì‚¬ìš©")
                self._tiktoken_encoder = None
            else:
                error_msg = f"tiktoken ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. ì„¤ì •ì—ì„œ í´ë°±ì„ í™œì„±í™”í•˜ê±°ë‚˜ tiktoken íŒ¨í‚¤ì§€ë¥¼ ì¬ì„¤ì¹˜í•˜ì„¸ìš”."
                logger.error(error_msg)
                if fallback_settings.get("strict_mode", True):
                    raise RuntimeError(error_msg)
                self._tiktoken_encoder = None
    
    def count_tokens(self, text: str) -> int:
        """ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°"""
        if not text or not text.strip():
            return 0
        
        try:
            if self._tiktoken_encoder:
                # tiktokenìœ¼ë¡œ ì •í™•í•œ ê³„ì‚°
                return len(self._tiktoken_encoder.encode(text))
            else:
                fallback_settings = self._get_fallback_settings()
                if fallback_settings.get("enable_token_counter_fallback", False):
                    # í´ë°±: í•œêµ­ì–´ + ì˜ì–´ í˜¼í•© í…ìŠ¤íŠ¸ ì¶”ì •
                    word_count = len(re.findall(r'\S+', text))
                    return max(1, int(word_count * 1.3))
                else:
                    error_msg = "tiktokenì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê±°ë‚˜ ì„¤ì •ì—ì„œ í´ë°±ì„ í™œì„±í™”í•˜ì„¸ìš”."
                    logger.error(error_msg)
                    if fallback_settings.get("strict_mode", True):
                        raise RuntimeError(error_msg)
                    return 1  # ìµœì†Œê°’ ë°˜í™˜
        except Exception as e:
            fallback_settings = self._get_fallback_settings()
            if fallback_settings.get("enable_token_counter_fallback", False):
                logger.error(f"í† í° ê³„ì‚° ì‹¤íŒ¨: {e} - ë¬¸ì ê¸°ë°˜ ì¶”ì • ì‚¬ìš©")
                return max(1, len(text) // 4)
            else:
                error_msg = f"í† í° ê³„ì‚° ì‹¤íŒ¨: {e}. ì„¤ì •ì—ì„œ í´ë°±ì„ í™œì„±í™”í•˜ì„¸ìš”."
                logger.error(error_msg)
                if fallback_settings.get("strict_mode", True):
                    raise RuntimeError(error_msg)
                return 1  # ìµœì†Œê°’ ë°˜í™˜


class PDFImageExtractor:
    """PDF ì´ë¯¸ì§€ bbox ì¶”ì¶œê¸° (í”„ë¡œë•ì…˜ ê°œì„ )"""
    
    def __init__(self):
        self._pymupdf_available = False
        self._pdfplumber_available = False
        self._init_pdf_libraries()
    
    def _init_pdf_libraries(self):
        """PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”"""
        try:
            import fitz  # PyMuPDF
            self._pymupdf_available = True
            logger.info("PyMuPDF ì´ˆê¸°í™” ì„±ê³µ - ê³ ì •ë°€ ì´ë¯¸ì§€ bbox ì¶”ì¶œ ì‚¬ìš©")
        except ImportError:
            logger.warning("PyMuPDF ë¯¸ì„¤ì¹˜")
        
        try:
            import pdfplumber
            self._pdfplumber_available = True
            logger.info("pdfplumber ì´ìš© ê°€ëŠ¥ - í´ë°± ì´ë¯¸ì§€ ì¶”ì¶œ ì§€ì›")
        except ImportError:
            logger.warning("pdfplumber ë¯¸ì„¤ì¹˜")
        
        if not self._pymupdf_available and not self._pdfplumber_available:
            logger.error("PDF ì´ë¯¸ì§€ ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤ (PyMuPDF ë˜ëŠ” pdfplumber í•„ìš”)")
    
    def extract_images_from_pdf(self, pdf_path: str) -> List[ImageRef]:
        """PDFì—ì„œ ì´ë¯¸ì§€ bbox ì¶”ì¶œ"""
        if self._pymupdf_available:
            return self._extract_with_pymupdf(pdf_path)
        elif self._pdfplumber_available:
            return self._extract_with_pdfplumber(pdf_path)
        else:
            logger.error("PDF ì´ë¯¸ì§€ ì¶”ì¶œ ë¶ˆê°€ëŠ¥ - ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            return []
    
    def _extract_with_pymupdf(self, pdf_path: str) -> List[ImageRef]:
        """PyMuPDFë¡œ ì´ë¯¸ì§€ ì¶”ì¶œ (ê³ ì •ë°€)"""
        try:
            import fitz
            images = []
            
            doc = fitz.open(pdf_path)
            for page_num in range(len(doc)):
                page = doc[page_num]
                image_list = page.get_images(full=True)
                
                for img_index, img in enumerate(image_list):
                    # ì´ë¯¸ì§€ ê°ì²´ ì •ë³´ ì¶”ì¶œ
                    xref = img[0]
                    bbox_info = page.get_image_bbox(img)
                    
                    if bbox_info:
                        # PyMuPDF bbox: (x0, y0, x1, y1) - PDF ì¢Œí‘œê³„
                        bbox = BBox(
                            x0=bbox_info.x0,
                            y0=bbox_info.y0,
                            x1=bbox_info.x1,
                            y1=bbox_info.y1,
                            page=page_num
                        )
                        
                        # ì´ë¯¸ì§€ íƒ€ì… ì¶”ì¶œ
                        try:
                            base_image = doc.extract_image(xref)
                            image_type = base_image.get("ext", "unknown")
                        except:
                            image_type = "unknown"
                        
                        image_ref = ImageRef(
                            image_id=f"page_{page_num}_img_{img_index}",
                            bbox=bbox,
                            image_type=image_type
                        )
                        images.append(image_ref)
                        
            doc.close()
            logger.info(f"PyMuPDFë¡œ {len(images)}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ")
            return images
            
        except Exception as e:
            logger.error(f"PyMuPDF ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_with_pdfplumber(self, pdf_path: str) -> List[ImageRef]:
        """pdfplumberë¡œ ì´ë¯¸ì§€ ì¶”ì¶œ (í´ë°±)"""
        try:
            import pdfplumber
            images = []
            
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_images = page.images
                    
                    for img_index, img in enumerate(page_images):
                        # pdfplumber bbox: 'x0', 'y0', 'x1', 'y1'
                        bbox = BBox(
                            x0=float(img['x0']),
                            y0=float(img['y0']),
                            x1=float(img['x1']),
                            y1=float(img['y1']),
                            page=page_num
                        )
                        
                        image_ref = ImageRef(
                            image_id=f"page_{page_num}_img_{img_index}",
                            bbox=bbox,
                            image_type="unknown"  # pdfplumberëŠ” íƒ€ì… ì •ë³´ ì œí•œì 
                        )
                        images.append(image_ref)
                        
            logger.info(f"pdfplumberë¡œ {len(images)}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ")
            return images
            
        except Exception as e:
            logger.error(f"pdfplumber ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []


class SmartTextSplitter:
    """í”„ë¡œë•ì…˜ ìˆ˜ì¤€ ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
    
    def __init__(self):        
        # PDF ì´ë¯¸ì§€ ì¶”ì¶œê¸° ì´ˆê¸°í™” (í”„ë¡œë•ì…˜ ê°œì„ )
        self.pdf_image_extractor = PDFImageExtractor()
        
        # í—¤ë”© íŒ¨í„´ë“¤ (ë ˆë²¨ í¬í•¨ - í”„ë¡œë•ì…˜ ê°œì„ )
        self.heading_patterns = [
            (re.compile(r'^(#{1,6})\s+(.+)'), 'markdown'),  # ë§ˆí¬ë‹¤ìš´ í—¤ë”© (# ## ###)
            (re.compile(r'^(\d+(?:\.\d+)*)\s+(.+)'), 'numbered'),  # 1. 1.1. 2.3.4 ë“±
            (re.compile(r'^([ê°€-í£])[\s]*\d*[\s]*[\.)]?\s*(.+)'), 'korean'),  # ê°€. ë‚˜1) ë“±
            (re.compile(r'^([A-Z][A-Z\s]{2,})$'), 'caps'),  # ì „ì²´ ëŒ€ë¬¸ì (ì§§ì€ ì œëª©)
        ]
        
        # ëª©ë¡ ì•„ì´í…œ íŒ¨í„´
        self.list_patterns = [
            re.compile(r'^\s*[-*+â€¢Â·]\s+'),  # ë¶ˆë¦¿ í¬ì¸íŠ¸
            re.compile(r'^\s*\d+[.)]\s+'),   # ìˆ«ì ëª©ë¡
            re.compile(r'^\s*[ê°€-í£][.)]\s+'),  # í•œê¸€ ëª©ë¡
            re.compile(r'^\s*[a-zA-Z][.)]\s+')  # ì˜ë¬¸ ëª©ë¡
        ]
        
        # í‘œ íŒ¨í„´ (ê°„ë‹¨í•œ ë²„ì „)
        self.table_pattern = re.compile(r'\|.*\|')
    
    
    def split_into_sentences(self, text: str, rules: ChunkingRules) -> List[SentenceInfo]:
        """í”„ë¡œë•ì…˜ ìˆ˜ì¤€ ë¬¸ì¥ ë¶„í•  (KSS/Kiwi í†µí•©)"""
        if not text or not text.strip():
            return []
        
        sentences = []
        lines = text.split('\n')
        sentence_index = 0
        token_counter = TokenCounter()
        
        # í—¤ë”© íŠ¸ë¦¬ êµ¬ì¶•ìš© (í”„ë¡œë•ì…˜ ê°œì„ )
        heading_stack = []  # í˜„ì¬ í—¤ë”© ê³„ì¸µ ì¶”ì 
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ë¼ì¸ ë‹¨ìœ„ë¡œ ë¶„ì„
            heading_info = self._get_heading_info(line)
            is_heading = heading_info is not None
            is_list_item = self._is_list_item(line)
            is_table_content = self._is_table_content(line)
            
            # í—¤ë”© ê²½ë¡œ êµ¬ì¶• (í”„ë¡œë•ì…˜ ê°œì„ )
            heading_level = None
            heading_path = None
            if is_heading:
                heading_level, heading_text = heading_info
                # í˜„ì¬ ë ˆë²¨ë³´ë‹¤ ë†’ê±°ë‚˜ ê°™ì€ ë ˆë²¨ì˜ í—¤ë”©ë“¤ì„ ìŠ¤íƒì—ì„œ ì œê±°
                while heading_stack and heading_stack[-1][0] >= heading_level:
                    heading_stack.pop()
                # í˜„ì¬ í—¤ë”©ì„ ìŠ¤íƒì— ì¶”ê°€
                heading_stack.append((heading_level, heading_text))
                # í—¤ë”© ê²½ë¡œ ìƒì„± (ìƒìœ„ í—¤ë”©ë“¤ì˜ í…ìŠ¤íŠ¸)
                heading_path = [h[1] for h in heading_stack[:-1]]  # í˜„ì¬ í—¤ë”© ì œì™¸
            
            if is_heading or is_list_item or is_table_content:
                # íŠ¹ìˆ˜ êµ¬ì¡°ëŠ” ë¼ì¸ ë‹¨ìœ„ë¡œ ìœ ì§€
                if line:
                    sentences.append(SentenceInfo(
                        text=line,
                        tokens=token_counter.count_tokens(line),
                        is_heading=is_heading,
                        is_list_item=is_list_item,
                        is_table_content=is_table_content,
                        index=sentence_index,
                        heading_level=heading_level,
                        heading_path=heading_path
                    ))
                    sentence_index += 1
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ì‚¬ìš©ì ì„ íƒ ë°©ë²•ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬
                line_sentences = self._split_sentences_by_method(line, rules)
                # í˜„ì¬ í—¤ë”© ê²½ë¡œë¥¼ ì¼ë°˜ í…ìŠ¤íŠ¸ì—ë„ ì ìš© (í”„ë¡œë•ì…˜ ê°œì„ )
                current_heading_path = [h[1] for h in heading_stack] if heading_stack else None
                
                for sent in line_sentences:
                    # ë¬¸ì¥ì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    if not isinstance(sent, str):
                        logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì¥ íƒ€ì…: {type(sent)}, ê°’: {sent}")
                        sent = str(sent) if sent is not None else ""
                    
                    sent = sent.strip()
                    if sent:
                        sentences.append(SentenceInfo(
                            text=sent,
                            tokens=token_counter.count_tokens(sent),
                            is_heading=False,
                            is_list_item=False,
                            is_table_content=False,
                            index=sentence_index,
                            heading_level=None,
                            heading_path=current_heading_path
                        ))
                        sentence_index += 1
        
        return sentences
    
    def _get_fallback_settings(self) -> Dict[str, Any]:
        """í´ë°± ì œì–´ ì„¤ì • ì¡°íšŒ"""
        try:
            from .settings_service import settings_service
            return settings_service.get_section_settings("fallback_control")
        except Exception:
            return {"enable_sentence_splitter_fallback": False, "strict_mode": True}
    
    def _split_sentences_by_method(self, text: str, rules: ChunkingRules) -> List[str]:
        """ì‚¬ìš©ì ì„ íƒ ë°©ë²•ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬ - ì„¤ì • ê¸°ë°˜ í´ë°± ì œì–´"""
        if not text.strip():
            return []
        
        method = rules.sentence_splitter.lower()
        logger.info(f"ğŸ“ ë¬¸ì¥ë¶„í• ë°©ë²• ì„ íƒ: {method.upper()}")
        
        try:
            if method == "kss":
                return self._split_sentences_kss(text, rules)
            elif method == "kiwi":
                return self._split_sentences_kiwi(text, rules)
            elif method == "regex":
                return self._split_sentences_regex(text, rules)
            elif method == "recursive":
                return self._split_sentences_recursive(text, rules)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì¥ ë¶„í•  ë°©ë²•: {method}")
        except ImportError as e:
            fallback_settings = self._get_fallback_settings()
            if fallback_settings.get("enable_sentence_splitter_fallback", False):
                logger.warning(f"{method.upper()} ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ - ì„¤ì •ì— ë”°ë¼ ì •ê·œì‹ ë°©ë²•ìœ¼ë¡œ í´ë°±")
                # ì •ê·œì‹ ë°©ë²•ìœ¼ë¡œ í´ë°±
                return self._split_sentences_regex(text, rules)
            else:
                error_msg = f"{method.upper()} ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}. íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê±°ë‚˜ ì„¤ì •ì—ì„œ í´ë°±ì„ í™œì„±í™”í•˜ì„¸ìš”."
                logger.error(error_msg)
                if fallback_settings.get("strict_mode", True):
                    raise RuntimeError(error_msg)
                return [text]  # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        except Exception as e:
            fallback_settings = self._get_fallback_settings()
            if fallback_settings.get("enable_sentence_splitter_fallback", False):
                logger.error(f"{method.upper()} ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e} - ì„¤ì •ì— ë”°ë¼ ì •ê·œì‹ ë°©ë²•ìœ¼ë¡œ í´ë°±")
                # ì •ê·œì‹ ë°©ë²•ìœ¼ë¡œ í´ë°±
                try:
                    return self._split_sentences_regex(text, rules)
                except Exception as regex_error:
                    logger.error(f"ì •ê·œì‹ í´ë°±ë„ ì‹¤íŒ¨: {regex_error} - ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜")
                    return [text]
            else:
                error_msg = f"{method.upper()} ë¬¸ì¥ ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì„¤ì •ì—ì„œ í´ë°±ì„ í™œì„±í™”í•˜ì„¸ìš”."
                logger.error(error_msg)
                if fallback_settings.get("strict_mode", True):
                    raise RuntimeError(error_msg)
                return [text]  # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜

    def _split_sentences_kss(self, text: str, rules: ChunkingRules) -> List[str]:
        """KSSë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ë¶„ë¦¬ (Python KSS 6.0.5 ì˜µì…˜ ì ìš©)"""
        try:
            from kss import Kss
            
            # KSS 6.0.5 ìƒˆë¡œìš´ API ì‚¬ìš©
            split_sentences = Kss("split_sentences")
            
            # íŒŒë¼ë¯¸í„° ì„¤ì •
            kwargs = {
                "backend": rules.kss_backend,
                "num_workers": rules.kss_num_workers,
                "strip": rules.kss_strip,
                "return_morphemes": rules.kss_return_morphemes
            }
            
            # ignores ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
            if rules.kss_ignores:
                kwargs["ignores"] = rules.kss_ignores
            
            logger.debug(f"KSS 6.0.5 íŒŒë¼ë¯¸í„°: {kwargs}")
            result = split_sentences(text, **kwargs)
            
            # KSS 6.0.5ëŠ” return_morphemes=Falseì¼ ë•Œ List[str]ì„ ë°˜í™˜í•´ì•¼ í•¨
            if not isinstance(result, list):
                logger.error(f"KSS 6.0.5 ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ íƒ€ì…: {type(result)}")
                logger.error(f"í˜„ì¬ ì„¤ì •: return_morphemes={rules.kss_return_morphemes}")
                logger.error("return_morphemes=Trueë¡œ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ Falseë¡œ ë³€ê²½í•˜ì„¸ìš”")
                return []
            
            # ë¹ˆ ê²°ê³¼ ì²˜ë¦¬
            if not result:
                logger.debug("KSS 6.0.5ì—ì„œ ë¹ˆ ê²°ê³¼ ë°˜í™˜")
                return []
            
            # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ í™•ì¸ ë° ì •ë¦¬
            sentences = []
            for item in result:
                if isinstance(item, str) and item.strip():
                    sentences.append(item.strip())
                elif item is not None:
                    logger.warning(f"KSS ê²°ê³¼ì— ë¬¸ìì—´ì´ ì•„ë‹Œ í•­ëª©: {type(item)} - {item}")
            
            logger.info(f"ğŸ” KSS ë¶„ë¦¬ ê²°ê³¼: {len(sentences)}ê°œ ë¬¸ì¥")
            logger.debug(f"KSS ì²« 3ê°œ ë¬¸ì¥ ìƒ˜í”Œ: {sentences[:3] if len(sentences) >= 3 else sentences}")
            return sentences
            
        except ImportError:
            logger.error("KSS 6.x ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            raise RuntimeError("KSS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install kssë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
                
        except Exception as e:
            logger.error(f"KSS 6.0.5 ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"KSS ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")

    def _split_sentences_kiwi(self, text: str, rules: ChunkingRules) -> List[str]:
        """Kiwië¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ë¶„ë¦¬"""
        from kiwipiepy import Kiwi
        
        # Kiwi ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì˜µì…˜ ì ìš©)
        kiwi_config = {}
        if rules.kiwi_model_path:
            kiwi_config['model_path'] = rules.kiwi_model_path
        if not rules.kiwi_load_default_dict:
            kiwi_config['load_default_dict'] = False
        if rules.kiwi_max_unk_form_len != 8:
            kiwi_config['max_unk_form_len'] = rules.kiwi_max_unk_form_len
        if not rules.kiwi_integrate_allomorph:
            kiwi_config['integrate_allomorph'] = False
        
        kiwi = Kiwi(**kiwi_config)
        result = kiwi.split_into_sents(text)
        return [sent.text for sent in result]

    def _split_sentences_regex(self, text: str, rules: ChunkingRules) -> List[str]:
        """ì •ê·œì‹ì„ ì‚¬ìš©í•œ ë¬¸ì¥ ë¶„ë¦¬ (ì‚¬ìš©ì ì •ì˜ ì˜µì…˜ ì ìš©)"""
        import re
        
        # ì‚¬ìš©ì ì •ì˜ íŒ¨í„´ ì ìš©
        pattern = rules.regex_sentence_endings
        
        # ì¤„ì„ë§ ë³´ì¡´ ì²˜ë¦¬
        if rules.regex_preserve_abbreviations:
            # ì¼ë°˜ì ì¸ ì¤„ì„ë§ ë³´ì¡´
            text = re.sub(r'\b(Dr|Mr|Mrs|Ms|Prof|etc|vs|Inc|Ltd|Co)\.',
                         r'\1ã€ˆDOTã€‰', text)
        
        # ì‚¬ìš©ì ì •ì˜ íŒ¨í„´ë“¤ ì ìš©
        for custom_pattern in rules.regex_custom_patterns:
            try:
                # ì‚¬ìš©ì íŒ¨í„´ì„ ã€ˆDOTã€‰ìœ¼ë¡œ ì„ì‹œ ì¹˜í™˜
                text = re.sub(custom_pattern + r'\.', 
                            custom_pattern.replace('.', 'ã€ˆDOTã€‰'), text)
            except re.error as e:
                logger.warning(f"ì˜ëª»ëœ ì‚¬ìš©ì ì •ì˜ íŒ¨í„´ ë¬´ì‹œ: {custom_pattern} - {e}")
        
        # ë¬¸ì¥ ì¢…ë£Œ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        try:
            sentences = re.split(pattern, text)
        except re.error as e:
            logger.error(f"ì˜ëª»ëœ ë¬¸ì¥ ì¢…ë£Œ íŒ¨í„´: {pattern} - {e}")
            # ê¸°ë³¸ íŒ¨í„´ìœ¼ë¡œ í´ë°±
            sentences = re.split(r'[.!?]', text)
        
        # ì›ë˜ ì  ë³µì› ë° ì •ë¦¬
        cleaned_sentences = []
        for sent in sentences:
            sent = sent.replace('ã€ˆDOTã€‰', '.').strip()
            if sent:
                cleaned_sentences.append(sent)
        
        logger.info(f"ğŸ” ì •ê·œì‹ ë¶„ë¦¬ ê²°ê³¼: {len(cleaned_sentences)}ê°œ ë¬¸ì¥")
        logger.debug(f"ì •ê·œì‹ ì²« 3ê°œ ë¬¸ì¥ ìƒ˜í”Œ: {cleaned_sentences[:3] if len(cleaned_sentences) >= 3 else cleaned_sentences}")
        return cleaned_sentences

    def _split_sentences_recursive(self, text: str, rules: ChunkingRules) -> List[str]:
        """RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ë¶„ë¦¬"""
        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
            
            # RecursiveCharacterTextSplitter ì„¤ì •
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=rules.max_tokens * 4,  # ëŒ€ëµì ì¸ character ìˆ˜ (í† í° * 4)
                chunk_overlap=rules.overlap_tokens * 4,  # ëŒ€ëµì ì¸ character ìˆ˜
                separators=rules.recursive_separators,
                keep_separator=rules.recursive_keep_separator,
                is_separator_regex=rules.recursive_is_separator_regex,
            )
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = text_splitter.split_text(text)
            
            # ë¹ˆ ì²­í¬ ì œê±°
            sentences = [chunk.strip() for chunk in chunks if chunk.strip()]
            
            logger.info(f"ğŸ” Recursive ë¶„ë¦¬ ê²°ê³¼: {len(sentences)}ê°œ ë¬¸ì¥")
            logger.debug(f"Recursive ì²« 3ê°œ ë¬¸ì¥ ìƒ˜í”Œ: {sentences[:3] if len(sentences) >= 3 else sentences}")
            return sentences
            
        except ImportError:
            logger.error("langchain_text_splitters ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            raise RuntimeError("langchain_text_splitters ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"RecursiveCharacterTextSplitter ì˜¤ë¥˜: {e}")
            raise RuntimeError(f"RecursiveCharacterTextSplitter ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
    
    
    def _is_heading(self, line: str) -> bool:
        """í—¤ë”©ì¸ì§€ íŒë‹¨"""
        return self._get_heading_info(line) is not None
    
    def _get_heading_info(self, line: str) -> Optional[Tuple[int, str]]:
        """í—¤ë”© ì •ë³´ ì¶”ì¶œ (ë ˆë²¨, í…ìŠ¤íŠ¸) - í”„ë¡œë•ì…˜ ê°œì„ """
        line_stripped = line.strip()
        if not line_stripped:
            return None
        
        for pattern, pattern_type in self.heading_patterns:
            match = pattern.match(line_stripped)
            if match:
                if pattern_type == 'markdown':
                    # ë§ˆí¬ë‹¤ìš´: # ## ### ê°œìˆ˜ê°€ ë ˆë²¨
                    level = len(match.group(1))
                    text = match.group(2).strip()
                    return (level, text)
                
                elif pattern_type == 'numbered':
                    # ë²ˆí˜¸: ì ì˜ ê°œìˆ˜ë¡œ ë ˆë²¨ ê²°ì • (1=1, 1.1=2, 1.1.1=3)
                    numbering = match.group(1)
                    level = numbering.count('.') + 1
                    text = match.group(2).strip()
                    return (min(level, 6), text)  # ìµœëŒ€ 6ë ˆë²¨
                
                elif pattern_type == 'korean':
                    # í•œêµ­ì–´: ê°€ë‚˜ë‹¤ ìˆœì„œë¡œ ë ˆë²¨ ì¶”ì •
                    char = match.group(1)
                    korean_order = ['ê°€', 'ë‚˜', 'ë‹¤', 'ë¼', 'ë§ˆ', 'ë°”']
                    if char in korean_order:
                        level = korean_order.index(char) + 1
                    else:
                        level = 1
                    text = match.group(2).strip() if len(match.groups()) > 1 else char
                    return (min(level, 6), text)
                
                elif pattern_type == 'caps':
                    # ëŒ€ë¬¸ì: ê¸°ë³¸ 1ë ˆë²¨
                    text = match.group(1).strip()
                    return (1, text)
        
        return None
    
    def _is_list_item(self, line: str) -> bool:
        """ëª©ë¡ ì•„ì´í…œì¸ì§€ íŒë‹¨"""
        return any(pattern.match(line) for pattern in self.list_patterns)
    
    def _is_table_content(self, line: str) -> bool:
        """í‘œ ë‚´ìš©ì¸ì§€ íŒë‹¨"""
        return bool(self.table_pattern.search(line))
    
    def split_into_sentences_with_images(self, text: str, rules: ChunkingRules, pdf_path: Optional[str] = None) -> List[SentenceInfo]:
        """ì´ë¯¸ì§€ ì •ë³´ë¥¼ í¬í•¨í•œ ë¬¸ì¥ ë¶„í•  (í”„ë¡œë•ì…˜ ê°œì„ )"""
        # ê¸°ë³¸ ë¬¸ì¥ ë¶„í• 
        sentences = self.split_into_sentences(text, rules)
        
        # PDF ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë§¤ì¹­
        if pdf_path and sentences:
            try:
                images = self.pdf_image_extractor.extract_images_from_pdf(pdf_path)
                if images:
                    self._attach_images_to_sentences(sentences, images)
                    logger.info(f"{len(images)}ê°œ ì´ë¯¸ì§€ì™€ {len(sentences)}ê°œ ë¬¸ì¥ ë§¤ì¹­ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
        
        return sentences
    
    def _attach_images_to_sentences(self, sentences: List[SentenceInfo], images: List[ImageRef], max_distance: float = 100.0):
        """ë¬¸ì¥ì— ê·¼ì ‘ ì´ë¯¸ì§€ ì—°ê²° (í”„ë¡œë•ì…˜ ê°œì„ )"""
        if not sentences or not images:
            return
        
        # ë¬¸ì¥ bboxê°€ ì—†ëŠ” ê²½ìš° ì¶”ì • (ë‹¨ìˆœí™”ëœ êµ¬í˜„)
        self._estimate_sentence_bboxes(sentences)
        
        # ê° ë¬¸ì¥ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ ì´ë¯¸ì§€ë“¤ ì°¾ê¸°
        for sentence in sentences:
            if not sentence.bbox:
                continue
                
            nearby_images = []
            for image in images:
                # ê°™ì€ í˜ì´ì§€ì˜ ì´ë¯¸ì§€ë§Œ ê³ ë ¤
                if sentence.bbox.page == image.bbox.page:
                    distance = sentence.bbox.vertical_distance_to(image.bbox)
                    if distance <= max_distance:
                        # ê±°ë¦¬ ì •ë³´ ì¶”ê°€
                        image_copy = ImageRef(
                            image_id=image.image_id,
                            bbox=image.bbox,
                            image_type=image.image_type,
                            description=image.description,
                            distance_to_text=distance
                        )
                        nearby_images.append(image_copy)
            
            # ê±°ë¦¬ ìˆœìœ¼ë¡œ ì •ë ¬
            nearby_images.sort(key=lambda img: img.distance_to_text)
            sentence.image_refs = nearby_images[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
    
    def _estimate_sentence_bboxes(self, sentences: List[SentenceInfo]):
        """ë¬¸ì¥ bbox ì¶”ì • (ë‹¨ìˆœí™”ëœ êµ¬í˜„ - í”„ë¡œë•ì…˜ ê°œì„ )"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” PDF íŒŒì‹± ì‹œ ë‹¨ì–´ bboxë¥¼ ìˆ˜ì§‘í•´ì„œ ë¬¸ì¥ bboxë¥¼ ê³„ì‚°í•´ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” í˜ì´ì§€ë³„ë¡œ ê· ë“± ë¶„ë°°í•˜ëŠ” ë‹¨ìˆœí•œ ë°©ì‹ ì‚¬ìš©
        
        current_page = 0
        page_sentences = []
        
        for sentence in sentences:
            if sentence.page is None:
                sentence.page = current_page
            
            if sentence.page != current_page:
                # ì´ì „ í˜ì´ì§€ ë¬¸ì¥ë“¤ì˜ bbox ì¶”ì •
                if page_sentences:
                    self._distribute_sentences_on_page(page_sentences, current_page)
                
                current_page = sentence.page
                page_sentences = [sentence]
            else:
                page_sentences.append(sentence)
        
        # ë§ˆì§€ë§‰ í˜ì´ì§€ ì²˜ë¦¬
        if page_sentences:
            self._distribute_sentences_on_page(page_sentences, current_page)
    
    def _distribute_sentences_on_page(self, sentences: List[SentenceInfo], page_num: int, page_width: float = 595, page_height: float = 842):
        """í˜ì´ì§€ ë‚´ì—ì„œ ë¬¸ì¥ë“¤ì„ ê· ë“± ë¶„ë°° (ë‹¨ìˆœí™”ëœ êµ¬í˜„)"""
        if not sentences:
            return
        
        # í˜ì´ì§€ ì—¬ë°± ê³ ë ¤
        margin_x, margin_y = 50, 50
        content_width = page_width - 2 * margin_x
        content_height = page_height - 2 * margin_y
        
        # ê° ë¬¸ì¥ì˜ ì„¸ë¡œ ìœ„ì¹˜ ê³„ì‚°
        sentence_height = content_height / len(sentences)
        
        for i, sentence in enumerate(sentences):
            y_top = page_height - margin_y - (i * sentence_height)
            y_bottom = y_top - sentence_height
            
            sentence.bbox = BBox(
                x0=margin_x,
                y0=y_bottom,
                x1=margin_x + content_width,
                y1=y_top,
                page=page_num
            )


class SmartChunkingService:
    """PRD ë°©ì‹ì˜ ì§€ëŠ¥í˜• ì²­í‚¹ ì„œë¹„ìŠ¤ (í”„ë¡œë•ì…˜ ê°œì„ )"""
    
    def __init__(self):
        self.text_splitter = SmartTextSplitter()
        self.token_counter = TokenCounter()
    
    def _get_fallback_settings(self) -> Dict[str, Any]:
        """í´ë°± ì œì–´ ì„¤ì • ì¡°íšŒ"""
        try:
            from .settings_service import settings_service
            return settings_service.get_section_settings("fallback_control")
        except Exception as e:
            logger.warning(f"í´ë°± ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {e} - ê¸°ë³¸ê°’(ì—„ê²© ëª¨ë“œ) ì‚¬ìš©")
            return {
                "enable_similarity_fallback": False,
                "enable_sentence_splitter_fallback": False,
                "enable_token_counter_fallback": False,
                "enable_pdf_extraction_fallback": False,
                "strict_mode": True
            }
    
    def propose_chunks_hierarchical(self, full_text: str, rules: ChunkingRules, pdf_path: Optional[str] = None) -> List[ChunkProposal]:
        """ê³„ì¸µì  í—¤ë”© ê¸°ë°˜ ì²­í‚¹ ì œì•ˆ (í”„ë¡œë•ì…˜ ê°œì„ )"""
        try:
            logger.info(f"ê³„ì¸µì  ì²­í‚¹ ì œì•ˆ ì‹œì‘ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text)}")
            
            # 1. ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  (í—¤ë”© ê²½ë¡œ + ì´ë¯¸ì§€ ì •ë³´ í¬í•¨ - í”„ë¡œë•ì…˜ ê°œì„ )
            if pdf_path:
                sentences = self.text_splitter.split_into_sentences_with_images(full_text, rules, pdf_path)
                logger.info("ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì—°ê´€ì„± í¬í•¨ ê³„ì¸µì  ëª¨ë“œë¡œ ë¬¸ì¥ ë¶„í• ")
            else:
                sentences = self.text_splitter.split_into_sentences(full_text, rules)
            
            if not sentences:
                return []
            
            logger.info(f"ë¬¸ì¥ ë¶„í•  ì™„ë£Œ - {len(sentences)}ê°œ ë¬¸ì¥")
            
            # 2. í—¤ë”© ê¸°ë°˜ ì„¹ì…˜ ë¶„í• 
            sections = self._group_by_headings(sentences)
            logger.info(f"í—¤ë”© ê¸°ë°˜ ì„¹ì…˜ ë¶„í•  ì™„ë£Œ - {len(sections)}ê°œ ì„¹ì…˜")
            
            # 3. ê° ì„¹ì…˜ ë‚´ì—ì„œ í† í° ê¸°ë°˜ ì²­í‚¹
            all_proposals = []
            chunk_order = 1
            
            for section in sections:
                section_proposals = self._chunk_section(section, rules, chunk_order)
                all_proposals.extend(section_proposals)
                chunk_order += len(section_proposals)
            
            # 4. ì „ì²´ ì²­í¬ ê°„ ì¤‘ë³µ ê²€ì‚¬
            duplicate_warnings = self.check_duplicate_chunks(all_proposals)
            for warning in duplicate_warnings:
                for proposal in all_proposals[:-1]:
                    if "ì—°ì† ì²­í¬" in warning.message:
                        proposal.quality_warnings.append(warning)
                        break
            
            logger.info(f"ê³„ì¸µì  ì²­í‚¹ ì œì•ˆ ì™„ë£Œ - {len(all_proposals)}ê°œ ì²­í¬ ìƒì„±")
            return all_proposals
            
        except Exception as e:
            logger.error(f"ê³„ì¸µì  ì²­í‚¹ ì œì•ˆ ì‹¤íŒ¨: {e}")
            raise
    
    def _group_by_headings(self, sentences: List[SentenceInfo]) -> List[Dict[str, Any]]:
        """í—¤ë”© ê¸°ë°˜ ì„¹ì…˜ ê·¸ë£¹í•‘ (í”„ë¡œë•ì…˜ ê°œì„ )"""
        sections = []
        current_section = {
            "heading": None,
            "heading_level": None,
            "heading_path": [],
            "sentences": [],
            "start_index": 0
        }
        
        for i, sentence in enumerate(sentences):
            if sentence.is_heading:
                # ì´ì „ ì„¹ì…˜ ì™„ë£Œ
                if current_section["sentences"]:
                    current_section["end_index"] = i - 1
                    sections.append(current_section.copy())
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section = {
                    "heading": sentence.text,
                    "heading_level": sentence.heading_level,
                    "heading_path": sentence.heading_path or [],
                    "sentences": [sentence],
                    "start_index": i
                }
            else:
                current_section["sentences"].append(sentence)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€
        if current_section["sentences"]:
            current_section["end_index"] = len(sentences) - 1
            sections.append(current_section)
        
        return sections
    
    def _chunk_section(self, section: Dict[str, Any], rules: ChunkingRules, start_order: int) -> List[ChunkProposal]:
        """ì„¹ì…˜ ë‚´ í† í° ê¸°ë°˜ ì²­í‚¹ (í”„ë¡œë•ì…˜ ê°œì„ )"""
        sentences = section["sentences"]
        if not sentences:
            return []
        
        # ì„¹ì…˜ì´ ì‘ìœ¼ë©´ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
        total_tokens = sum(s.tokens for s in sentences)
        if total_tokens <= rules.max_tokens:
            chunk = self._create_section_chunk(section, start_order, rules)
            return [chunk] if chunk else []
        
        # í° ì„¹ì…˜ì€ í† í° ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
        chunk_groups = self._group_by_tokens(sentences, rules)
        proposals = []
        
        for i, group in enumerate(chunk_groups):
            # ì„¹ì…˜ ì •ë³´ ì¶”ê°€
            group["section_heading"] = section["heading"]
            group["section_heading_path"] = section["heading_path"]
            chunk = self._create_chunk_proposal(group, start_order + i, rules)
            proposals.append(chunk)
        
        return proposals
    
    def _create_section_chunk(self, section: Dict[str, Any], order: int, rules: ChunkingRules) -> Optional[ChunkProposal]:
        """ì„¹ì…˜ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ìƒì„± (í”„ë¡œë•ì…˜ ê°œì„ )"""
        sentences = section["sentences"]
        if not sentences:
            return None
        
        text = " ".join(s.text for s in sentences)
        total_tokens = sum(s.tokens for s in sentences)
        
        # í’ˆì§ˆ ê²€ì‚¬
        group = {
            "sentences": sentences,
            "total_tokens": total_tokens,
            "has_heading": any(s.is_heading for s in sentences),
            "has_table": any(s.is_table_content for s in sentences),
            "has_list": any(s.is_list_item for s in sentences)
        }
        warnings = self._check_chunk_quality(group, rules)
        
        # í—¤ë”© ê²½ë¡œ (ì„¹ì…˜ í—¤ë”© í¬í•¨)
        heading_path = section["heading_path"].copy()
        if section["heading"]:
            heading_path.append(section["heading"])
        
        # ì´ë¯¸ì§€ ì°¸ì¡° í†µí•© (í”„ë¡œë•ì…˜ ê°œì„ )
        image_refs = self._consolidate_image_refs(sentences)
        
        # í˜ì´ì§€ ë²”ìœ„ ê³„ì‚°
        pages = [s.page for s in sentences if s.page is not None]
        page_start = min(pages) if pages else None
        page_end = max(pages) if pages else None
        
        return ChunkProposal(
            chunk_id=str(uuid.uuid4()),
            order=order,
            text=text,
            token_estimate=total_tokens,
            page_start=page_start,
            page_end=page_end,
            heading_path=heading_path if heading_path else None,
            sentences=sentences,
            quality_warnings=warnings,
            image_refs=image_refs
        )
    
    def propose_chunks(self, full_text: str, rules: ChunkingRules, use_hierarchical: bool = True, pdf_path: Optional[str] = None) -> List[ChunkProposal]:
        """ìë™ ì²­í‚¹ ì œì•ˆ (PRD í•µì‹¬ ë¡œì§ - í”„ë¡œë•ì…˜ ê°œì„ )"""
        try:
            logger.info(f"ì²­í‚¹ ì œì•ˆ ì‹œì‘ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text)}, ê·œì¹™: {asdict(rules)}, ê³„ì¸µì : {use_hierarchical}")
            
            # í—¤ë”©ì´ ìˆê³  ê³„ì¸µì  ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
            if use_hierarchical and self._has_headings(full_text):
                logger.info("ê³„ì¸µì  í—¤ë”© ê¸°ë°˜ ì²­í‚¹ ëª¨ë“œ ì‚¬ìš©")
                return self.propose_chunks_hierarchical(full_text, rules, pdf_path)
            
            # ê¸°ì¡´ í† í° ê¸°ë°˜ ì²­í‚¹ ëª¨ë“œ
            logger.info("í† í° ê¸°ë°˜ ì²­í‚¹ ëª¨ë“œ ì‚¬ìš©")
            
            # 1. ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨ - í”„ë¡œë•ì…˜ ê°œì„ )
            if pdf_path:
                sentences = self.text_splitter.split_into_sentences_with_images(full_text, rules, pdf_path)
                logger.info("ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì—°ê´€ì„± í¬í•¨ ëª¨ë“œë¡œ ë¬¸ì¥ ë¶„í• ")
            else:
                sentences = self.text_splitter.split_into_sentences(full_text, rules)
            
            if not sentences:
                return []
            
            logger.info(f"ë¬¸ì¥ ë¶„í•  ì™„ë£Œ - {len(sentences)}ê°œ ë¬¸ì¥")
            
            # 2. í† í° ê¸°ë°˜ ê·¸ë£¹í•‘
            chunk_groups = self._group_by_tokens(sentences, rules)
            logger.info(f"í† í° ê¸°ë°˜ ê·¸ë£¹í•‘ ì™„ë£Œ - {len(chunk_groups)}ê°œ ê·¸ë£¹")
            
            # 3. í—¤ë”© ê²½ê³„ ì¡°ì • (ì˜µì…˜)
            if rules.respect_headings:
                chunk_groups = self._adjust_heading_boundaries(chunk_groups, rules)
                logger.info("í—¤ë”© ê²½ê³„ ì¡°ì • ì™„ë£Œ")
            
            # 4. êµ¬ì¡° ë³´ì¡´ (í‘œ, ëª©ë¡)
            if rules.preserve_tables or rules.preserve_lists:
                chunk_groups = self._preserve_structures(chunk_groups, rules)
                logger.info("êµ¬ì¡° ë³´ì¡´ ì¡°ì • ì™„ë£Œ")
            
            # 5. ì˜¤ë²„ë© ì ìš©
            if rules.overlap_tokens > 0:
                chunk_groups = self._apply_overlap(chunk_groups, rules)
                logger.info(f"ì˜¤ë²„ë© ì ìš© ì™„ë£Œ - {rules.overlap_tokens} í† í°")
            
            # 6. ì²­í¬ ì œì•ˆ ìƒì„±
            proposals = []
            for i, group in enumerate(chunk_groups):
                chunk = self._create_chunk_proposal(group, i + 1, rules)
                proposals.append(chunk)
            
            # 7. ì „ì²´ ì²­í¬ ê°„ ì¤‘ë³µ ê²€ì‚¬
            duplicate_warnings = self.check_duplicate_chunks(proposals)
            
            # ì¤‘ë³µ ê²½ê³ ë¥¼ í•´ë‹¹ ì²­í¬ì— ì¶”ê°€
            for warning in duplicate_warnings:
                # ë©”ì‹œì§€ì—ì„œ ì²­í¬ ìˆœì„œ ì¶”ì¶œí•˜ì—¬ í•´ë‹¹ ì²­í¬ì— ê²½ê³  ì¶”ê°€
                for i, proposal in enumerate(proposals[:-1]):  # ë§ˆì§€ë§‰ ì œì™¸
                    if f"ì—°ì† ì²­í¬" in warning.message:
                        proposal.quality_warnings.append(warning)
                        break
            
            logger.info(f"ì²­í‚¹ ì œì•ˆ ì™„ë£Œ - {len(proposals)}ê°œ ì²­í¬ ìƒì„±")
            return proposals
            
        except Exception as e:
            logger.error(f"ì²­í‚¹ ì œì•ˆ ì‹¤íŒ¨: {e}")
            raise
    
    def _has_headings(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì— í—¤ë”©ì´ ìˆëŠ”ì§€ í™•ì¸ (í”„ë¡œë•ì…˜ ê°œì„ )"""
        lines = text.split('\n')[:50]  # ì²˜ìŒ 50ì¤„ë§Œ í™•ì¸ (ì„±ëŠ¥ìƒ)
        heading_count = 0
        
        for line in lines:
            if self.text_splitter._is_heading(line.strip()):
                heading_count += 1
                if heading_count >= 2:  # 2ê°œ ì´ìƒì˜ í—¤ë”©ì´ ìˆìœ¼ë©´ ê³„ì¸µì  ëª¨ë“œ ì‚¬ìš©
                    return True
        
        return False
    
    def _consolidate_image_refs(self, sentences: List[SentenceInfo]) -> List[ImageRef]:
        """ë¬¸ì¥ë“¤ì˜ ì´ë¯¸ì§€ ì°¸ì¡° í†µí•© (í”„ë¡œë•ì…˜ ê°œì„ )"""
        if not sentences:
            return []
        
        # ëª¨ë“  ì´ë¯¸ì§€ ì°¸ì¡° ìˆ˜ì§‘
        all_image_refs = []
        for sentence in sentences:
            if sentence.image_refs:
                all_image_refs.extend(sentence.image_refs)
        
        if not all_image_refs:
            return []
        
        # ì´ë¯¸ì§€ ID ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ê°€ì¥ ê°€ê¹Œìš´ ê±°ë¦¬ ìš°ì„ )
        unique_images = {}
        for img_ref in all_image_refs:
            existing = unique_images.get(img_ref.image_id)
            if not existing or img_ref.distance_to_text < existing.distance_to_text:
                unique_images[img_ref.image_id] = img_ref
        
        # ê±°ë¦¬ìˆœìœ¼ë¡œ ì •ë ¬
        consolidated = list(unique_images.values())
        consolidated.sort(key=lambda img: img.distance_to_text)
        
        logger.debug(f"ì²­í¬ ì´ë¯¸ì§€ ì°¸ì¡° í†µí•©: {len(all_image_refs)}ê°œ â†’ {len(consolidated)}ê°œ")
        return consolidated
    
    def _group_by_tokens(self, sentences: List[SentenceInfo], rules: ChunkingRules) -> List[Dict[str, Any]]:
        """í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ë“¤ì„ ê·¸ë£¹í•‘ (PRD2: ê°•ì œ ë¶„ì ˆ í¬í•¨)"""
        groups = []
        current_group = {
            "sentences": [],
            "total_tokens": 0,
            "has_heading": False,
            "has_table": False,
            "has_list": False
        }
        
        for sentence in sentences:
            # PRD2: ê°•ì œ ë¶„ì ˆ - í•œ ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ìë™ ë¶„ì ˆ
            if sentence.tokens > rules.hard_sentence_max_tokens:
                logger.info(f"ê¸´ ë¬¸ì¥ ê°•ì œ ë¶„ì ˆ: {sentence.tokens} > {rules.hard_sentence_max_tokens} í† í°")
                
                # í˜„ì¬ ê·¸ë£¹ì´ ìˆìœ¼ë©´ ë¨¼ì € ì™„ë£Œ
                if current_group["sentences"]:
                    groups.append(current_group.copy())
                    current_group = {
                        "sentences": [],
                        "total_tokens": 0,
                        "has_heading": False,
                        "has_table": False,
                        "has_list": False
                    }
                
                # ê¸´ ë¬¸ì¥ì„ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë¶„í• 
                split_sentences = self._force_split_sentence(sentence, rules)
                
                # ë¶„í• ëœ ì¡°ê°ë“¤ì„ ê·¸ë£¹í•‘
                for split_sentence in split_sentences:
                    potential_tokens = current_group["total_tokens"] + split_sentence.tokens
                    
                    if potential_tokens > rules.max_tokens and current_group["sentences"]:
                        groups.append(current_group.copy())
                        current_group = {
                            "sentences": [split_sentence],
                            "total_tokens": split_sentence.tokens,
                            "has_heading": split_sentence.is_heading,
                            "has_table": split_sentence.is_table_content,
                            "has_list": split_sentence.is_list_item
                        }
                    else:
                        current_group["sentences"].append(split_sentence)
                        current_group["total_tokens"] = potential_tokens
                        if split_sentence.is_heading:
                            current_group["has_heading"] = True
                        if split_sentence.is_table_content:
                            current_group["has_table"] = True
                        if split_sentence.is_list_item:
                            current_group["has_list"] = True
                
                continue
            
            # ì¼ë°˜ì ì¸ ë¬¸ì¥ ì²˜ë¦¬
            potential_tokens = current_group["total_tokens"] + sentence.tokens
            
            # ìµœëŒ€ í† í° ì´ˆê³¼ ì‹œ í˜„ì¬ ê·¸ë£¹ ì™„ë£Œ
            if potential_tokens > rules.max_tokens and current_group["sentences"]:
                groups.append(current_group.copy())
                current_group = {
                    "sentences": [sentence],
                    "total_tokens": sentence.tokens,
                    "has_heading": sentence.is_heading,
                    "has_table": sentence.is_table_content,
                    "has_list": sentence.is_list_item
                }
            else:
                # í˜„ì¬ ê·¸ë£¹ì— ì¶”ê°€
                current_group["sentences"].append(sentence)
                current_group["total_tokens"] = potential_tokens
                if sentence.is_heading:
                    current_group["has_heading"] = True
                if sentence.is_table_content:
                    current_group["has_table"] = True
                if sentence.is_list_item:
                    current_group["has_list"] = True
        
        # ë§ˆì§€ë§‰ ê·¸ë£¹ ì¶”ê°€
        if current_group["sentences"]:
            groups.append(current_group)
        
        return groups
    
    def _adjust_heading_boundaries(self, groups: List[Dict[str, Any]], rules: ChunkingRules) -> List[Dict[str, Any]]:
        """í—¤ë”© ê²½ê³„ì—ì„œ ì²­í¬ ë¶„í•  ì¡°ì •"""
        adjusted_groups = []
        
        for group in groups:
            sentences = group["sentences"]
            
            # í—¤ë”©ì´ ì¤‘ê°„ì— ìˆìœ¼ë©´ ë¶„í• 
            split_points = []
            for i, sentence in enumerate(sentences):
                if sentence.is_heading and i > 0:  # ì²« ë²ˆì§¸ê°€ ì•„ë‹Œ í—¤ë”©
                    split_points.append(i)
            
            if split_points:
                # í—¤ë”© ìœ„ì¹˜ì—ì„œ ë¶„í• 
                start = 0
                for split_point in split_points:
                    if start < split_point:
                        new_group = self._create_group_from_sentences(sentences[start:split_point])
                        adjusted_groups.append(new_group)
                    start = split_point
                
                # ë§ˆì§€ë§‰ ë¶€ë¶„
                if start < len(sentences):
                    new_group = self._create_group_from_sentences(sentences[start:])
                    adjusted_groups.append(new_group)
            else:
                adjusted_groups.append(group)
        
        return adjusted_groups
    
    def _preserve_structures(self, groups: List[Dict[str, Any]], rules: ChunkingRules) -> List[Dict[str, Any]]:
        """í‘œì™€ ëª©ë¡ êµ¬ì¡° ë³´ì¡´"""
        # ê°„ë‹¨í•œ êµ¬í˜„: í‘œë‚˜ ëª©ë¡ì´ ë¶„í• ë˜ì§€ ì•Šë„ë¡ ì¸ì ‘ ê·¸ë£¹ê³¼ ë³‘í•©
        preserved_groups = []
        
        i = 0
        while i < len(groups):
            current_group = groups[i]
            
            # ë‹¤ìŒ ê·¸ë£¹ê³¼ ë³‘í•© ê°€ëŠ¥í•œì§€ ì²´í¬
            if (i + 1 < len(groups) and 
                (current_group["has_table"] or current_group["has_list"]) and
                current_group["total_tokens"] + groups[i + 1]["total_tokens"] <= rules.max_tokens * 1.2):
                
                # ë³‘í•©
                merged_group = self._merge_groups(current_group, groups[i + 1])
                preserved_groups.append(merged_group)
                i += 2  # ë‘ ê·¸ë£¹ì„ ê±´ë„ˆëœ€
            else:
                preserved_groups.append(current_group)
                i += 1
        
        return preserved_groups
    
    def _apply_overlap(self, groups: List[Dict[str, Any]], rules: ChunkingRules) -> List[Dict[str, Any]]:
        """ì˜¤ë²„ë© ì ìš©"""
        if len(groups) <= 1:
            return groups
        
        overlapped_groups = [groups[0]]  # ì²« ë²ˆì§¸ ê·¸ë£¹ì€ ê·¸ëŒ€ë¡œ
        
        for i in range(1, len(groups)):
            current_group = groups[i].copy()
            prev_group = groups[i - 1]
            
            # ì´ì „ ê·¸ë£¹ì—ì„œ ì˜¤ë²„ë©í•  ë¬¸ì¥ë“¤ ì¶”ì¶œ
            overlap_sentences = self._extract_overlap_sentences(
                prev_group["sentences"], 
                rules.overlap_tokens
            )
            
            if overlap_sentences:
                # ì˜¤ë²„ë© ë¬¸ì¥ë“¤ì„ í˜„ì¬ ê·¸ë£¹ ì•ì— ì¶”ê°€
                current_group["sentences"] = overlap_sentences + current_group["sentences"]
                current_group["total_tokens"] += sum(s.tokens for s in overlap_sentences)
            
            overlapped_groups.append(current_group)
        
        return overlapped_groups
    
    def _extract_overlap_sentences(self, sentences: List[SentenceInfo], target_tokens: int) -> List[SentenceInfo]:
        """ì˜¤ë²„ë©ìš© ë¬¸ì¥ë“¤ ì¶”ì¶œ (ë’¤ì—ì„œë¶€í„°)"""
        overlap_sentences = []
        current_tokens = 0
        
        for sentence in reversed(sentences):
            if current_tokens + sentence.tokens <= target_tokens:
                overlap_sentences.insert(0, sentence)
                current_tokens += sentence.tokens
            else:
                break
        
        return overlap_sentences
    
    def _create_group_from_sentences(self, sentences: List[SentenceInfo]) -> Dict[str, Any]:
        """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ê·¸ë£¹ ìƒì„±"""
        return {
            "sentences": sentences,
            "total_tokens": sum(s.tokens for s in sentences),
            "has_heading": any(s.is_heading for s in sentences),
            "has_table": any(s.is_table_content for s in sentences),
            "has_list": any(s.is_list_item for s in sentences)
        }
    
    def _merge_groups(self, group1: Dict[str, Any], group2: Dict[str, Any]) -> Dict[str, Any]:
        """ë‘ ê·¸ë£¹ ë³‘í•©"""
        return {
            "sentences": group1["sentences"] + group2["sentences"],
            "total_tokens": group1["total_tokens"] + group2["total_tokens"],
            "has_heading": group1["has_heading"] or group2["has_heading"],
            "has_table": group1["has_table"] or group2["has_table"],
            "has_list": group1["has_list"] or group2["has_list"]
        }
    
    def _create_chunk_proposal(self, group: Dict[str, Any], order: int, rules: ChunkingRules) -> ChunkProposal:
        """ê·¸ë£¹ìœ¼ë¡œë¶€í„° ì²­í¬ ì œì•ˆ ìƒì„±"""
        sentences = group["sentences"]
        text = " ".join(s.text for s in sentences)
        
        # í’ˆì§ˆ ê²€ì‚¬
        warnings = self._check_chunk_quality(group, rules)
        
        # í—¤ë”© ê²½ë¡œ ì¶”ì¶œ
        heading_path = [s.text for s in sentences if s.is_heading]
        
        # ì´ë¯¸ì§€ ì°¸ì¡° í†µí•© (í”„ë¡œë•ì…˜ ê°œì„ )
        image_refs = self._consolidate_image_refs(sentences)
        
        # í˜ì´ì§€ ë²”ìœ„ ê³„ì‚°
        pages = [s.page for s in sentences if s.page is not None]
        page_start = min(pages) if pages else None
        page_end = max(pages) if pages else None
        
        return ChunkProposal(
            chunk_id=str(uuid.uuid4()),
            order=order,
            text=text,
            token_estimate=group["total_tokens"],
            page_start=page_start,
            page_end=page_end,
            heading_path=heading_path if heading_path else None,
            sentences=sentences,
            quality_warnings=warnings,
            image_refs=image_refs
        )
    
    def _check_chunk_quality(self, group: Dict[str, Any], rules: ChunkingRules) -> List[QualityWarning]:
        """ì²­í¬ í’ˆì§ˆ ê²€ì‚¬"""
        warnings = []
        total_tokens = group["total_tokens"]
        sentences = group["sentences"]
        
        # ë„ˆë¬´ ê¸´ ì²­í¬
        if total_tokens > rules.max_tokens * 1.1:
            warnings.append(QualityWarning(
                issue_type=ChunkQualityIssue.TOO_LONG,
                severity="warning",
                message=f"ì²­í¬ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({total_tokens} > {rules.max_tokens} í† í°)",
                suggestion="ê²½ê³„ ì¡°ì • ê¶Œì¥ - ì²­í¬ë¥¼ ë¶„í• í•˜ê±°ë‚˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”"
            ))
        
        # ë„ˆë¬´ ì§§ì€ ì²­í¬ (í‘œ/ëª©ë¡ ì˜ˆì™¸ ê³ ë ¤)
        is_special_content = group.get("has_table", False) or group.get("has_list", False)
        if total_tokens < rules.min_tokens and not rules.drop_short_chunks and not is_special_content:
            warnings.append(QualityWarning(
                issue_type=ChunkQualityIssue.TOO_SHORT,
                severity="warning",
                message=f"ì²­í¬ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({total_tokens} < {rules.min_tokens} í† í°)",
                suggestion="ì¸ì ‘ ì²­í¬ì™€ ë³‘í•©í•˜ê±°ë‚˜ ìµœì†Œ í† í° ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”"
            ))
        
        # ë‚´ìš©ì´ ì—†ëŠ” ì²­í¬
        if not sentences or all(not s.text.strip() for s in sentences):
            warnings.append(QualityWarning(
                issue_type=ChunkQualityIssue.NO_CONTENT,
                severity="error",
                message="ì²­í¬ì— ìœ íš¨í•œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤",
                suggestion="ì´ ì²­í¬ë¥¼ ì œê±°í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì²­í¬ì™€ ë³‘í•©í•˜ì„¸ìš”"
            ))
        
        # í—¤ë”© ê²½ê³„ ìœ„ë°˜ ê²€ì‚¬
        has_heading = group.get("has_heading", False)
        if has_heading and len(sentences) > 1:
            # í—¤ë”©ì´ ì¤‘ê°„ì— ìˆëŠ”ì§€ í™•ì¸
            heading_positions = [i for i, s in enumerate(sentences) if s.is_heading]
            if heading_positions and (heading_positions[0] > 0 or heading_positions[-1] < len(sentences) - 1):
                warnings.append(QualityWarning(
                    issue_type=ChunkQualityIssue.HEADING_BOUNDARY,
                    severity="warning",
                    message="í—¤ë”© ê²½ê³„ ìœ„ë°˜ - í—¤ë”© ì§í›„/ì§ì „ì— ì²­í¬ ê²½ê³„ê°€ ì—†ìŒ",
                    suggestion="í—¤ë”© ê²½ê³„ ìŠ¤ëƒ… ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ê²½ê³„ë¥¼ ì¡°ì •í•˜ì„¸ìš”"
                ))
        
        # ê³ ë¦½ëœ ìº¡ì…˜ ê²€ì‚¬
        text_content = " ".join(s.text for s in sentences)
        if self._is_isolated_caption(text_content):
            warnings.append(QualityWarning(
                issue_type=ChunkQualityIssue.ISOLATED_CAPTION,
                severity="warning",
                message="í‘œ/ê·¸ë¦¼ ìº¡ì…˜ì´ ê³ ë¦½ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
                suggestion="ì¸ì ‘ ë¬¸ë‹¨ê³¼ ë³‘í•©ì„ ê¶Œì¥í•©ë‹ˆë‹¤"
            ))
        
        return warnings
    
    def _is_isolated_caption(self, text: str) -> bool:
        """ìº¡ì…˜ë§Œ ìˆëŠ” ì²­í¬ì¸ì§€ íŒë‹¨"""
        text_lower = text.lower().strip()
        
        # ìº¡ì…˜ íŒ¨í„´ë“¤
        caption_patterns = [
            r'^\s*(ê·¸ë¦¼|figure|fig\.?)\s*\d+',  # ê·¸ë¦¼ 1, Figure 1, Fig. 1
            r'^\s*(í‘œ|table|tab\.?)\s*\d+',     # í‘œ 1, Table 1, Tab. 1
            r'^\s*(ë„|chart)\s*\d+',            # ë„ 1, Chart 1
            r'^\s*<.*>$',                       # <ìº¡ì…˜ í…ìŠ¤íŠ¸>
        ]
        
        for pattern in caption_patterns:
            if re.match(pattern, text_lower):
                return True
        
        # ì§§ê³  ì„¤ëª…ì ì¸ í…ìŠ¤íŠ¸ (ìº¡ì…˜ ê°€ëŠ¥ì„±)
        if len(text.split()) < 10 and ('ì„¤ëª…' in text or 'ë³´ì—¬ì£¼' in text or 'ë‚˜íƒ€ë‚´' in text):
            return True
        
        return False
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í”„ë¡œë•ì…˜ ìˆ˜ì¤€ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (MinHash/SimHash ìš°ì„ , ì½”ì‚¬ì¸ í´ë°±)"""
        if not text1.strip() or not text2.strip():
            return 0.0
        
        try:
            # MinHash ì‹œë„ (ë” ë¹ ë¥´ê³  ì •í™•)
            return self._calculate_minhash_similarity(text1, text2)
        except:
            # í´ë°±: ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            return self._calculate_cosine_similarity(text1, text2)
    
    def _calculate_minhash_similarity(self, text1: str, text2: str) -> float:
        """MinHash ê¸°ë°˜ ìœ ì‚¬ë„ (í”„ë¡œë•ì…˜ìš©) - ì„¤ì • ê¸°ë°˜ í´ë°± ì œì–´"""
        try:
            from datasketch import MinHash
            
            # ë¬¸ì„œë¥¼ shingle ë‹¨ìœ„ë¡œ ë¶„í•  (3-gram)
            def get_shingles(text: str, k: int = 3) -> set:
                words = text.lower().split()
                if len(words) < k:
                    return {' '.join(words)}
                return {' '.join(words[i:i+k]) for i in range(len(words) - k + 1)}
            
            shingles1 = get_shingles(text1)
            shingles2 = get_shingles(text2)
            
            # MinHash ê°ì²´ ìƒì„±
            mh1, mh2 = MinHash(), MinHash()
            
            # ë¬¸ìì—´ì„ ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì¶”ê°€
            for shingle in shingles1:
                mh1.update(shingle.encode('utf8'))
            for shingle in shingles2:
                mh2.update(shingle.encode('utf8'))
            
            return mh1.jaccard(mh2)
            
        except ImportError:
            # ì„¤ì • ê¸°ë°˜ í´ë°± ì œì–´
            fallback_settings = self._get_fallback_settings()
            if fallback_settings.get("enable_similarity_fallback", False):
                logger.warning("datasketch ë¯¸ì„¤ì¹˜ - ì„¤ì •ì— ë”°ë¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í´ë°± ì‚¬ìš©")
                return self._calculate_cosine_similarity(text1, text2)
            else:
                error_msg = "datasketch íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install datasketchë¡œ ì„¤ì¹˜í•˜ê±°ë‚˜ ì„¤ì •ì—ì„œ í´ë°±ì„ í™œì„±í™”í•˜ì„¸ìš”."
                logger.error(error_msg)
                if fallback_settings.get("strict_mode", True):
                    raise RuntimeError(error_msg)
                return 0.0
        except Exception as e:
            # ì„¤ì • ê¸°ë°˜ í´ë°± ì œì–´
            fallback_settings = self._get_fallback_settings()
            if fallback_settings.get("enable_similarity_fallback", False):
                logger.error(f"MinHash ê³„ì‚° ì‹¤íŒ¨: {e} - ì„¤ì •ì— ë”°ë¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í´ë°± ì‚¬ìš©")
                return self._calculate_cosine_similarity(text1, text2)
            else:
                error_msg = f"MinHash ê³„ì‚° ì‹¤íŒ¨: {e}. ì„¤ì •ì—ì„œ í´ë°±ì„ í™œì„±í™”í•˜ê±°ë‚˜ datasketch íŒ¨í‚¤ì§€ë¥¼ ì¬ì„¤ì¹˜í•˜ì„¸ìš”."
                logger.error(error_msg)
                if fallback_settings.get("strict_mode", True):
                    raise RuntimeError(error_msg)
                return 0.0
    
    def _calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (í´ë°±)"""
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        words1 = Counter(text1.lower().split())
        words2 = Counter(text2.lower().split())
        
        # ê³µí†µ ë‹¨ì–´
        common_words = set(words1.keys()) & set(words2.keys())
        if not common_words:
            return 0.0
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        dot_product = sum(words1[word] * words2[word] for word in common_words)
        magnitude1 = math.sqrt(sum(count ** 2 for count in words1.values()))
        magnitude2 = math.sqrt(sum(count ** 2 for count in words2.values()))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def check_duplicate_chunks(self, chunks: List[ChunkProposal]) -> List[QualityWarning]:
        """ì²­í¬ ê°„ ì¤‘ë³µ ê²€ì‚¬ (PRD2: ë‹¨ì–´ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬ í¬í•¨)"""
        warnings = []
        
        for i in range(len(chunks) - 1):
            current_chunk = chunks[i]
            next_chunk = chunks[i + 1]
            
            # 1. ê¸°ì¡´ ì „ì²´ ìœ ì‚¬ë„ ê²€ì‚¬ (ì •ë°€ ê²€ì‚¬)
            similarity = self._calculate_text_similarity(current_chunk.text, next_chunk.text)
            
            # 2. PRD2: ë‹¨ì–´ ê¸°ë°˜ ê²¹ì¹¨ìœ¨ ê²€ì‚¬ (ë¹ ë¥¸ ê²€ì‚¬)
            word_overlap_ratio = self._calculate_word_overlap_ratio(current_chunk.text, next_chunk.text)
            
            # 3. ì¤‘ë³µ íŒì • ê¸°ì¤€ (ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ)
            high_similarity = similarity > 0.95
            high_word_overlap = word_overlap_ratio > 0.85
            
            if high_similarity or high_word_overlap:
                # ë” ë†’ì€ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê²½ê³  ë©”ì‹œì§€ ìƒì„±
                if high_similarity and similarity >= word_overlap_ratio:
                    main_score = similarity
                    method = "ì „ì²´ ìœ ì‚¬ë„"
                else:
                    main_score = word_overlap_ratio
                    method = "ë‹¨ì–´ ê²¹ì¹¨ìœ¨"
                
                warnings.append(QualityWarning(
                    issue_type=ChunkQualityIssue.DUPLICATE_CONTENT,
                    severity="warning",
                    message=f"ì—°ì† ì²­í¬ ê°„ ë†’ì€ ì¤‘ë³µ ê°ì§€ ({method}: {main_score:.2f})",
                    suggestion="ë³‘í•© ë˜ëŠ” í•˜ë‚˜ ì œê±°ë¥¼ ê³ ë ¤í•˜ì„¸ìš”"
                ))
        
        return warnings
    
    def _calculate_word_overlap_ratio(self, text1: str, text2: str) -> float:
        """PRD2: ë‹¨ì–´ ê¸°ë°˜ ê²¹ì¹¨ìœ¨ ê³„ì‚° (ë¹ ë¥¸ ì¤‘ë³µ ê²€ì‚¬)"""
        if not text1.strip() or not text2.strip():
            return 0.0
        
        try:
            # ë‹¨ì–´ ì§‘í•© ìƒì„± (ì†Œë¬¸ì ë³€í™˜, ì¤‘ë³µ ì œê±°)
            words1 = set(re.findall(r'\w+', text1.lower()))
            words2 = set(re.findall(r'\w+', text2.lower()))
            
            if not words1 or not words2:
                return 0.0
            
            # êµì§‘í•©ê³¼ í•©ì§‘í•© ê³„ì‚°
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            
            if union == 0:
                return 0.0
            
            # Jaccard ìœ ì‚¬ë„ (êµì§‘í•© / í•©ì§‘í•©)
            overlap_ratio = intersection / union
            
            logger.debug(f"ë‹¨ì–´ ê²¹ì¹¨ìœ¨ ê³„ì‚°: {intersection}/{union} = {overlap_ratio:.3f}")
            return overlap_ratio
            
        except Exception as e:
            logger.error(f"ë‹¨ì–´ ê²¹ì¹¨ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def merge_chunks(self, chunk1: ChunkProposal, chunk2: ChunkProposal, rules: ChunkingRules) -> ChunkProposal:
        """ë‘ ì²­í¬ ë³‘í•©"""
        # ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ë³‘í•©
        sentences1 = chunk1.sentences or []
        sentences2 = chunk2.sentences or []
        merged_sentences = sentences1 + sentences2
        
        # ë³‘í•©ëœ ê·¸ë£¹ ìƒì„±
        merged_group = self._create_group_from_sentences(merged_sentences)
        
        # ìƒˆë¡œìš´ ì²­í¬ ì œì•ˆ ìƒì„±
        merged_chunk = self._create_chunk_proposal(
            merged_group, 
            min(chunk1.order, chunk2.order), 
            rules
        )
        
        # í¸ì§‘ ë¡œê·¸ ì¶”ê°€ (í”„ë¡œë•ì…˜ ê°œì„ )
        merged_chunk.add_edit_log("merge", chunk_ids=[chunk1.chunk_id, chunk2.chunk_id])
        
        return merged_chunk
    
    def split_chunk(self, chunk: ChunkProposal, split_position: int, rules: ChunkingRules) -> Tuple[ChunkProposal, ChunkProposal]:
        """ì²­í¬ë¥¼ ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• """
        if not chunk.sentences or split_position <= 0 or split_position >= len(chunk.sentences):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ë¶„í•  ìœ„ì¹˜ì…ë‹ˆë‹¤")
        
        # ë¬¸ì¥ ë¶„í• 
        sentences1 = chunk.sentences[:split_position]
        sentences2 = chunk.sentences[split_position:]
        
        # ë‘ ê·¸ë£¹ ìƒì„±
        group1 = self._create_group_from_sentences(sentences1)
        group2 = self._create_group_from_sentences(sentences2)
        
        # ìƒˆë¡œìš´ ì²­í¬ ì œì•ˆë“¤ ìƒì„±
        chunk1 = self._create_chunk_proposal(group1, chunk.order, rules)
        chunk2 = self._create_chunk_proposal(group2, chunk.order + 1, rules)
        
        # í¸ì§‘ ë¡œê·¸ ì¶”ê°€ (í”„ë¡œë•ì…˜ ê°œì„ )
        chunk1.add_edit_log("split", original_chunk_id=chunk.chunk_id, split_position=split_position, part="first")
        chunk2.add_edit_log("split", original_chunk_id=chunk.chunk_id, split_position=split_position, part="second")
        
        return chunk1, chunk2
    
    def snap_to_heading_boundaries(self, chunks: List[ChunkProposal], rules: ChunkingRules) -> List[ChunkProposal]:
        """í—¤ë”© ê²½ê³„ ìœ„ë°˜ì„ ìë™ ìˆ˜ì • (í”„ë¡œë•ì…˜ ê°œì„ )"""
        if not chunks:
            return chunks
        
        corrected_chunks = []
        i = 0
        
        while i < len(chunks):
            current_chunk = chunks[i]
            
            # í—¤ë”© ê²½ê³„ ìœ„ë°˜ ê²€ì‚¬
            if self._has_heading_boundary_violation(current_chunk):
                logger.info(f"ì²­í¬ {current_chunk.order}ì˜ í—¤ë”© ê²½ê³„ ìœ„ë°˜ ìë™ ìˆ˜ì • ì‹œì‘")
                
                # í—¤ë”© ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì¬ë¶„í• 
                corrected = self._split_by_heading_boundaries(current_chunk, rules)
                corrected_chunks.extend(corrected)
                
                # ì²­í¬ ë²ˆí˜¸ ì¬ì •ë ¬
                for j, chunk in enumerate(corrected):
                    chunk.order = len(corrected_chunks) - len(corrected) + j + 1
                    chunk.add_edit_log("heading_boundary_snap", auto_correction=True)
                
                logger.info(f"í—¤ë”© ê²½ê³„ ìˆ˜ì • ì™„ë£Œ: 1ê°œ â†’ {len(corrected)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
            else:
                corrected_chunks.append(current_chunk)
            
            i += 1
        
        return corrected_chunks
    
    def _has_heading_boundary_violation(self, chunk: ChunkProposal) -> bool:
        """í—¤ë”© ê²½ê³„ ìœ„ë°˜ ì—¬ë¶€ ê²€ì‚¬ (í”„ë¡œë•ì…˜ ê°œì„ )"""
        if not chunk.sentences or len(chunk.sentences) <= 1:
            return False
        
        heading_positions = [i for i, s in enumerate(chunk.sentences) if s.is_heading]
        if not heading_positions:
            return False
        
        # ì²« ë²ˆì§¸ í—¤ë”©ì´ ì²« ë¬¸ì¥ì´ ì•„ë‹ˆê±°ë‚˜, ë§ˆì§€ë§‰ í—¤ë”©ì´ ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì•„ë‹˜
        return (heading_positions[0] > 0 or heading_positions[-1] < len(chunk.sentences) - 1)
    
    def _split_by_heading_boundaries(self, chunk: ChunkProposal, rules: ChunkingRules) -> List[ChunkProposal]:
        """í—¤ë”© ê²½ê³„ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ì¬ë¶„í•  (í”„ë¡œë•ì…˜ ê°œì„ )"""
        if not chunk.sentences:
            return [chunk]
        
        # í—¤ë”© ìœ„ì¹˜ ì°¾ê¸°
        heading_positions = [i for i, s in enumerate(chunk.sentences) if s.is_heading]
        if not heading_positions:
            return [chunk]
        
        # ë¶„í•  ì§€ì  ê²°ì • (ê° í—¤ë”© ì§ì „ì—ì„œ ë¶„í• )
        split_points = []
        for pos in heading_positions:
            if pos > 0:  # ì²« ë²ˆì§¸ í—¤ë”©ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                split_points.append(pos)
        
        if not split_points:
            return [chunk]
        
        # ì‹¤ì œ ë¶„í•  ìˆ˜í–‰
        chunks = []
        start = 0
        
        for split_point in split_points:
            if start < split_point:
                section_sentences = chunk.sentences[start:split_point]
                section_chunk = self._create_chunk_from_sentences(section_sentences, chunk.order, rules)
                chunks.append(section_chunk)
            start = split_point
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜
        if start < len(chunk.sentences):
            section_sentences = chunk.sentences[start:]
            section_chunk = self._create_chunk_from_sentences(section_sentences, chunk.order, rules)
            chunks.append(section_chunk)
        
        return chunks
    
    def _force_split_sentence(self, sentence: SentenceInfo, rules: ChunkingRules) -> List[SentenceInfo]:
        """PRD2: ê¸´ ë¬¸ì¥ì„ ê°•ì œ ë¶„ì ˆ (ê³µë°± ê¸°ì¤€ në“±ë¶„)"""
        if sentence.tokens <= rules.hard_sentence_max_tokens:
            return [sentence]
        
        try:
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
            words = sentence.text.split()
            if len(words) <= 1:
                # ë¶„í•  ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì›ë³¸ ë°˜í™˜ (URL, ê¸´ ë‹¨ì–´ ë“±)
                logger.warning(f"ë¶„í•  ë¶ˆê°€ëŠ¥í•œ ê¸´ ë¬¸ì¥: {sentence.text[:100]}...")
                return [sentence]
            
            # í•„ìš”í•œ ë¶„í•  ìˆ˜ ê³„ì‚°
            approx_splits = max(1, sentence.tokens // rules.hard_sentence_max_tokens)
            words_per_split = max(1, len(words) // (approx_splits + 1))
            
            logger.debug(f"ë¬¸ì¥ ë¶„ì ˆ: {len(words)}ê°œ ë‹¨ì–´ë¥¼ {approx_splits + 1}ê°œ ì¡°ê°ìœ¼ë¡œ ë¶„í•  (ì¡°ê°ë‹¹ ~{words_per_split}ê°œ ë‹¨ì–´)")
            
            split_sentences = []
            current_words = []
            
            for i, word in enumerate(words):
                current_words.append(word)
                
                # ì¡°ê° ì™„ì„± ì¡°ê±´: ì§€ì •ëœ ë‹¨ì–´ ìˆ˜ì— ë„ë‹¬í•˜ê±°ë‚˜ ë§ˆì§€ë§‰ ë‹¨ì–´
                if (len(current_words) >= words_per_split and i < len(words) - 1) or i == len(words) - 1:
                    segment_text = " ".join(current_words).strip()
                    
                    if segment_text:
                        # ìƒˆë¡œìš´ SentenceInfo ìƒì„± (ê¸°ë³¸ ì†ì„± ìœ ì§€)
                        segment_tokens = self.token_counter.count_tokens(segment_text)
                        
                        split_sentence = SentenceInfo(
                            text=segment_text,
                            tokens=segment_tokens,
                            page=sentence.page,
                            is_heading=sentence.is_heading and len(split_sentences) == 0,  # ì²« ì¡°ê°ë§Œ í—¤ë”© ìœ ì§€
                            is_list_item=sentence.is_list_item,
                            is_table_content=sentence.is_table_content,
                            index=sentence.index + len(split_sentences),  # ì¸ë±ìŠ¤ ì¡°ì •
                            heading_level=sentence.heading_level if len(split_sentences) == 0 else None,
                            heading_path=sentence.heading_path,
                            bbox=sentence.bbox,  # bboxëŠ” ì›ë³¸ ìœ ì§€ (ê·¼ì‚¬ì¹˜)
                            image_refs=sentence.image_refs if len(split_sentences) == 0 else []  # ì²« ì¡°ê°ë§Œ ì´ë¯¸ì§€ ìœ ì§€
                        )
                        
                        split_sentences.append(split_sentence)
                        current_words = []
            
            logger.info(f"ë¬¸ì¥ ê°•ì œ ë¶„ì ˆ ì™„ë£Œ: 1ê°œ â†’ {len(split_sentences)}ê°œ ì¡°ê°")
            return split_sentences
            
        except Exception as e:
            logger.error(f"ë¬¸ì¥ ê°•ì œ ë¶„ì ˆ ì‹¤íŒ¨: {e} - ì›ë³¸ ë°˜í™˜")
            return [sentence]
    
    def _create_chunk_from_sentences(self, sentences: List[SentenceInfo], base_order: int, rules: ChunkingRules) -> ChunkProposal:
        """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ìƒˆ ì²­í¬ ìƒì„± (í”„ë¡œë•ì…˜ ê°œì„ )"""
        if not sentences:
            raise ValueError("ë¹ˆ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œëŠ” ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        group = self._create_group_from_sentences(sentences)
        return self._create_chunk_proposal(group, base_order, rules)


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
chunking_service = SmartChunkingService()