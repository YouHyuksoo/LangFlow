"""
ê³ ì„±ëŠ¥ ë³‘ë ¬ ë²¡í„°í™” ì„œë¹„ìŠ¤
- ë³‘ë ¬ ì²­í¬ ì²˜ë¦¬
- ìŠ¤íŠ¸ë¦¬ë° ì„ë² ë”© ìƒì„±
- ì—°ê²° í’€ë§
- ìºì‹± ì‹œìŠ¤í…œ
"""
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Dict, Any, Optional, AsyncGenerator, Tuple
from dataclasses import dataclass
from queue import Queue
import threading
from functools import lru_cache
import logging

from ..core.config import settings
from .vector_service import VectorService, _create_embedding_function
from .cache_manager import get_cache_manager


@dataclass
class ChunkBatch:
    """ì²­í¬ ë°°ì¹˜ ë°ì´í„° í´ë˜ìŠ¤"""
    chunks: List[str]
    start_index: int
    batch_id: int
    metadata: Dict[str, Any]


@dataclass
class EmbeddingResult:
    """ì„ë² ë”© ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    embeddings: List[List[float]]
    batch_id: int
    processing_time: float
    success: bool
    error: Optional[str] = None


class ParallelVectorService:
    """ê³ ì„±ëŠ¥ ë³‘ë ¬ ë²¡í„°í™” ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.base_service = VectorService()
        
        # ë™ì  ì„±ëŠ¥ ì„¤ì • ë¡œë“œ
        self._load_performance_settings()
        
        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´
        self.embedding_semaphore = asyncio.Semaphore(self.max_concurrent_embeddings)
        self.chunk_semaphore = asyncio.Semaphore(self.max_concurrent_chunks)
        
        # ì„ë² ë”© í•¨ìˆ˜ í’€
        self.embedding_pool = []
        self._embedding_pool_lock = threading.Lock()
        
        # ê³ ì„±ëŠ¥ ìºì‹œ ë§¤ë‹ˆì €
        self.cache_manager = get_cache_manager()
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "total_chunks_processed": 0,
            "total_embeddings_created": 0,
            "average_embedding_time": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        
        self.logger.info(f"ParallelVectorService ì´ˆê¸°í™” ì™„ë£Œ - ë™ì‹œ ì„ë² ë”©: {self.max_concurrent_embeddings}, ì²­í¬: {self.max_concurrent_chunks}")
    
    def _load_performance_settings(self):
        """ì‹œìŠ¤í…œ ì„¤ì •ì—ì„œ ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # ì‹œìŠ¤í…œ ì„¤ì • ë¡œë“œ ì‹œë„
            from ..api.settings import load_settings
            system_settings = load_settings()
            
            # ë²¡í„°í™” ì„±ëŠ¥ ì„¤ì •ì„ ëª¨ë¸ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸° (í†µí•© ì„¤ì • ì‚¬ìš©)
            try:
                from .model_settings_service import get_current_model_config
                model_config = await get_current_model_config()
                model_settings_config = model_config.get("settings", {})
                
                # ëª¨ë¸ ì„¤ì • ìš°ì„ , ì‹œìŠ¤í…œ ì„¤ì • í´ë°±, ê¸°ë³¸ê°’ ìµœì¢… í´ë°±
                self.max_concurrent_embeddings = model_settings_config.get("max_concurrent_embeddings", 
                                                  system_settings.get("maxConcurrentEmbeddings", settings.MAX_CONCURRENT_EMBEDDINGS))
                self.max_concurrent_chunks = model_settings_config.get("max_concurrent_chunks", 
                                            system_settings.get("maxConcurrentChunks", settings.MAX_CONCURRENT_CHUNKS))
                self.embedding_pool_size = model_settings_config.get("embedding_pool_size", 
                                          system_settings.get("embeddingPoolSize", settings.EMBEDDING_POOL_SIZE))
                self.connection_pool_size = model_settings_config.get("connection_pool_size", 
                                           system_settings.get("connectionPoolSize", settings.CONNECTION_POOL_SIZE))
            except:
                # í´ë°±: ì‹œìŠ¤í…œ ì„¤ì • ì‚¬ìš©
                self.max_concurrent_embeddings = system_settings.get("maxConcurrentEmbeddings", settings.MAX_CONCURRENT_EMBEDDINGS)
                self.max_concurrent_chunks = system_settings.get("maxConcurrentChunks", settings.MAX_CONCURRENT_CHUNKS)
                self.embedding_pool_size = system_settings.get("embeddingPoolSize", settings.EMBEDDING_POOL_SIZE)
                self.connection_pool_size = system_settings.get("connectionPoolSize", settings.CONNECTION_POOL_SIZE)
            
            # ì‹œìŠ¤í…œ ì „ë°˜ ì„±ëŠ¥ ì„¤ì • (í†µí•©í•˜ì§€ ì•ŠìŒ)
            self.chunk_buffer_size = system_settings.get("chunkStreamBufferSize", settings.CHUNK_STREAM_BUFFER_SIZE)
            self.cache_ttl_seconds = system_settings.get("cacheTtlSeconds", settings.CACHE_TTL_SECONDS)
            self.enable_parallel = system_settings.get("enableParallelProcessing", True)
            self.enable_streaming = system_settings.get("enableStreamingChunks", True)
            self.enable_caching = system_settings.get("enableSmartCaching", True)
            
            print(f"ğŸ”§ ë™ì  ì„±ëŠ¥ ì„¤ì • ì ìš©: ì„ë² ë”©={self.max_concurrent_embeddings}, ì²­í¬={self.max_concurrent_chunks}, ë³‘ë ¬={self.enable_parallel}")
            
        except Exception as e:
            # ì„¤ì • ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
            print(f"âš ï¸ ì„±ëŠ¥ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {str(e)}")
            self.max_concurrent_embeddings = settings.MAX_CONCURRENT_EMBEDDINGS
            self.max_concurrent_chunks = settings.MAX_CONCURRENT_CHUNKS
            self.embedding_pool_size = settings.EMBEDDING_POOL_SIZE
            self.chunk_buffer_size = settings.CHUNK_STREAM_BUFFER_SIZE
            self.connection_pool_size = settings.CONNECTION_POOL_SIZE
            self.cache_ttl_seconds = settings.CACHE_TTL_SECONDS
            self.enable_parallel = True
            self.enable_streaming = True
            self.enable_caching = True
    
    async def _get_embedding_function(self):
        """ì„ë² ë”© í•¨ìˆ˜ í’€ì—ì„œ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ì—°ê²° í’€ë§)"""
        with self._embedding_pool_lock:
            if len(self.embedding_pool) < self.embedding_pool_size:
                # ìƒˆ ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
                embedding_func = await _create_embedding_function()
                self.embedding_pool.append(embedding_func)
                self.logger.debug(f"ìƒˆ ì„ë² ë”© í•¨ìˆ˜ ìƒì„± - í’€ í¬ê¸°: {len(self.embedding_pool)}")
                return embedding_func
            else:
                # ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš© (ë¼ìš´ë“œ ë¡œë¹ˆ)
                return self.embedding_pool[len(self.embedding_pool) % self.embedding_pool_size]
    
    async def _create_embeddings_batch(self, chunks: List[str], batch_id: int) -> EmbeddingResult:
        """ë°°ì¹˜ë³„ ì„ë² ë”© ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)"""
        async with self.embedding_semaphore:
            start_time = time.time()
            
            try:
                # ê³ ì„±ëŠ¥ ìºì‹œ í™•ì¸
                cached_embeddings = self.cache_manager.get_cached_embedding(chunks)
                if cached_embeddings is not None:
                    self.stats["cache_hits"] += 1
                    self.logger.debug(f"ë°°ì¹˜ {batch_id} ìºì‹œ íˆíŠ¸")
                    return EmbeddingResult(
                        embeddings=cached_embeddings,
                        batch_id=batch_id,
                        processing_time=time.time() - start_time,
                        success=True
                    )
                else:
                    self.stats["cache_misses"] += 1
                
                # ì„ë² ë”© í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                embedding_func = await self._get_embedding_function()
                
                # ë³‘ë ¬ ì„ë² ë”© ìƒì„±
                embedding_tasks = []
                for chunk in chunks:
                    task = asyncio.create_task(self._create_single_embedding(embedding_func, chunk))
                    embedding_tasks.append(task)
                
                # ëª¨ë“  ì„ë² ë”© ì™„ë£Œ ëŒ€ê¸°
                embeddings = await asyncio.gather(*embedding_tasks, return_exceptions=True)
                
                # ì—ëŸ¬ í™•ì¸
                valid_embeddings = []
                for i, embedding in enumerate(embeddings):
                    if isinstance(embedding, Exception):
                        self.logger.error(f"ë°°ì¹˜ {batch_id} ì²­í¬ {i} ì„ë² ë”© ì‹¤íŒ¨: {embedding}")
                        return EmbeddingResult(
                            embeddings=[],
                            batch_id=batch_id,
                            processing_time=time.time() - start_time,
                            success=False,
                            error=str(embedding)
                        )
                    valid_embeddings.append(embedding)
                
                # ê³ ì„±ëŠ¥ ìºì‹œì— ì €ì¥
                self.cache_manager.cache_embedding(chunks, valid_embeddings)
                
                processing_time = time.time() - start_time
                self.stats["total_embeddings_created"] += len(valid_embeddings)
                self.stats["average_embedding_time"] = (
                    (self.stats["average_embedding_time"] * self.stats["total_chunks_processed"] + processing_time) /
                    (self.stats["total_chunks_processed"] + len(chunks))
                )
                self.stats["total_chunks_processed"] += len(chunks)
                
                self.logger.debug(f"ë°°ì¹˜ {batch_id} ì„ë² ë”© ì™„ë£Œ - {len(chunks)}ê°œ ì²­í¬, {processing_time:.2f}ì´ˆ")
                
                return EmbeddingResult(
                    embeddings=valid_embeddings,
                    batch_id=batch_id,
                    processing_time=processing_time,
                    success=True
                )
                
            except Exception as e:
                self.logger.error(f"ë°°ì¹˜ {batch_id} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                return EmbeddingResult(
                    embeddings=[],
                    batch_id=batch_id,
                    processing_time=time.time() - start_time,
                    success=False,
                    error=str(e)
                )
    
    async def _create_single_embedding(self, embedding_func, chunk: str) -> List[float]:
        """ë‹¨ì¼ ì²­í¬ ì„ë² ë”© ìƒì„±"""
        try:
            # í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_chunk = chunk.strip()
            if not normalized_chunk:
                return [0.0] * 1536  # OpenAI ì„ë² ë”© ì°¨ì›ìˆ˜ ê¸°ë³¸ê°’
            
            # ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°)
            embedding = await asyncio.to_thread(embedding_func, normalized_chunk)
            return embedding
            
        except Exception as e:
            self.logger.error(f"ë‹¨ì¼ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def _stream_chunk_batches(self, chunks: List[str], metadata: Dict[str, Any]) -> AsyncGenerator[ChunkBatch, None]:
        """ì²­í¬ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°°ì¹˜ ìƒì„±"""
        batch_size = settings.BATCH_SIZE
        total_chunks = len(chunks)
        
        self.logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì²˜ë¦¬ ì‹œì‘ - ì´ {total_chunks}ê°œ ì²­í¬, ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        for batch_id, start_idx in enumerate(range(0, total_chunks, batch_size)):
            end_idx = min(start_idx + batch_size, total_chunks)
            batch_chunks = chunks[start_idx:end_idx]
            
            yield ChunkBatch(
                chunks=batch_chunks,
                start_index=start_idx,
                batch_id=batch_id,
                metadata=metadata
            )
            
            # ë²„í¼ ì œì–´ë¥¼ ìœ„í•œ ì•½ê°„ì˜ ì§€ì—°
            if batch_id % self.chunk_buffer_size == 0:
                await asyncio.sleep(0.01)
    
    async def vectorize_document_parallel(
        self, 
        file_id: str, 
        chunks: List[str], 
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë³‘ë ¬ ë¬¸ì„œ ë²¡í„°í™” (ë©”ì¸ í•¨ìˆ˜)"""
        start_time = time.time()
        total_chunks = len(chunks)
        
        self.logger.info(f"ğŸš€ ë³‘ë ¬ ë²¡í„°í™” ì‹œì‘ - íŒŒì¼: {file_id}, ì²­í¬: {total_chunks}ê°œ")
        
        try:
            # ChromaDB ì—°ê²° í™•ì¸
            await self.base_service._ensure_client()
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸
            embedding_tasks = []
            batch_results = []
            
            # ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ë°°ì¹˜ ì²˜ë¦¬
            async for chunk_batch in self._stream_chunk_batches(chunks, metadata):
                # ë³‘ë ¬ ì„ë² ë”© ìƒì„± íƒœìŠ¤í¬ ì¶”ê°€
                task = asyncio.create_task(
                    self._create_embeddings_batch(chunk_batch.chunks, chunk_batch.batch_id)
                )
                embedding_tasks.append((task, chunk_batch))
                
                # ë™ì‹œ ì²˜ë¦¬ ì œí•œ í™•ì¸
                if len(embedding_tasks) >= self.max_concurrent_embeddings:
                    # ì™„ë£Œëœ íƒœìŠ¤í¬ë“¤ ì²˜ë¦¬
                    completed_tasks, embedding_tasks = await self._process_completed_tasks(embedding_tasks)
                    batch_results.extend(completed_tasks)
            
            # ë‚¨ì€ íƒœìŠ¤í¬ë“¤ ì™„ë£Œ ëŒ€ê¸°
            if embedding_tasks:
                completed_tasks, _ = await self._process_completed_tasks(embedding_tasks, wait_all=True)
                batch_results.extend(completed_tasks)
            
            # ê²°ê³¼ ê²€ì¦ ë° ChromaDB ì €ì¥
            successful_batches = [r for r in batch_results if r.success]
            failed_batches = [r for r in batch_results if not r.success]
            
            if failed_batches:
                self.logger.warning(f"ì‹¤íŒ¨í•œ ë°°ì¹˜: {len(failed_batches)}ê°œ")
                for failed in failed_batches:
                    self.logger.error(f"ë°°ì¹˜ {failed.batch_id} ì‹¤íŒ¨: {failed.error}")
            
            if not successful_batches:
                return {
                    "success": False,
                    "error": "ëª¨ë“  ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨",
                    "processing_time": time.time() - start_time
                }
            
            # ChromaDBì— ì €ì¥
            chromadb_start = time.time()
            success = await self._save_to_chromadb_parallel(file_id, successful_batches, chunks, metadata)
            chromadb_time = time.time() - chromadb_start
            
            total_time = time.time() - start_time
            successful_chunks = sum(len(batch.embeddings) for batch in successful_batches)
            
            result = {
                "success": success,
                "chunks_count": successful_chunks,
                "processing_method": "parallel_vectorization",
                "processing_time": total_time,
                "embedding_time": total_time - chromadb_time,
                "chromadb_time": chromadb_time,
                "failed_batches": len(failed_batches),
                "cache_hit_rate": self.stats["cache_hits"] / (self.stats["cache_hits"] + self.stats["cache_misses"]) if (self.stats["cache_hits"] + self.stats["cache_misses"]) > 0 else 0,
                "performance_stats": self.stats.copy()
            }
            
            if success:
                self.logger.info(f"âœ… ë³‘ë ¬ ë²¡í„°í™” ì™„ë£Œ - {successful_chunks}ê°œ ì²­í¬, {total_time:.2f}ì´ˆ (ì„ë² ë”©: {total_time - chromadb_time:.2f}ì´ˆ, DBì €ì¥: {chromadb_time:.2f}ì´ˆ)")
            else:
                self.logger.error(f"âŒ ë³‘ë ¬ ë²¡í„°í™” ì‹¤íŒ¨ - {total_time:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"ë³‘ë ¬ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    async def _process_completed_tasks(
        self, 
        embedding_tasks: List[Tuple[asyncio.Task, ChunkBatch]], 
        wait_all: bool = False
    ) -> Tuple[List[EmbeddingResult], List[Tuple[asyncio.Task, ChunkBatch]]]:
        """ì™„ë£Œëœ íƒœìŠ¤í¬ë“¤ ì²˜ë¦¬"""
        if wait_all:
            # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
            completed_results = []
            for task, chunk_batch in embedding_tasks:
                result = await task
                completed_results.append(result)
            return completed_results, []
        else:
            # ì™„ë£Œëœ íƒœìŠ¤í¬ë§Œ ì²˜ë¦¬
            completed_results = []
            remaining_tasks = []
            
            for task, chunk_batch in embedding_tasks:
                if task.done():
                    try:
                        result = await task
                        completed_results.append(result)
                    except Exception as e:
                        self.logger.error(f"íƒœìŠ¤í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        completed_results.append(EmbeddingResult(
                            embeddings=[],
                            batch_id=chunk_batch.batch_id,
                            processing_time=0,
                            success=False,
                            error=str(e)
                        ))
                else:
                    remaining_tasks.append((task, chunk_batch))
            
            return completed_results, remaining_tasks
    
    async def _save_to_chromadb_parallel(
        self, 
        file_id: str, 
        batch_results: List[EmbeddingResult], 
        original_chunks: List[str], 
        metadata: Dict[str, Any]
    ) -> bool:
        """ë³‘ë ¬ë¡œ ìƒì„±ëœ ì„ë² ë”©ì„ ChromaDBì— ì €ì¥"""
        try:
            # ë°°ì¹˜ ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            batch_results.sort(key=lambda x: x.batch_id)
            
            # ëª¨ë“  ì„ë² ë”©ê³¼ ì²­í¬ ì¬êµ¬ì„±
            all_embeddings = []
            batch_size = settings.BATCH_SIZE
            
            for batch_result in batch_results:
                all_embeddings.extend(batch_result.embeddings)
            
            # ChromaDBì— ì €ì¥ (ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©)
            return await self.base_service.add_document_chunks(file_id, original_chunks, metadata)
            
        except Exception as e:
            self.logger.error(f"ChromaDB ë³‘ë ¬ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        cache_stats = self.cache_manager.get_comprehensive_stats()
        return {
            **self.stats,
            "embedding_pool_size": len(self.embedding_pool),
            "efficiency_score": self.stats["cache_hits"] / max(1, self.stats["cache_hits"] + self.stats["cache_misses"]),
            "cache_manager_stats": cache_stats
        }
    
    def clear_cache(self):
        """ìºì‹œ ë¹„ìš°ê¸°"""
        self.cache_manager.clear_all_caches()
        self.logger.info("ìºì‹œê°€ ë¹„ì›Œì¡ŒìŠµë‹ˆë‹¤.")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_parallel_vector_service = None
_service_lock = threading.Lock()


def get_parallel_vector_service() -> ParallelVectorService:
    """ParallelVectorService ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _parallel_vector_service
    
    if _parallel_vector_service is None:
        with _service_lock:
            if _parallel_vector_service is None:
                _parallel_vector_service = ParallelVectorService()
    
    return _parallel_vector_service