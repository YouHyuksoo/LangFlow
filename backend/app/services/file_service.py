import os
import uuid
import aiofiles
import json
import time
import logging
import asyncio
from typing import List, Optional, Dict, Any
from fastapi import UploadFile, HTTPException
from ..models.schemas import FileUploadResponse, FileInfo, FileProcessingOptions, DoclingResult
from ..core.config import settings
from datetime import datetime
from .category_service import CategoryService
from ..api.settings import load_settings
# LangflowServiceëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬ (ìˆœí™˜ import ë°©ì§€)
# VectorServiceëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬

class FileService:
    def __init__(self):
        self.upload_dir = settings.UPLOAD_DIR
        self.max_file_size = settings.MAX_FILE_SIZE
        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(__name__)
        # ë™ì ìœ¼ë¡œ ì„¤ì •ì—ì„œ í—ˆìš© í™•ì¥ì ë¡œë“œ
        from ..api.settings import load_settings
        current_settings = load_settings()
        # ì„¤ì •ì—ì„œ í—ˆìš©ëœ íŒŒì¼ í˜•ì‹ì„ ê°€ì ¸ì™€ì„œ í™•ì¥ìë¡œ ë³€í™˜ (.pdf -> .pdf)
        allowed_file_types = current_settings.get("allowedFileTypes", ["pdf"])
        self.allowed_extensions = {f".{ext}" if not ext.startswith('.') else ext for ext in allowed_file_types}
        self.category_service = CategoryService()
        # ë²¡í„° ì„œë¹„ìŠ¤ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬ - ë²¡í„°í™” ê´€ë ¨ ì‘ì—…ì—ì„œë§Œ ì´ˆê¸°í™”
        self._vector_service = None
        # Docling ì„œë¹„ìŠ¤ëŠ” ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì²˜ë¦¬
        self._docling_service = None
        
        # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.upload_dir, exist_ok=True)
        
        # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ íŒŒì¼
        self.files_metadata_file = os.path.join(settings.DATA_DIR, "files_metadata.json")
        self._ensure_data_dir()
        self._load_files_metadata()
    
    @property
    def vector_service(self):
        """ë²¡í„° ì„œë¹„ìŠ¤ë¥¼ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤ (ë²¡í„°í™” ì‘ì—…ì—ì„œë§Œ ì´ˆê¸°í™”)."""
        if self._vector_service is None:
            print("VectorService ì§€ì—° ë¡œë”© ì´ˆê¸°í™” ì¤‘...")
            try:
                from .vector_service import VectorService
                self._vector_service = VectorService()
            except Exception as e:
                print(f"VectorService ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                # VectorService ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ
                # ëŒ€ì‹  Noneì„ ë°˜í™˜í•˜ì—¬ ìƒìœ„ì—ì„œ ì²˜ë¦¬
                self._vector_service = None
        return self._vector_service
    
    @property
    def docling_service(self):
        """Docling ì„œë¹„ìŠ¤ë¥¼ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if self._docling_service is None:
            print("DoclingService ì§€ì—° ë¡œë”© ì´ˆê¸°í™” ì¤‘...")
            try:
                from .docling_service import DoclingService
                self._docling_service = DoclingService()
            except Exception as e:
                print(f"DoclingService ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                self._docling_service = None
        return self._docling_service
    
    def _ensure_data_dir(self):
        """ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±"""
        data_dir = settings.DATA_DIR
        os.makedirs(data_dir, exist_ok=True)
    
    def _load_files_metadata(self):
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if os.path.exists(self.files_metadata_file):
            try:
                with open(self.files_metadata_file, 'r', encoding='utf-8') as f:
                    loaded_metadata = json.load(f)
                
                # ë¡œë“œëœ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                if isinstance(loaded_metadata, dict):
                    self.files_metadata = loaded_metadata
                elif isinstance(loaded_metadata, list):
                    print("files_metadataê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì…ë‹ˆë‹¤. ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                    self.files_metadata = {}
                else:
                    print(f"files_metadataì˜ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…: {type(loaded_metadata)}")
                    self.files_metadata = {}
            except Exception as e:
                print(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                self.files_metadata = {}
        else:
            self.files_metadata = {}
    
    def _save_files_metadata(self):
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            with open(self.files_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.files_metadata, f, ensure_ascii=False, indent=2, default=str)
        except Exception as e:
            print(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def reset_files_metadata(self) -> Dict[str, Any]:
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° JSONì„ ì™„ì „ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            removed_entries = len(getattr(self, 'files_metadata', {}) or {})
            # íŒŒì¼ ë‹«í˜ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ë¨¼ì € ë©”ëª¨ë¦¬ ë°ì´í„° ë¹„ì›€
            self.files_metadata = {}
            # íŒŒì¼ ìì²´ ì‚­ì œ ì‹œë„ (ì¡´ì¬í•˜ë©´)
            if os.path.exists(self.files_metadata_file):
                try:
                    os.remove(self.files_metadata_file)
                except Exception:
                    # ì‚­ì œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë‚´ìš©ìœ¼ë¡œ ë®ì–´ì“°ê¸°
                    self._save_files_metadata()
            else:
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ íŒŒì¼ë¡œ ìƒì„±í•´ ì¼ê´€ì„± ìœ ì§€
                self._save_files_metadata()
            return {"status": "success", "removed_entries": removed_entries}
        except Exception as e:
            print(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return {"status": "error", "error": str(e)}
    
    async def upload_file(self, file: UploadFile, category_id: Optional[str] = None, allow_global_duplicates: bool = False, force_replace: bool = False, processing_options: Optional[FileProcessingOptions] = None) -> FileUploadResponse:
        """íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë²¡í„°í™” ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ í™•ì¥ì ê²€ì¦
            file_extension = os.path.splitext(file.filename)[1].lower()
            if file_extension not in self.allowed_extensions:
                raise HTTPException(
                    status_code=400, 
                    detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(self.allowed_extensions)}"
                )
            
            # íŒŒì¼ í¬ê¸° ê²€ì¦ (ë™ì  ì„¤ì • ì‚¬ìš©)
            current_settings = load_settings()
            max_file_size_mb = current_settings.get("maxFileSize", 10)
            max_file_size_bytes = max_file_size_mb * 1024 * 1024
            
            if file.size and file.size > max_file_size_bytes:
                raise HTTPException(
                    status_code=400,
                    detail=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ í¬ê¸°: {max_file_size_mb}MB"
                )
            
            # ì¤‘ë³µ íŒŒì¼ ê²€ì¶œ
            content = await file.read()
            file_size = len(content)
            
            # íŒŒì¼ ë‚´ìš©ìœ¼ë¡œ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê²€ì¶œìš©)
            import hashlib
            file_hash = hashlib.md5(content).hexdigest()
            
            # ì¤‘ë³µ íŒŒì¼ ê²€ì¶œ - ì‚­ì œë˜ì§€ ì•Šì€ íŒŒì¼ë“¤ë§Œ í™•ì¸
            print(f"íŒŒì¼ ì¤‘ë³µ ê²€ì‚¬ ì‹œì‘: {file.filename} (í•´ì‹œ: {file_hash[:8]}...)")
            for existing_file_id, existing_file_data in self.files_metadata.items():
                # ì‚­ì œëœ íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
                if existing_file_data.get("status") == "deleted":
                    continue
                
                existing_hash = existing_file_data.get("file_hash")
                existing_filename = existing_file_data.get("filename")
                existing_category = existing_file_data.get("category_id")
                
                print(f"ê¸°ì¡´ íŒŒì¼ í™•ì¸: {existing_filename} (í•´ì‹œ: {existing_hash[:8] if existing_hash else 'None'}...)")
                
                # í•´ì‹œê°€ ê°™ì€ íŒŒì¼ ê²€ì¶œ (ì¹´í…Œê³ ë¦¬ ì¡°ê±´ í™•ì¸)
                if (existing_hash == file_hash and 
                    existing_hash is not None and  # í•´ì‹œê°€ ìˆëŠ” íŒŒì¼ë§Œ ë¹„êµ
                    (allow_global_duplicates or existing_category == category_id)):  # ì „ì—­ í—ˆìš© ë˜ëŠ” ê°™ì€ ì¹´í…Œê³ ë¦¬
                    
                    print(f"ì¤‘ë³µ íŒŒì¼ ë°œê²¬: {existing_filename} (ID: {existing_file_id})")
                    
                    if force_replace:
                        # ê°•ì œ êµì²´ ëª¨ë“œ: ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í›„ ìƒˆ íŒŒì¼ë¡œ êµì²´
                        print(f"ê¸°ì¡´ íŒŒì¼ êµì²´ ëª¨ë“œ: {existing_filename}")
                        await self.delete_file(existing_file_id)
                        # êµì²´ í›„ ìƒˆ íŒŒì¼ ì—…ë¡œë“œë¥¼ ê³„ì† ì§„í–‰í•˜ê¸° ìœ„í•´ continue
                        continue
                    else:
                        # ì¤‘ë³µ íŒŒì¼ ì •ë³´ì™€ í•¨ê»˜ ì‘ë‹µ ìƒì„±
                        raise HTTPException(
                            status_code=409,
                            detail={
                                "error": "duplicate_file",
                                "message": "ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
                                "existing_file": {
                                    "file_id": existing_file_id,
                                    "filename": existing_filename,
                                    "category_id": existing_category,
                                    "category_name": existing_file_data.get("category_name", ""),
                                },
                                "new_file": {
                                    "filename": file.filename,
                                    "size": file_size,
                                    "category_id": category_id,
                                }
                            }
                        )
                
                # íŒŒì¼ëª…ê³¼ í¬ê¸°ê°€ ë™ì¼í•˜ì§€ë§Œ í•´ì‹œê°€ ì—†ëŠ” ê¸°ì¡´ íŒŒì¼ì˜ ê²½ìš° (ì´ì „ ë²„ì „ í˜¸í™˜ì„±)
                elif (existing_filename == file.filename and 
                      existing_file_data.get("file_size") == file_size and
                      existing_category == category_id and
                      existing_hash is None):  # ê¸°ì¡´ íŒŒì¼ì— í•´ì‹œê°€ ì—†ëŠ” ê²½ìš°
                    
                    print(f"í•´ì‹œ ì—†ëŠ” ê¸°ì¡´ íŒŒì¼ ë°œê²¬: {existing_filename}")
                    
                    if force_replace:
                        # ê°•ì œ êµì²´ ëª¨ë“œ: ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í›„ ìƒˆ íŒŒì¼ë¡œ êµì²´
                        print(f"ê¸°ì¡´ íŒŒì¼ êµì²´ ëª¨ë“œ (í•´ì‹œ ì—†ìŒ): {existing_filename}")
                        await self.delete_file(existing_file_id)
                        # êµì²´ í›„ ìƒˆ íŒŒì¼ ì—…ë¡œë“œë¥¼ ê³„ì† ì§„í–‰í•˜ê¸° ìœ„í•´ continue
                        continue
                    else:
                        # ê¸°ì¡´ íŒŒì¼ì— í•´ì‹œ ì •ë³´ ì¶”ê°€
                        existing_file_data["file_hash"] = file_hash
                        self._save_files_metadata()
                        
                        # ì¤‘ë³µ íŒŒì¼ ì‘ë‹µ ìƒì„± (í•´ì‹œ ì¶”ê°€ë¨)
                        response = FileUploadResponse(
                            file_id=existing_file_id,
                            filename=file.filename,
                            status=existing_file_data.get("status", "pending_vectorization"),
                            file_size=file_size,
                            category_id=category_id,
                            category_name=existing_file_data.get("category_name"),
                            message=f"ì¤‘ë³µ íŒŒì¼ ê°ì§€: '{existing_filename}'ê³¼ ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."
                        )
                        # ì¤‘ë³µ í”Œë˜ê·¸ ì¶”ê°€ (ë™ì  ì†ì„±)
                        response.__dict__['is_duplicate'] = True
                        response.__dict__['existing_file_id'] = existing_file_id
                        return response
            
            print(f"ì¤‘ë³µ íŒŒì¼ ì—†ìŒ, ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì—…ë¡œë“œ ì§„í–‰: {file.filename}")
            
            # ê³ ìœ  íŒŒì¼ ID ìƒì„±
            file_id = str(uuid.uuid4())
            file_extension = os.path.splitext(file.filename)[1]
            saved_filename = f"{file_id}{file_extension}"
            file_path = os.path.join(self.upload_dir, saved_filename)
            
            # ì¹´í…Œê³ ë¦¬ ê²€ì¦
            category_name = None
            if category_id:
                category = await self.category_service.get_category(category_id)
                if not category:
                    raise HTTPException(status_code=400, detail="ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
                category_name = category.name
            
            # íŒŒì¼ ì €ì¥
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(content)
            
            # ì—…ë¡œë“œ ë‹¨ê³„ì—ì„œëŠ” Docling ì „ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•ŠìŒ (ë²¡í„°í™” ë‹¨ê³„ì—ì„œ ì²˜ë¦¬)

            # íŒŒì¼ ì •ë³´ ì €ì¥
            file_info = {
                "file_id": file_id,
                "filename": file.filename,
                "saved_filename": saved_filename,
                "file_path": file_path,
                "file_size": file_size,
                "file_hash": file_hash,  # ì¤‘ë³µ ê²€ì¶œìš© í•´ì‹œ ì¶”ê°€
                "category_id": category_id,
                "category_name": category_name,
                "status": "uploaded",  # ì—…ë¡œë“œ ì™„ë£Œ ìƒíƒœ
                "upload_time": datetime.now(),
                "vectorized": False  # ë²¡í„°í™”ëŠ” ë³„ë„ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰
            }
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self.files_metadata[file_id] = file_info
            self._save_files_metadata()
            
            # ì—…ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
            message = "íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„°í™”ëŠ” ë²¡í„°í™” í˜ì´ì§€ì—ì„œ ìˆ˜í–‰í•´ì£¼ì„¸ìš”."
            
            return FileUploadResponse(
                file_id=file_id,
                filename=file.filename,
                status="uploaded",  # ì—…ë¡œë“œë§Œ ì™„ë£Œ
                file_size=file_size,
                category_id=category_id,
                category_name=category_name,
                message=message
            )
            
        except HTTPException:
            # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì „íŒŒ
            raise
        except Exception as e:
            print(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def _start_vectorization(self, file_id: str):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ íŒŒì¼ ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        vectorization_start_time = time.time()
        try:
            print(f"=== ë²¡í„°í™” ì‹œì‘: {file_id} ===")
            print(f"ğŸ•°ï¸ ë²¡í„°í™” ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            # íŒŒì¼ ì •ë³´ ì¡°íšŒ
            print("ğŸ” íŒŒì¼ ì •ë³´ ì¡°íšŒ ì¤‘...")
            file_info = await self.get_file_info(file_id)
            if not file_info:
                print(f"âŒ íŒŒì¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                return
            
            file_size = file_info.file_size or 0
            print(f"âœ… íŒŒì¼ ì •ë³´ í™•ì¸: {file_info.filename} ({file_size / 1024 / 1024:.2f} MB)")
            print(f"ğŸ“Š íŒŒì¼ ì„¸ë¶€ì‚¬í•­:")
            print(f"   - íŒŒì¼ëª…: {file_info.filename}")
            print(f"   - ì¹´í…Œê³ ë¦¬: {file_info.category_name}")
            print(f"   - íŒŒì¼ ì‚¬ì´ì¦ˆ: {file_size / 1024 / 1024:.2f} MB")
            print(f"   - ì—…ë¡œë“œ ê²½ë¡œ: {file_info.file_path}")
            
            # íŒŒì¼ ìƒíƒœë¥¼ ë²¡í„°í™” ëŒ€ê¸° ì¤‘ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            if file_id in self.files_metadata:
                self.files_metadata[file_id]["status"] = "pending_vectorization"
                self._save_files_metadata()
                print(f"ìƒíƒœ ì—…ë°ì´íŠ¸: pending_vectorization")
            
            # ë²¡í„°í™” Flow ID ê²°ì •
            print("ë²¡í„°í™” Flow ID ê²°ì • ì¤‘...")
            vectorization_flow_id = await self._determine_vectorization_flow(file_id)
            
            if not vectorization_flow_id:
                print(f"ì ì ˆí•œ ë²¡í„°í™” Flowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë²¡í„°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {file_id}")
                # ë²¡í„°í™” ëŒ€ê¸° ìƒíƒœë¡œ ìœ ì§€
                if file_id in self.files_metadata:
                    self.files_metadata[file_id]["status"] = "pending_vectorization"
                    self.files_metadata[file_id]["vectorized"] = False
                    self._save_files_metadata()
                return
            
            print(f"ì„ íƒëœ Flow ID: {vectorization_flow_id}")
            
            # ë²¡í„°í™” ì‹œì‘ - ìƒíƒœë¥¼ ë²¡í„°í™” ì¤‘ìœ¼ë¡œ ë³€ê²½
            if file_id in self.files_metadata:
                self.files_metadata[file_id]["status"] = "vectorizing"
                self._save_files_metadata()
                print(f"ìƒíƒœ ì—…ë°ì´íŠ¸: vectorizing")
                
                # SSE ë²¡í„°í™” ì‹œì‘ ì´ë²¤íŠ¸ ë°œì†¡
                print("ğŸ“¡ SSE ì´ë²¤íŠ¸ ì „ì†¡ ì‹œë„ ì¤‘...")
                try:
                    from ..api.sse import get_sse_manager
                    sse_manager = get_sse_manager()
                    await sse_manager.broadcast("vectorization_started", {
                        "file_id": file_id,
                        "filename": file_info.filename if file_info else "Unknown",
                        "status": "started",
                        "message": "ë²¡í„°í™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
                    })
                    print(f"âœ… SSE ë²¡í„°í™” ì‹œì‘ ì´ë²¤íŠ¸ ì „ì†¡ ì„±ê³µ: {file_id}")
                    print("ğŸ“¢ í´ë¼ì´ì–¸íŠ¸ì— ë²¡í„°í™” ì‹œì‘ ì•Œë¦¼ ì „ì†¡")
                except Exception as sse_error:
                    print(f"âŒ SSE ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {sse_error}")
            
            # Docling í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            print("Docling í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘...")
            result = {"status": "failed", "error": "ë²¡í„°í™” ì‹¤íŒ¨"}
            
            try:
                # VectorServiceì˜ ìƒˆë¡œìš´ í†µí•© íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
                if self.vector_service:
                    # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                    vector_metadata = {
                        "filename": file_info.filename,
                        "category_id": file_info.category_id,
                        "category_name": file_info.category_name,
                        "flow_id": vectorization_flow_id,  # ê²°ì •ëœ flow_id ì¶”ê°€
                        "upload_time": file_info.upload_time.isoformat() if file_info.upload_time else None,
                        "file_size": file_info.file_size
                    }
                    
                    # Docling ì˜µì…˜ ê²°ì • (ë©”íƒ€ë°ì´í„° ìš°ì„ , ì—†ìœ¼ë©´ ì„¤ì •ê°’)
                    from ..models.schemas import DoclingOptions, FileProcessingOptions

                    enable_docling = False
                    docling_options = None
                    
                    # 1. íŒŒì¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥ëœ ì²˜ë¦¬ ì˜µì…˜ í™•ì¸
                    processing_options_data = file_info.dict().get("processing_options")
                    if processing_options_data and processing_options_data.get("use_docling"):
                        docling_options_data = processing_options_data.get("docling_options")
                        if docling_options_data:
                            enable_docling = True
                            docling_options = DoclingOptions(**docling_options_data)
                            print(f"ğŸ”§ ë²¡í„°í™” ì‹œì  Docling í™œì„±í™” (íŒŒì¼ ë©”íƒ€ë°ì´í„° ê¸°ë°˜): {file_info.filename} (OCR: {docling_options.ocr_enabled})")

                    # 2. ë©”íƒ€ë°ì´í„°ì— ì—†ìœ¼ë©´, ì „ì—­ ì„¤ì •ê°’ ì‚¬ìš©
                    if not docling_options and not enable_docling:
                        from .model_settings_service import ModelSettingsService
                        model_settings_service = ModelSettingsService()
                        docling_settings = await model_settings_service.get_docling_settings()
                        enable_docling = docling_settings.enabled

                        if enable_docling:
                            docling_options = DoclingOptions(
                                enabled=docling_settings.enabled,
                                extract_tables=docling_settings.default_extract_tables,
                                extract_images=docling_settings.default_extract_images,
                                ocr_enabled=docling_settings.default_ocr_enabled,
                                output_format=docling_settings.default_output_format
                            )
                            print(f"ğŸ”§ ë²¡í„°í™” ì‹œì  Docling í™œì„±í™” (ì „ì—­ ì„¤ì • ê¸°ë°˜): {file_info.filename} (OCR: {docling_options.ocr_enabled})")
                        else:
                            print(f"ğŸ”§ ë²¡í„°í™” ì‹œì  Docling ë¹„í™œì„±í™” (ì „ì—­ ì„¤ì • ê¸°ë°˜): {file_info.filename}")
                    
                    # í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”)
                    vectorization_result = await self.vector_service.vectorize_with_docling_pipeline(
                        file_path=file_info.file_path,
                        file_id=file_id,
                        metadata=vector_metadata,
                        enable_docling=enable_docling,
                        docling_options=docling_options,
                        use_parallel=True  # ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
                    )
                    
                    if vectorization_result["success"]:
                        result = {
                            "status": "completed",
                            "chunks_count": vectorization_result["chunks_count"],
                            "processing_method": vectorization_result.get("processing_method", "unknown"),
                            "processing_time": vectorization_result.get("processing_time", 0)
                        }
                        print(f"âœ… í†µí•© ë²¡í„°í™” ì™„ë£Œ: {vectorization_result['chunks_count']}ê°œ ì²­í¬, ë°©ë²•: {vectorization_result.get('processing_method')}")
                    else:
                        result = {
                            "status": "failed", 
                            "error": vectorization_result.get("error", "ë²¡í„°í™” ì‹¤íŒ¨")
                        }
                        print(f"âŒ í†µí•© ë²¡í„°í™” ì‹¤íŒ¨: {vectorization_result.get('error')}")
                else:
                    print("âŒ VectorServiceë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    result = {"status": "failed", "error": "VectorService ì´ˆê¸°í™” ì‹¤íŒ¨"}
                    
            except Exception as e:
                print(f"âŒ í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                result = {"status": "failed", "error": str(e)}
            
            # ë²¡í„°í™” ì™„ë£Œ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸
            print("ğŸ“‹ ë²¡í„°í™” ê²°ê³¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘...")
            total_elapsed = time.time() - vectorization_start_time
            
            if file_id in self.files_metadata:
                if result.get("status") == "completed":
                    self.files_metadata[file_id]["status"] = "vectorized"
                    self.files_metadata[file_id]["vectorized"] = True
                    self.files_metadata[file_id]["vectorized_at"] = datetime.now()
                    self.files_metadata[file_id]["used_flow_id"] = vectorization_flow_id
                    print(f"ğŸ‰ ë²¡í„°í™” ì„±ê³µ ì™„ë£Œ: {file_id} (Flow: {vectorization_flow_id})")
                    print(f"â±ï¸ ì „ì²´ ë²¡í„°í™” ì‹œê°„: {total_elapsed:.2f}ì´ˆ")
                    
                    # SSE ë²¡í„°í™” ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
                    try:
                        from ..api.sse import get_sse_manager
                        sse_manager = get_sse_manager()
                        await sse_manager.broadcast("vectorization_completed", {
                            "file_id": file_id,
                            "filename": file_info.filename if file_info else "Unknown",
                            "status": "completed",
                            "vectorized": True
                        })
                        print(f"âœ… SSE ë²¡í„°í™” ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡: {file_id}")
                    except Exception as sse_error:
                        print(f"âŒ SSE ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {sse_error}")
                else:
                    self.files_metadata[file_id]["status"] = "vectorization_failed"
                    self.files_metadata[file_id]["error"] = result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                    print(f"âŒ ë²¡í„°í™” ì‹¤íŒ¨: {file_id} (ì†Œìš” ì‹œê°„: {total_elapsed:.2f}ì´ˆ)")
                    print(f"ğŸ” ì˜¤ë¥˜ ë‚´ìš©: {result.get('error')}")
                    print(f"â±ï¸ ì‹¤íŒ¨ê¹Œì§€ ê²½ê³¼ ì‹œê°„: {total_elapsed:.2f}ì´ˆ")
                    
                    # SSE ë²¡í„°í™” ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡
                    try:
                        from ..api.sse import get_sse_manager
                        sse_manager = get_sse_manager()
                        await sse_manager.broadcast("vectorization_failed", {
                            "file_id": file_id,
                            "filename": file_info.filename if file_info else "Unknown",
                            "status": "failed",
                            "error": result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                        })
                        print(f"âš ï¸ SSE ë²¡í„°í™” ì‹¤íŒ¨ ì´ë²¤íŠ¸ ì „ì†¡: {file_id}")
                    except Exception as sse_error:
                        print(f"âŒ SSE ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {sse_error}")
                
                self._save_files_metadata()
                print("âœ… íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                print("âš ï¸ ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœ€ë‹ˆë‹¤.")
            
            print(f"=== ë²¡í„°í™” ìµœì¢… ì™„ë£Œ: {file_id} ===")
            print(f"ğŸ“Š ì „ì²´ í†µê³„:")
            print(f"   - ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.2f}ì´ˆ")
            print(f"   - íŒŒì¼ í¬ê¸°: {file_size / 1024 / 1024:.2f} MB")
            print(f"   - ì²˜ë¦¬ ì†ë„: {(file_size / 1024 / 1024) / total_elapsed:.2f} MB/ì´ˆ")
            print(f"   - ìµœì¢… ìƒíƒœ: {result.get('status', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
            if result.get('status') == 'completed':
                print(f"   - ìƒì„±ëœ ì²­í¬: {result.get('chunks_count', 0)}ê°œ")
            print(f"=== ë²¡í„°í™” ì„¸ì…˜ ì¢…ë£Œ: {datetime.now().strftime('%H:%M:%S')} ===")
                
        except Exception as e:
            total_elapsed = time.time() - vectorization_start_time
            print(f"âŒ ë²¡í„°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)} (ì†Œìš” ì‹œê°„: {total_elapsed:.2f}ì´ˆ)")
            import traceback
            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
            if file_id in self.files_metadata:
                self.files_metadata[file_id]["status"] = "vectorization_failed"
                self.files_metadata[file_id]["error"] = str(e)
                self._save_files_metadata()
                print("âœ… ì˜¤ë¥˜ ìƒíƒœë¡œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
            print(f"=== ë²¡í„°í™” ì˜¤ë¥˜ ì¢…ë£Œ: {file_id} (ì†Œìš” ì‹œê°„: {total_elapsed:.2f}ì´ˆ) ===")
    
    async def _determine_vectorization_flow(self, file_id: str) -> Optional[str]:
        """ê´€ë¦¬ìê°€ í™œì„±í™”í•œ ë²¡í„°í™” Flowë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
        try:
            # 1. í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ ë²¡í„°í™” Flow ID í™•ì¸
            default_flow_id = getattr(settings, 'DEFAULT_VECTORIZATION_FLOW_ID', None)
            
            if default_flow_id:
                print(f"í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ ë²¡í„°í™” Flow ì‚¬ìš©: {default_flow_id}")
                return default_flow_id
            
            # 2. ì„¤ì • íŒŒì¼ì—ì„œ ê¸°ë³¸ ë²¡í„°í™” Flow ID í™•ì¸
            config_file = os.path.join(settings.BASE_DIR, "langflow", "config.json")
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config_data = json.load(f)
                    config_flow_id = config_data.get("default_vectorization_flow_id")
                    if config_flow_id:
                        print(f"ì„¤ì • íŒŒì¼ì—ì„œ ê¸°ë³¸ ë²¡í„°í™” Flow ì‚¬ìš©: {config_flow_id}")
                        return config_flow_id
                except Exception as e:
                    print(f"ì„¤ì • íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
            
            # 3. LangflowServiceì—ì„œ í™œì„± Flow í™•ì¸
            try:
                from .langflow_service import LangflowService
                langflow_service = LangflowService()
                all_flows = await langflow_service.get_flows()
                
                print(f"=== Flow ê²°ì • ë””ë²„ê¹… ì •ë³´ ===")
                print(f"ì´ ë°œê²¬ëœ Flow ìˆ˜: {len(all_flows)}")
                for flow in all_flows:
                    print(f"Flow ID: {flow.get('flow_id')}, Name: {flow.get('name')}")
                    print(f"  - is_active: {flow.get('is_active')}")
                    print(f"  - is_default_vectorization: {flow.get('flow_data', {}).get('is_default_vectorization', False)}")
                
                # ë¨¼ì € ê¸°ë³¸ ë²¡í„°í™” Flow ì°¾ê¸° (is_default_vectorization: true)
                default_vectorization_flows = [flow for flow in all_flows if flow.get("flow_data", {}).get("is_default_vectorization", False) == True]
                
                print(f"ê¸°ë³¸ ë²¡í„°í™” Flow ê°œìˆ˜: {len(default_vectorization_flows)}")
                
                if default_vectorization_flows:
                    # ê¸°ë³¸ ë²¡í„°í™” Flowê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê²ƒ ì‚¬ìš©
                    default_flow = default_vectorization_flows[0]
                    print(f"âœ… ê¸°ë³¸ ë²¡í„°í™” Flow ì‚¬ìš©: {default_flow['flow_id']} ({default_flow['name']})")
                    return default_flow['flow_id']
                
                # ê¸°ë³¸ ë²¡í„°í™” Flowê°€ ì—†ìœ¼ë©´ í™œì„± Flow ì¤‘ì—ì„œ ì„ íƒ
                vectorization_flows = [flow for flow in all_flows if flow.get("is_active", True)]
                
                print(f"í™œì„± Flow ê°œìˆ˜: {len(vectorization_flows)}")
                
                if vectorization_flows:
                    # ê°€ì¥ ìµœê·¼ì— ìˆ˜ì •ëœ Flow ì„ íƒ
                    latest_flow = max(vectorization_flows, key=lambda x: x.get("updated_at", x.get("created_at", "")))
                    print(f"âœ… í™œì„± Flow ì‚¬ìš©: {latest_flow['flow_id']} ({latest_flow['name']})")
                    return latest_flow['flow_id']
                
                print(f"âŒ {len(all_flows)}ê°œ Flowë¥¼ ì°¾ì•˜ì§€ë§Œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²ƒì´ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ LangflowService ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                import traceback
                traceback.print_exc()
            
            print("í™œì„±í™”ëœ ë²¡í„°í™” Flowê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        except Exception as e:
            print(f"Flow ê²°ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return getattr(settings, 'DEFAULT_VECTORIZATION_FLOW_ID', None)
    
    async def vectorize_file(self, file_id: str) -> bool:
        """íŒŒì¼ì„ ë²¡í„°í™”í•©ë‹ˆë‹¤. (LangFlowë§Œ ì‚¬ìš©)"""
        try:
            # ë²¡í„°í™” Flow ID ê²°ì •
            vectorization_flow_id = await self._determine_vectorization_flow(file_id)
            
            if not vectorization_flow_id:
                print(f"LangFlow Flow IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {file_id}")
                return False
            
            # Docling í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ì„ í†µí•œ ë²¡í„°í™”
            file_info = await self.get_file_info(file_id)
            if not file_info:
                print(f"íŒŒì¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                return False
            
            try:
                # VectorServiceì˜ ìƒˆë¡œìš´ í†µí•© íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
                if self.vector_service:
                    # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                    vector_metadata = {
                        "filename": file_info.filename,
                        "category_id": file_info.category_id,
                        "category_name": file_info.category_name,
                        "flow_id": vectorization_flow_id,  # ê²°ì •ëœ flow_id ì¶”ê°€
                        "upload_time": file_info.upload_time.isoformat() if file_info.upload_time else None,
                        "file_size": file_info.file_size
                    }
                    
                    # Docling ì˜µì…˜ ê²°ì • (ë©”íƒ€ë°ì´í„° ìš°ì„ , ì—†ìœ¼ë©´ ì„¤ì •ê°’)
                    from ..models.schemas import DoclingOptions, FileProcessingOptions

                    enable_docling = False
                    docling_options = None
                    
                    # 1. íŒŒì¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥ëœ ì²˜ë¦¬ ì˜µì…˜ í™•ì¸
                    processing_options_data = file_info.dict().get("processing_options")
                    if processing_options_data and processing_options_data.get("use_docling"):
                        docling_options_data = processing_options_data.get("docling_options")
                        if docling_options_data:
                            enable_docling = True
                            docling_options = DoclingOptions(**docling_options_data)
                            print(f"ğŸ”§ ë²¡í„°í™” ì‹œì  Docling í™œì„±í™” (íŒŒì¼ ë©”íƒ€ë°ì´í„° ê¸°ë°˜): {file_info.filename} (OCR: {docling_options.ocr_enabled})")

                    # 2. ë©”íƒ€ë°ì´í„°ì— ì—†ìœ¼ë©´, ì „ì—­ ì„¤ì •ê°’ ì‚¬ìš©
                    if not docling_options and not enable_docling:
                        from .model_settings_service import ModelSettingsService
                        model_settings_service = ModelSettingsService()
                        docling_settings = await model_settings_service.get_docling_settings()
                        enable_docling = docling_settings.enabled

                        if enable_docling:
                            docling_options = DoclingOptions(
                                enabled=docling_settings.enabled,
                                extract_tables=docling_settings.default_extract_tables,
                                extract_images=docling_settings.default_extract_images,
                                ocr_enabled=docling_settings.default_ocr_enabled,
                                output_format=docling_settings.default_output_format
                            )
                            print(f"ğŸ”§ ë²¡í„°í™” ì‹œì  Docling í™œì„±í™” (ì „ì—­ ì„¤ì • ê¸°ë°˜): {file_info.filename} (OCR: {docling_options.ocr_enabled})")
                        else:
                            print(f"ğŸ”§ ë²¡í„°í™” ì‹œì  Docling ë¹„í™œì„±í™” (ì „ì—­ ì„¤ì • ê¸°ë°˜): {file_info.filename}")
                    
                    # í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”)
                    vectorization_result = await self.vector_service.vectorize_with_docling_pipeline(
                        file_path=file_info.file_path,
                        file_id=file_id,
                        metadata=vector_metadata,
                        enable_docling=enable_docling,
                        docling_options=docling_options,
                        use_parallel=True  # ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
                    )
                    
                    if vectorization_result["success"]:
                        print(f"âœ… ê°•ì œ ë²¡í„°í™” ì™„ë£Œ: {file_id}, {vectorization_result['chunks_count']}ê°œ ì²­í¬")
                        
                        # íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸
                        if file_id in self.files_metadata:
                            self.files_metadata[file_id]["status"] = "vectorized"
                            self.files_metadata[file_id]["vectorized"] = True
                            self.files_metadata[file_id]["vectorization_method"] = vectorization_result.get("processing_method", "unknown")
                            self.files_metadata[file_id]["vectorization_time"] = datetime.now()
                            self._save_files_metadata()
                        
                        return True
                    else:
                        print(f"âŒ ê°•ì œ ë²¡í„°í™” ì‹¤íŒ¨: {file_id}, ì˜¤ë¥˜: {vectorization_result.get('error')}")
                        return False
                else:
                    print("âŒ VectorServiceë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                    
            except Exception as e:
                print(f"âŒ ê°•ì œ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return False
                
        except Exception as e:
            print(f"ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def get_file_info(self, file_id: str) -> Optional[FileInfo]:
        """íŒŒì¼ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            file_data = self.files_metadata.get(file_id)
            if file_data:
                return FileInfo(**file_data)
            return None
        except Exception as e:
            print(f"íŒŒì¼ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def list_files(self, category_id: Optional[str] = None) -> List[FileInfo]:
        """ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            # ìƒì„¸ ë””ë²„ê·¸ ì¶œë ¥ ì œê±°
            files = []
            orphaned_metadata = []  # ì‹¤ì œ íŒŒì¼ì´ ì—†ëŠ” ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not hasattr(self, 'files_metadata') or self.files_metadata is None:
                # ì¡°ìš©íˆ ì¬ë¡œë“œ
                self._load_files_metadata()
            
            if not self.files_metadata:
                # ë©”íƒ€ë°ì´í„° ì—†ìŒ
                return []
            
            # ê³¼ë„í•œ ë””ë²„ê·¸ ì œê±°
            
            # íŒŒì¼ ëª©ë¡ ì¡°íšŒ - LangflowService ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            for file_id, file_data in self.files_metadata.items():
                try:
                    # ìƒì„¸ ì²˜ë¦¬ ë¡œê·¸ ì œê±°
                    
                    # file_data íƒ€ì… ê²€ì¦
                    if not isinstance(file_data, dict):
                        # íƒ€ì… ë¹„ì •ìƒì€ ìŠ¤í‚µ
                        continue
                    
                    # ì‚­ì œëœ íŒŒì¼ ì œì™¸
                    if file_data.get("status") == "deleted":
                        # ì‚­ì œëœ íŒŒì¼ ìŠ¤í‚µ
                        continue
                        
                    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                    if category_id is not None and file_data.get("category_id") != category_id:
                        # ì¹´í…Œê³ ë¦¬ í•„í„° ìŠ¤í‚µ
                        continue
                    
                    # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    file_path = file_data.get("file_path")
                    # ê²½ë¡œ í™•ì¸ ë””ë²„ê·¸ ì œê±°
                    
                    # Windows ê²½ë¡œ êµ¬ë¶„ì ì •ê·œí™”
                    if file_path:
                        file_path = file_path.replace('\\', '/')
                        # ê²½ë¡œ ì •ê·œí™” ë””ë²„ê·¸ ì œê±°
                    
                    if file_path and os.path.exists(file_path):
                        pass
                    else:
                        # ì‹¤ì œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ ê¸°ë¡
                        # ì‹¤ì œ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë¡í•˜ê³  ê±´ë„ˆë›°ê¸°
                        orphaned_metadata.append(file_id)
                        continue
                    try:
                        # upload_time ì²˜ë¦¬ - ë¬¸ìì—´ì¸ ê²½ìš° datetimeìœ¼ë¡œ ë³€í™˜
                        upload_time_raw = file_data.get("upload_time")
                        upload_time = datetime.now()  # ê¸°ë³¸ê°’ ì„¤ì •
                        
                        if isinstance(upload_time_raw, str):
                            try:
                                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                                if upload_time_raw.endswith('Z'):
                                    upload_time = datetime.fromisoformat(upload_time_raw.replace('Z', '+00:00'))
                                elif 'T' in upload_time_raw:
                                    upload_time = datetime.fromisoformat(upload_time_raw)
                                else:
                                    # "2025-08-05 19:56:50.113296" í˜•ì‹ ì²˜ë¦¬
                                    upload_time = datetime.strptime(upload_time_raw, "%Y-%m-%d %H:%M:%S.%f")
                            except ValueError as e:
                                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                                upload_time = datetime.now()
                        elif isinstance(upload_time_raw, datetime):
                            upload_time = upload_time_raw
                        else:
                            # ê¸°ë³¸ê°’ ì‚¬ìš©
                            pass
                        
                        # ì•ˆì „í•œ ë°ì´í„° íƒ€ì… ë³€í™˜
                        try:
                            file_size = int(file_data.get("file_size", file_data.get("size", 0)))
                        except (ValueError, TypeError):
                            file_size = 0
                        
                        try:
                            vectorized = bool(file_data.get("vectorized", False))
                        except (ValueError, TypeError):
                            vectorized = False
                        
                        # FileInfo ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë°ì´í„° ì •ë¦¬
                        file_info_data = {
                            "file_id": str(file_id),
                            "filename": str(file_data.get("filename", "")),
                            "status": str(file_data.get("status", "unknown")),
                            "file_size": file_size,
                            "file_path": file_data.get("file_path"),
                            "category_id": file_data.get("category_id"),
                            "category_name": file_data.get("category_name"),
                            "upload_time": upload_time,
                            "vectorized": vectorized,
                            "vectorization_status": file_data.get("vectorization_status"),
                            "error_message": file_data.get("error_message"),
                            "chunk_count": file_data.get("chunk_count")
                        }
                        
                        # ë²¡í„°í™” ìƒíƒœê°€ ë³€ê²½ëœ ê²½ìš°ë§Œ ë¡œê·¸ ì¶œë ¥
                        if file_data.get('vectorized') != vectorized:
                            print(f"ğŸ”„ ë²¡í„°í™” ìƒíƒœ ë³€ê²½ - {file_data.get('filename', 'Unknown')}: {file_data.get('vectorized')} â†’ {vectorized}")
                        
                        # ë””ë²„ê·¸: ë²¡í„°í™” ìƒíƒœ ì •ë³´ ì¶œë ¥ (í•„ìš”ì‹œ í™œì„±í™”)
                        # print(f"ğŸ” íŒŒì¼ {file_data.get('filename', 'Unknown')}: vectorized={vectorized} (raw: {file_data.get('vectorized')}), status={file_data.get('status')}, vectorization_status={file_data.get('vectorization_status')}")
                        
                        # ìƒì„± ì‹œë„ ë¡œê·¸ ì œê±°
                        
                        try:
                            file_info = FileInfo(**file_info_data)
                            # ì „ì—­ Docling ì„¤ì •ì„ ë°˜ì˜í•˜ì—¬ ì²˜ë¦¬ ì˜µì…˜ ë™ì  ì—…ë°ì´íŠ¸
                            # ì „ì—­ Docling ì„¤ì •(ModelSettingsService) ê¸°ì¤€ìœ¼ë¡œ ë°˜ì˜
                            from .model_settings_service import ModelSettingsService
                            model_settings_service = ModelSettingsService()
                            docling_settings = await model_settings_service.get_docling_settings()
                            docling_enabled = docling_settings.enabled

                            if file_info.processing_options:
                                file_info.processing_options.use_docling = docling_enabled
                            else:
                                file_info.processing_options = FileProcessingOptions(use_docling=docling_enabled)
                            
                            files.append(file_info)
                            # ì„±ê³µ ë¡œê·¸ ì œê±°
                        except Exception as e:
                            # ì‹¤íŒ¨ ì‹œë§Œ ê°„ì†Œ ë©”ì‹œì§€
                            print(f"FileInfo ìƒì„± ì‹¤íŒ¨ - íŒŒì¼ ID: {file_id}, ì˜¤ë¥˜: {str(e)}")
                            continue
                        
                    except (KeyError, ValueError, TypeError) as e:
                        self.logger.warning(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜ (íŒŒì¼ ID: {file_id}): {str(e)}")
                        continue
                    except Exception as e:
                        self.logger.error(f"íŒŒì¼ ì •ë³´ ë³€í™˜ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (íŒŒì¼ ID: {file_id}): {str(e)}")
                        continue
                        
                except (FileNotFoundError, PermissionError) as file_error:
                    self.logger.warning(f"íŒŒì¼ ì‹œìŠ¤í…œ ì˜¤ë¥˜ (íŒŒì¼ ID: {file_id}): {str(file_error)}")
                    continue
                except Exception as file_error:
                    self.logger.error(f"ê°œë³„ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (íŒŒì¼ ID: {file_id}): {str(file_error)}")
                    continue
            
            # ê³ ì•„ ë©”íƒ€ë°ì´í„° ì •ë¦¬ (ì„ íƒì‚¬í•­)
            if orphaned_metadata:
                print(f"ë°œê²¬ëœ ê³ ì•„ ë©”íƒ€ë°ì´í„° {len(orphaned_metadata)}ê°œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.")
                for file_id in orphaned_metadata:
                    del self.files_metadata[file_id]
                self._save_files_metadata()
            
            # ì—…ë¡œë“œ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
            files.sort(key=lambda x: x.upload_time, reverse=True)
            
            # ì™„ë£Œ ë¡œê·¸ ì œê±°
            return files
            
        except FileNotFoundError as e:
            self.logger.warning(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {str(e)}")
            return []
        except PermissionError as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ ê¶Œí•œ ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(status_code=500, detail="íŒŒì¼ ì‹œìŠ¤í…œ ê¶Œí•œ ì˜¤ë¥˜")
        except json.JSONDecodeError as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            raise HTTPException(status_code=500, detail="ë©”íƒ€ë°ì´í„° íŒŒì¼ ì†ìƒ")
        except Exception as e:
            self.logger.error(f"list_filesì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            raise HTTPException(status_code=500, detail="íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    
    async def get_files_by_categories(self, category_ids: List[str] = None, categories: List[str] = None) -> List[FileInfo]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ë“¤ì˜ íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            files = []
            for file_data in self.files_metadata.values():
                # ì‚­ì œëœ íŒŒì¼ ì œì™¸
                if file_data.get("status") == "deleted":
                    continue
                    
                file_category_id = file_data.get("category_id")
                file_category_name = file_data.get("category_name")
                
                # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë¨¼ì €
                file_path = file_data.get("file_path")
                # Windows ê²½ë¡œ êµ¬ë¶„ì ì •ê·œí™”
                if file_path:
                    file_path = file_path.replace('\\', '/')
                if not (file_path and os.path.exists(file_path)):
                    continue  # ì‹¤ì œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                
                # upload_time ì²˜ë¦¬
                upload_time_raw = file_data.get("upload_time")
                if isinstance(upload_time_raw, str):
                    try:
                        if upload_time_raw.endswith('Z'):
                            upload_time = datetime.fromisoformat(upload_time_raw.replace('Z', '+00:00'))
                        elif 'T' in upload_time_raw:
                            upload_time = datetime.fromisoformat(upload_time_raw)
                        else:
                            upload_time = datetime.strptime(upload_time_raw, "%Y-%m-%d %H:%M:%S.%f")
                    except ValueError:
                        upload_time = datetime.now()
                elif isinstance(upload_time_raw, datetime):
                    upload_time = upload_time_raw
                else:
                    upload_time = datetime.now()
                
                # FileInfo ë°ì´í„° ì¤€ë¹„
                file_info_data = {
                    "file_id": file_data.get("file_id", ""),
                    "filename": file_data.get("filename", ""),
                    "status": file_data.get("status", "unknown"),
                    "file_size": int(file_data.get("file_size", file_data.get("size", 0))),
                    "file_path": file_data.get("file_path"),
                    "category_id": file_data.get("category_id"),
                    "category_name": file_data.get("category_name"),
                    "upload_time": upload_time,
                    "vectorized": bool(file_data.get("vectorized", False))
                }
                
                # ì¹´í…Œê³ ë¦¬ IDë¡œ í•„í„°ë§
                if category_ids and file_category_id in category_ids:
                    try:
                        files.append(FileInfo(**file_info_data))
                    except Exception as e:
                        print(f"FileInfo ìƒì„± ì‹¤íŒ¨ (ì¹´í…Œê³ ë¦¬ ID): {e}")
                    continue
                
                # ì¹´í…Œê³ ë¦¬ ì´ë¦„ìœ¼ë¡œ í•„í„°ë§
                if categories and file_category_name in categories:
                    try:
                        files.append(FileInfo(**file_info_data))
                    except Exception as e:
                        print(f"FileInfo ìƒì„± ì‹¤íŒ¨ (ì¹´í…Œê³ ë¦¬ ì´ë¦„): {e}")
                    continue
            
            # ì—…ë¡œë“œ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
            files.sort(key=lambda x: x.upload_time, reverse=True)
            return files
            
        except Exception as e:
            print(f"ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def delete_file(self, file_id: str) -> bool:
        """íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ ì •ë³´ ì¡°íšŒ
            file_data = self.files_metadata.get(file_id)
            if not file_data:
                print(f"íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                return False
            
            deletion_errors = []
            
            # ë¬¼ë¦¬ì  íŒŒì¼ ì‚­ì œ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
            try:
                file_path = file_data.get("file_path")
                if file_path and os.path.exists(file_path):
                    os.remove(file_path)
                    print(f"ë¬¼ë¦¬ì  íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_path}")
                elif file_path:
                    print(f"ë¬¼ë¦¬ì  íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
                else:
                    print(f"íŒŒì¼ ê²½ë¡œ ì •ë³´ê°€ ì—†ìŒ: {file_id}")
            except Exception as e:
                error_msg = f"ë¬¼ë¦¬ì  íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
                print(error_msg)
                deletion_errors.append(error_msg)
            
            # ë ˆê±°ì‹œ ë²¡í„° íŒŒì¼ ì‚­ì œ ì½”ë“œ ì œê±°ë¨ - ChromaDBì™€ SQLite ë©”íƒ€ë°ì´í„° ì‚¬ìš©
            
            # ë²¡í„° ì„œë¹„ìŠ¤ì—ì„œ ë²¡í„° ë°ì´í„° ì‚­ì œ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
            try:
                from .vector_service import VectorService
                vector_service = VectorService()
                await vector_service.delete_document_vectors(file_id)
                print(f"ChromaDB ë²¡í„° ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {file_id}")
            except Exception as e:
                error_msg = f"ChromaDB ë²¡í„° ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
                print(error_msg)
                deletion_errors.append(error_msg)
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒì¼ì„ ì‚­ì œëœ ê²ƒìœ¼ë¡œ ë§ˆí‚¹ (ì™„ì „ ì‚­ì œ ëŒ€ì‹ )
            try:
                self.files_metadata[file_id]["status"] = "deleted"
                self.files_metadata[file_id]["deleted_at"] = datetime.now()
                self.files_metadata[file_id]["deletion_errors"] = deletion_errors if deletion_errors else None
                self._save_files_metadata()
                print(f"íŒŒì¼ì„ ì‚­ì œë¨ìœ¼ë¡œ ë§ˆí‚¹ ì™„ë£Œ: {file_id}")
            except Exception as e:
                print(f"ë©”íƒ€ë°ì´í„° ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
                return False
            
            # ì‚­ì œ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸í•˜ì§€ë§Œ, ë©”íƒ€ë°ì´í„°ëŠ” ì‚­ì œë˜ì—ˆìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            if deletion_errors:
                print(f"íŒŒì¼ ì‚­ì œ ê³¼ì •ì—ì„œ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒí–ˆì§€ë§Œ ë©”íƒ€ë°ì´í„°ëŠ” ì‚­ì œë¨: {deletion_errors}")
            
            return True
            
        except Exception as e:
            print(f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    async def generate_missing_hashes(self) -> Dict[str, Any]:
        """ê¸°ì¡´ íŒŒì¼ë“¤ì˜ ëˆ„ë½ëœ í•´ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            import hashlib
            updated_files = []
            error_files = []
            
            for file_id, file_data in self.files_metadata.items():
                # í•´ì‹œê°€ ì—†ê³  ì‚­ì œë˜ì§€ ì•Šì€ íŒŒì¼ë“¤
                if (file_data.get("file_hash") is None and 
                    file_data.get("status") != "deleted"):
                    
                    file_path = file_data.get("file_path")
                    if file_path and os.path.exists(file_path):
                        try:
                            # íŒŒì¼ ë‚´ìš©ìœ¼ë¡œ í•´ì‹œ ìƒì„±
                            with open(file_path, 'rb') as f:
                                content = f.read()
                                file_hash = hashlib.md5(content).hexdigest()
                            
                            # í•´ì‹œ ì •ë³´ ì¶”ê°€
                            file_data["file_hash"] = file_hash
                            updated_files.append({
                                "file_id": file_id,
                                "filename": file_data.get("filename"),
                                "hash": file_hash[:8]
                            })
                            
                        except Exception as e:
                            error_files.append({
                                "file_id": file_id,
                                "filename": file_data.get("filename"),
                                "error": str(e)
                            })
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            if updated_files:
                self._save_files_metadata()
                print(f"âœ… {len(updated_files)}ê°œ íŒŒì¼ì˜ í•´ì‹œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            
            return {
                "updated_count": len(updated_files),
                "error_count": len(error_files),
                "updated_files": updated_files,
                "error_files": error_files
            }
            
        except Exception as e:
            print(f"í•´ì‹œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"error": str(e)}
    
    async def extract_text_from_pdf(self, file_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (pymupdf ì‚¬ìš© - í•œê¸€ í°íŠ¸ ë§¤í•‘ ìµœì í™”)."""
        try:
            extracted_text = ""
            
            # 1) pymupdf ìš°ì„  ì‚¬ìš© (í•œê¸€ í°íŠ¸ ë§¤í•‘ì— ê°€ì¥ ê°•ë ¥)
            try:
                import fitz  # pymupdf
                print("ğŸ“„ pymupdfë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„...")
                
                doc = fitz.open(file_path)
                
                for page_num in range(len(doc)):
                    try:
                        page = doc.load_page(page_num)
                        
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì˜µì…˜ìœ¼ë¡œ í•œê¸€ ì²˜ë¦¬ ìµœì í™”)
                        text_dict = page.get_text("dict")
                        page_text = ""
                        
                        # ë¸”ë¡ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ë³´ì¡´)
                        for block in text_dict["blocks"]:
                            if "lines" in block:  # í…ìŠ¤íŠ¸ ë¸”ë¡
                                for line in block["lines"]:
                                    line_text = ""
                                    for span in line["spans"]:
                                        # í°íŠ¸ ì •ë³´ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                        span_text = span.get("text", "")
                                        if span_text.strip():
                                            line_text += span_text
                                    if line_text.strip():
                                        page_text += line_text + "\n"
                                page_text += "\n"
                        
                        # ëŒ€ì²´ ë°©ë²•: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if not page_text.strip():
                            page_text = page.get_text()
                        
                        # CID ì½”ë“œ í™•ì¸ ë° ì²˜ë¦¬
                        if page_text.strip():
                            if "(cid:" not in page_text.lower():
                                extracted_text += f"[í˜ì´ì§€ {page_num + 1}]\n{page_text}\n\n"
                                print(f"âœ… í˜ì´ì§€ {page_num + 1} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                            else:
                                print(f"âš ï¸ í˜ì´ì§€ {page_num + 1}ì—ì„œ CID ì½”ë“œ ê°ì§€ - OCR ë°©ë²• í•„ìš”")
                        
                    except Exception as e:
                        print(f"pymupdf í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                        continue
                
                doc.close()
                
                # pymupdfë¡œ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œëœ ê²½ìš°
                if extracted_text.strip():
                    print("âœ… pymupdfë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                    return extracted_text.strip()
                else:
                    print("âš ï¸ pymupdf ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ - ë‹¤ë¥¸ ë°©ë²• ì‹œë„")
                    
            except ImportError:
                print("âŒ pymupdf ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - pip install pymupdf í•„ìš”")
            except Exception as e:
                print(f"pymupdf ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

            # 2) pypdf í´ë°±
            try:
                from pypdf import PdfReader
                print("ğŸ“„ pypdfë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„...")
                
                with open(file_path, 'rb') as file:
                    pdf_reader = PdfReader(file)
                    for page_num, page in enumerate(pdf_reader.pages):
                        try:
                            page_text = page.extract_text() or ""
                            if page_text.strip() and "(cid:" not in page_text.lower():
                                extracted_text += f"[í˜ì´ì§€ {page_num + 1}]\n{page_text}\n\n"
                        except Exception as e:
                            print(f"pypdf í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                            continue
                
                if extracted_text.strip():
                    print("âœ… pypdfë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                    return extracted_text.strip()
                    
            except Exception as e:
                print(f"pypdf ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

            # 3) pdfminer.six ë§ˆì§€ë§‰ í´ë°±
            try:
                from pdfminer.high_level import extract_text as pdf_extract_text
                print("ğŸ“„ pdfminer.sixë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„...")
                
                text = pdf_extract_text(file_path) or ""
                if text.strip():
                    # CID íŒ¨í„´ ì œê±° ë° ëŒ€ì²´
                    import re
                    # CID ì½”ë“œë¥¼ ì ì ˆí•œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
                    text = re.sub(r'\(cid:\d+\)', '', text)
                    text = re.sub(r'\s+', ' ', text)  # ê³µë°± ì •ë¦¬
                    
                    if text.strip():
                        print("âœ… pdfminer.sixë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                        return text.strip()
                        
            except Exception as e:
                print(f"pdfminer.six ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

            # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
            print("âŒ ëª¨ë“  PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨")
            return "âš ï¸ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nì´ PDFëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n1. ì´ë¯¸ì§€ ê¸°ë°˜ PDF (OCR í•„ìš”)\n2. íŠ¹ìˆ˜ í°íŠ¸ ì¸ì½”ë”© ì‚¬ìš©\n3. ë³´ì•ˆ ì„¤ì •ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì œí•œ\n\ní•´ê²°ì±…: Doclingì„ í™œì„±í™”í•˜ê±°ë‚˜ OCR ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”."
            
        except Exception as e:
            print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return f"âš ï¸ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    async def extract_text_from_office(self, file_path: str) -> str:
        """Office íŒŒì¼(DOC, PPT, XLS)ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            file_extension = os.path.splitext(file_path)[1].lower()
            text = ""
            
            if file_extension in ['.doc', '.docx']:
                # Word ë¬¸ì„œ ì²˜ë¦¬
                try:
                    from docx import Document
                    if file_extension == '.docx':
                        doc = Document(file_path)
                        for paragraph in doc.paragraphs:
                            if paragraph.text.strip():
                                text += paragraph.text + "\n"
                    else:
                        # .doc íŒŒì¼ì€ python-docxë¡œ ì§ì ‘ ì²˜ë¦¬ ë¶ˆê°€, ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ê°€
                        text = f"âš ï¸ .doc íŒŒì¼ì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. .docx íŒŒì¼ë¡œ ë³€í™˜ í›„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                except ImportError:
                    text = f"âš ï¸ Word íŒŒì¼ ì²˜ë¦¬ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (python-docx)"
                except Exception as e:
                    text = f"âš ï¸ Word íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
                    
            elif file_extension in ['.ppt', '.pptx']:
                # PowerPoint íŒŒì¼ ì²˜ë¦¬
                try:
                    from pptx import Presentation
                    if file_extension == '.pptx':
                        prs = Presentation(file_path)
                        for slide_num, slide in enumerate(prs.slides):
                            text += f"[ìŠ¬ë¼ì´ë“œ {slide_num + 1}]\n"
                            for shape in slide.shapes:
                                if hasattr(shape, 'text') and shape.text.strip():
                                    text += shape.text + "\n"
                            text += "\n"
                    else:
                        # .ppt íŒŒì¼ì€ python-pptxë¡œ ì§ì ‘ ì²˜ë¦¬ ë¶ˆê°€
                        text = f"âš ï¸ .ppt íŒŒì¼ì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. .pptx íŒŒì¼ë¡œ ë³€í™˜ í›„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                except ImportError:
                    text = f"âš ï¸ PowerPoint íŒŒì¼ ì²˜ë¦¬ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (python-pptx)"
                except Exception as e:
                    text = f"âš ï¸ PowerPoint íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
                    
            elif file_extension in ['.xls', '.xlsx']:
                # Excel íŒŒì¼ ì²˜ë¦¬
                try:
                    import openpyxl
                    if file_extension == '.xlsx':
                        wb = openpyxl.load_workbook(file_path, data_only=True)
                        for sheet_name in wb.sheetnames:
                            text += f"[ì‹œíŠ¸: {sheet_name}]\n"
                            sheet = wb[sheet_name]
                            for row in sheet.iter_rows(values_only=True):
                                row_text = []
                                for cell_value in row:
                                    if cell_value is not None:
                                        row_text.append(str(cell_value))
                                if row_text:
                                    text += " | ".join(row_text) + "\n"
                            text += "\n"
                    else:
                        # .xls íŒŒì¼ì€ xlrd ë“±ì´ í•„ìš”í•˜ì§€ë§Œ ë³µì¡í•˜ë¯€ë¡œ ê²½ê³  ë©”ì‹œì§€
                        text = f"âš ï¸ .xls íŒŒì¼ì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. .xlsx íŒŒì¼ë¡œ ë³€í™˜ í›„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                except ImportError:
                    text = f"âš ï¸ Excel íŒŒì¼ ì²˜ë¦¬ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (openpyxl)"
                except Exception as e:
                    text = f"âš ï¸ Excel íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
            
            return text.strip() if text else "âš ï¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"Office íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def extract_text_from_file(self, file_path: str) -> str:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²•ì„ ì„ íƒí•©ë‹ˆë‹¤ (Docling ë¹„í™œì„±í™” ì‹œ ì‚¬ìš©)."""
        try:
            file_extension = os.path.splitext(file_path)[1].lower()
            filename = os.path.basename(file_path)
            
            print(f"ğŸ“„ FileService í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {filename} ({file_extension})")
            
            # PDF íŒŒì¼
            if file_extension == '.pdf':
                print("ğŸ“„ PDF ì²˜ë¦¬ - pymupdf + ë‹¤ì¤‘ í´ë°± ë°©ì‹")
                return await self.extract_text_from_pdf(file_path)
            
            # Office íŒŒì¼ë“¤
            elif file_extension in ['.doc', '.docx', '.ppt', '.pptx', '.xls', '.xlsx']:
                print(f"ğŸ“„ Office íŒŒì¼ ì²˜ë¦¬ - {file_extension}")
                return await self.extract_text_from_office(file_path)
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤
            elif file_extension in ['.txt', '.md', '.csv']:
                print(f"ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ - {file_extension}")
                return await self.extract_text_from_text_file(file_path)
            
            # HTML íŒŒì¼ë“¤
            elif file_extension in ['.html', '.htm']:
                print(f"ğŸŒ HTML íŒŒì¼ ì²˜ë¦¬ - {file_extension}")
                return await self.extract_text_from_html(file_path)
            
            # JSON íŒŒì¼
            elif file_extension == '.json':
                print("ğŸ“‹ JSON íŒŒì¼ ì²˜ë¦¬")
                return await self.extract_text_from_json(file_path)
            
            # XML íŒŒì¼
            elif file_extension == '.xml':
                print("ğŸ·ï¸ XML íŒŒì¼ ì²˜ë¦¬")
                return await self.extract_text_from_xml(file_path)
            
            # ê¸°íƒ€ í…ìŠ¤íŠ¸ë¡œ ì½ê¸° ì‹œë„
            else:
                print(f"â“ ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹ ({file_extension}) - í…ìŠ¤íŠ¸ë¡œ ì½ê¸° ì‹œë„")
                return await self.extract_text_from_unknown(file_path)
                
        except Exception as e:
            print(f"âŒ FileService í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
    
    async def extract_text_from_text_file(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í•œê¸€ ì¸ì½”ë”© ì²˜ë¦¬ í¬í•¨)."""
        try:
            # UTF-8 ìš°ì„  ì‹œë„
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print("âœ… UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° ì„±ê³µ")
                return content.strip()
            except UnicodeDecodeError:
                pass
            
            # CP949 (í•œê¸€ ì¸ì½”ë”©) ì‹œë„
            try:
                with open(file_path, 'r', encoding='cp949') as f:
                    content = f.read()
                print("âœ… CP949 ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° ì„±ê³µ")
                return content.strip()
            except UnicodeDecodeError:
                pass
            
            # EUC-KR ì‹œë„
            try:
                with open(file_path, 'r', encoding='euc-kr') as f:
                    content = f.read()
                print("âœ… EUC-KR ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° ì„±ê³µ")
                return content.strip()
            except UnicodeDecodeError:
                pass
            
            # Latin-1 í´ë°± (ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë„ ì½ìŒ)
            with open(file_path, 'r', encoding='latin-1') as f:
                content = f.read()
            print("âš ï¸ Latin-1 í´ë°± ì¸ì½”ë”© ì‚¬ìš©")
            return content.strip()
                
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            return f"âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"
    
    async def extract_text_from_html(self, file_path: str) -> str:
        """HTML íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # BeautifulSoup ì‚¬ìš© ì‹œë„
            try:
                from bs4 import BeautifulSoup
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                
                soup = BeautifulSoup(html_content, 'html.parser')
                # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                for script in soup(["script", "style"]):
                    script.extract()
                
                text = soup.get_text()
                # ì¤„ë°”ê¿ˆ ì •ë¦¬
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = '\n'.join(chunk for chunk in chunks if chunk)
                
                print("âœ… BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹± ì™„ë£Œ")
                return text
                
            except ImportError:
                print("âš ï¸ BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ì§ì ‘ ì½ê¸°ë¡œ ëŒ€ì²´")
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read().strip()
                    
        except Exception as e:
            print(f"HTML íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return f"âš ï¸ HTML íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
    
    async def extract_text_from_json(self, file_path: str) -> str:
        """JSON íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # JSONì„ ë³´ê¸° ì¢‹ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            text_content = json.dumps(data, ensure_ascii=False, indent=2)
            print("âœ… JSON íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
            return text_content
            
        except Exception as e:
            print(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            # JSON íŒŒì‹±ì´ ì‹¤íŒ¨í•˜ë©´ í…ìŠ¤íŠ¸ë¡œ ì½ê¸°
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read().strip()
            except:
                return f"âš ï¸ JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
    
    async def extract_text_from_xml(self, file_path: str) -> str:
        """XML íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # xml.etree ì‚¬ìš© ì‹œë„
            try:
                import xml.etree.ElementTree as ET
                tree = ET.parse(file_path)
                root = tree.getroot()
                
                def extract_text_from_element(element):
                    text_parts = []
                    if element.text:
                        text_parts.append(element.text.strip())
                    for child in element:
                        text_parts.extend(extract_text_from_element(child))
                        if child.tail:
                            text_parts.append(child.tail.strip())
                    return [part for part in text_parts if part]
                
                all_texts = extract_text_from_element(root)
                text = '\n'.join(all_texts)
                print("âœ… XML íŒŒì‹±ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
                return text
                
            except Exception as xml_error:
                print(f"XML íŒŒì‹± ì‹¤íŒ¨: {xml_error}. ì§ì ‘ ì½ê¸°ë¡œ ëŒ€ì²´")
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read().strip()
                    
        except Exception as e:
            print(f"XML íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return f"âš ï¸ XML íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
    
    async def extract_text_from_unknown(self, file_path: str) -> str:
        """ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹ì„ í…ìŠ¤íŠ¸ë¡œ ì½ê¸° ì‹œë„í•©ë‹ˆë‹¤."""
        try:
            # ë¨¼ì € í…ìŠ¤íŠ¸ë¡œ ì½ê¸° ì‹œë„
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if content.strip():
                    print("âœ… ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ìŒ")
                    return content.strip()
            except UnicodeDecodeError:
                pass
            
            # CP949ë¡œ ì‹œë„
            try:
                with open(file_path, 'r', encoding='cp949') as f:
                    content = f.read()
                if content.strip():
                    print("âœ… ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ì„ CP949ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ìŒ")
                    return content.strip()
            except UnicodeDecodeError:
                pass
            
            # ë°”ì´ë„ˆë¦¬ë¡œ ì½ê³  í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            with open(file_path, 'rb') as f:
                raw_data = f.read()
            
            # UTF-8ë¡œ ë””ì½”ë“œ ì‹œë„ (ì˜¤ë¥˜ ë¬´ì‹œ)
            content = raw_data.decode('utf-8', errors='ignore')
            
            if content.strip():
                print("âœ… ë°”ì´ë„ˆë¦¬ì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ ì™„ë£Œ")
                return content.strip()
            else:
                return "âš ï¸ íŒŒì¼ì—ì„œ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
    
    async def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - overlap
            
            if start >= len(text):
                break
        
        return chunks

    async def update_file_vectorization_status(self, file_id: str, vectorized: bool = True, error_message: str = None, vectorized_at: str = None, chunk_count: int = None) -> bool:
        """íŒŒì¼ì˜ ë²¡í„°í™” ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            if file_id not in self.files_metadata:
                print(f"íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                return False
            
            # í˜„ì¬ ì‹œê°„
            if vectorized_at is None:
                vectorized_at = datetime.now().isoformat()
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            update_data = {
                "vectorized": vectorized,
                "vectorized_at": vectorized_at,
                "status": "vectorized" if vectorized else "vectorization_failed"
            }
            
            # ì²­í¬ ìˆ˜ê°€ ì œê³µëœ ê²½ìš° ì¶”ê°€
            if chunk_count is not None:
                update_data["chunk_count"] = chunk_count
                
            self.files_metadata[file_id].update(update_data)
            
            # ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if error_message:
                self.files_metadata[file_id]["error_message"] = error_message
                self.files_metadata[file_id]["vectorization_status"] = "failed"
            else:
                # ì„±ê³µí•œ ê²½ìš° ì˜¤ë¥˜ ì •ë³´ ì œê±°
                self.files_metadata[file_id].pop("error_message", None)
                self.files_metadata[file_id]["vectorization_status"] = "completed" if vectorized else "failed"
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ì— ì €ì¥
            self._save_files_metadata()
            
            print(f"íŒŒì¼ ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {file_id} -> vectorized={vectorized}")
            return True
            
        except Exception as e:
            print(f"íŒŒì¼ ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    async def get_file_path(self, file_id: str) -> Optional[str]:
        """íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            if file_id not in self.files_metadata:
                return None
            
            file_info = self.files_metadata[file_id]
            file_path = file_info.get("file_path")
            
            if file_path and os.path.exists(file_path):
                return file_path
            
            return None
        except Exception as e:
            print(f"íŒŒì¼ ê²½ë¡œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

    async def cleanup_orphaned_metadata(self) -> int:
        """ê³ ì•„ ë©”íƒ€ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤. (ì‹¤ì œ íŒŒì¼ì´ ì—†ëŠ” ë©”íƒ€ë°ì´í„° ì œê±°)"""
        try:
            orphaned_count = 0
            orphaned_ids = []
            
            for file_id, file_data in self.files_metadata.items():
                file_path = file_data.get("file_path")
                if not (file_path and os.path.exists(file_path)):
                    orphaned_ids.append(file_id)
                    print(f"ê³ ì•„ ë©”íƒ€ë°ì´í„° ë°œê²¬: {file_data.get('filename')} ({file_path})")
            
            # ê³ ì•„ ë©”íƒ€ë°ì´í„° ì œê±°
            for file_id in orphaned_ids:
                del self.files_metadata[file_id]
                orphaned_count += 1
            
            if orphaned_count > 0:
                self._save_files_metadata()
                print(f"ì´ {orphaned_count}ê°œì˜ ê³ ì•„ ë©”íƒ€ë°ì´í„°ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            
            return orphaned_count
            
        except Exception as e:
            print(f"ê³ ì•„ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return 0

    async def sync_files_with_storage(self) -> dict:
        """ìŠ¤í† ë¦¬ì§€ì™€ ë©”íƒ€ë°ì´í„° ë™ê¸°í™” ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            # ë©”íƒ€ë°ì´í„°ì—ëŠ” ìˆì§€ë§Œ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
            orphaned_metadata = []
            # íŒŒì¼ì€ ìˆì§€ë§Œ ë©”íƒ€ë°ì´í„°ì— ì—†ëŠ” ê²½ìš°
            orphaned_files = []
            
            # ë©”íƒ€ë°ì´í„° í™•ì¸
            for file_id, file_data in self.files_metadata.items():
                file_path = file_data.get("file_path")
                if not (file_path and os.path.exists(file_path)):
                    orphaned_metadata.append({
                        "file_id": file_id,
                        "filename": file_data.get("filename"),
                        "file_path": file_path
                    })
            
            # ì‹¤ì œ íŒŒì¼ í™•ì¸
            if os.path.exists(self.upload_dir):
                for filename in os.listdir(self.upload_dir):
                    file_path = os.path.join(self.upload_dir, filename)
                    if os.path.isfile(file_path):
                        # ì´ íŒŒì¼ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                        found_metadata = False
                        for file_data in self.files_metadata.values():
                            if file_data.get("file_path") == file_path:
                                found_metadata = True
                                break
                        
                        if not found_metadata:
                            orphaned_files.append({
                                "filename": filename,
                                "file_path": file_path,
                                "size": os.path.getsize(file_path)
                            })
            
            return {
                "orphaned_metadata": orphaned_metadata,
                "orphaned_files": orphaned_files,
                "orphaned_metadata_count": len(orphaned_metadata),
                "orphaned_files_count": len(orphaned_files)
            }
            
        except Exception as e:
            print(f"íŒŒì¼ ë™ê¸°í™” í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "orphaned_metadata": [],
                "orphaned_files": [],
                "orphaned_metadata_count": 0,
                "orphaned_files_count": 0,
                "error": str(e)
            }

    async def retry_vectorization(self, file_id: str) -> bool:
        """íŒŒì¼ì˜ ë²¡í„°í™”ë¥¼ ì¬ì‹œë„í•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ ì •ë³´ í™•ì¸
            file_info = await self.get_file_info(file_id)
            if not file_info:
                print(f"íŒŒì¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                return False
            
            print(f"=== ë²¡í„°í™” ì¬ì‹œë„ ì‹œì‘: {file_id} ({file_info.filename}) ===")
            
            # íŒŒì¼ ìƒíƒœë¥¼ pending_vectorizationìœ¼ë¡œ ì¬ì„¤ì •
            if file_id in self.files_metadata:
                self.files_metadata[file_id]["status"] = "pending_vectorization"
                self.files_metadata[file_id]["vectorized"] = False
                # ì´ì „ ì˜¤ë¥˜ ì •ë³´ ì œê±°
                if "error" in self.files_metadata[file_id]:
                    del self.files_metadata[file_id]["error"]
                self._save_files_metadata()
                print(f"íŒŒì¼ ìƒíƒœë¥¼ pending_vectorizationìœ¼ë¡œ ì¬ì„¤ì •: {file_id}")
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„°í™” ì¬ì‹œì‘
            import asyncio
            asyncio.create_task(self._start_vectorization(file_id))
            
            print(f"ë²¡í„°í™” ì¬ì‹œë„ ì‘ì—… ì‹œì‘: {file_id}")
            return True
            
        except Exception as e:
            print(f"ë²¡í„°í™” ì¬ì‹œë„ ì¤‘ ì˜¤ë¥˜: {file_id}, {str(e)}")
            return False 

    async def set_search_flow(self, flow_id: str) -> bool:
        """ê²€ìƒ‰ Flow ì„¤ì •"""
        try:
            from .langflow_service import LangflowService
            langflow_service = LangflowService()
            return await langflow_service.set_search_flow(flow_id)
        except Exception as e:
            print(f"ê²€ìƒ‰ Flow ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            return False

    async def delete_flow(self, flow_id: str) -> bool:
        """Flow ì‚­ì œ"""
        try:
            from .langflow_service import LangflowService
            langflow_service = LangflowService()
            return await langflow_service.delete_flow(flow_id)
        except Exception as e:
            print(f"Flow ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            return False 

    async def sync_vectorization_status(self) -> dict:
        """ë²¡í„°í™” ìƒíƒœë¥¼ ì‹¤ì œ ChromaDB ë°ì´í„°ì™€ ë™ê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            print("=== ë²¡í„°í™” ìƒíƒœ ë™ê¸°í™” ì‹œì‘ ===")
            
            # VectorService ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            from .vector_service import VectorService
            vector_service = VectorService()
            
            # ChromaDB ìƒíƒœ í™•ì¸
            chroma_status = vector_service.get_chromadb_status()
            actual_vector_count = chroma_status.get("collection_count", 0)
            
            print(f"ì‹¤ì œ ChromaDB ì´ ë²¡í„° ê°œìˆ˜: {actual_vector_count}")
            
            # íŒŒì¼ë³„ ë²¡í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            sync_results = {
                "total_files": len(self.files_metadata),
                "files_with_vectors": 0,
                "files_without_vectors": 0,
                "status_corrected": 0,
                "details": []
            }
            
            for file_id, file_data in self.files_metadata.items():
                filename = file_data.get("filename", "Unknown")
                current_status = file_data.get("status", "unknown")
                current_vectorized = file_data.get("vectorized", False)
                
                # ChromaDBì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ ë²¡í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                try:
                    file_vectors = await vector_service.get_document_chunks(file_id)
                    has_vectors = len(file_vectors) > 0
                    
                    # ìƒíƒœ ë¶ˆì¼ì¹˜ í™•ì¸
                    status_mismatch = False
                    if has_vectors and not current_vectorized:
                        # ë²¡í„°ëŠ” ìˆì§€ë§Œ ë©”íƒ€ë°ì´í„°ì— vectorized=False
                        status_mismatch = True
                        self.files_metadata[file_id]["vectorized"] = True
                        self.files_metadata[file_id]["status"] = "vectorized"
                        sync_results["status_corrected"] += 1
                        print(f"âœ… ìˆ˜ì •: {filename} - ë²¡í„° ì¡´ì¬í•˜ì§€ë§Œ ë©”íƒ€ë°ì´í„°ì— vectorized=False")
                        
                    elif not has_vectors and current_vectorized:
                        # ë²¡í„°ëŠ” ì—†ì§€ë§Œ ë©”íƒ€ë°ì´í„°ì— vectorized=True
                        status_mismatch = True
                        self.files_metadata[file_id]["vectorized"] = False
                        self.files_metadata[file_id]["status"] = "vectorization_failed"
                        sync_results["status_corrected"] += 1
                        print(f"âŒ ìˆ˜ì •: {filename} - ë²¡í„° ì—†ì§€ë§Œ ë©”íƒ€ë°ì´í„°ì— vectorized=True")
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    if has_vectors:
                        sync_results["files_with_vectors"] += 1
                    else:
                        sync_results["files_without_vectors"] += 1
                    
                    sync_results["details"].append({
                        "file_id": file_id,
                        "filename": filename,
                        "has_vectors": has_vectors,
                        "metadata_vectorized": current_vectorized,
                        "status_mismatch": status_mismatch,
                        "vector_count": len(file_vectors)
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë²¡í„° í™•ì¸ ì‹¤íŒ¨: {filename} - {str(e)}")
                    sync_results["details"].append({
                        "file_id": file_id,
                        "filename": filename,
                        "has_vectors": False,
                        "metadata_vectorized": current_vectorized,
                        "status_mismatch": False,
                        "error": str(e)
                    })
                    sync_results["files_without_vectors"] += 1
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            if sync_results["status_corrected"] > 0:
                self._save_files_metadata()
                print(f"âœ… {sync_results['status_corrected']}ê°œ íŒŒì¼ì˜ ìƒíƒœë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.")
            
            print(f"=== ë²¡í„°í™” ìƒíƒœ ë™ê¸°í™” ì™„ë£Œ ===")
            print(f"ì´ íŒŒì¼: {sync_results['total_files']}")
            print(f"ë²¡í„° ìˆëŠ” íŒŒì¼: {sync_results['files_with_vectors']}")
            print(f"ë²¡í„° ì—†ëŠ” íŒŒì¼: {sync_results['files_without_vectors']}")
            print(f"ìƒíƒœ ìˆ˜ì •ëœ íŒŒì¼: {sync_results['status_corrected']}")
            
            return sync_results
            
        except Exception as e:
            print(f"ë²¡í„°í™” ìƒíƒœ ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return {
                "error": str(e),
                "total_files": 0,
                "files_with_vectors": 0,
                "files_without_vectors": 0,
                "status_corrected": 0,
                "details": []
            } 