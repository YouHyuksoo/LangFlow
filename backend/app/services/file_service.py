
import os
import uuid
import aiofiles
import time
import logging
import asyncio
from typing import List, Optional, Dict, Any
from fastapi import UploadFile, HTTPException
from datetime import datetime

from ..models.schemas import FileUploadResponse, FileInfo, FileProcessingOptions
from ..models.vector_models import FileMetadata, FileMetadataService
from ..models.schemas import FileStatus
from ..core.config import settings
from .category_service import CategoryService
from .preprocessing_service import PreprocessingService
from .vector_service import VectorService
from .settings_service import settings_service

# SSE ì´ë²¤íŠ¸ ì „ì†¡ìš©
try:
    from ..api.sse import get_sse_manager
    SSE_AVAILABLE = True
except ImportError:
    SSE_AVAILABLE = False
    print("SSE ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

class FileService:
    """íŒŒì¼ ê´€ë¦¬ ë° ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤."""
    def __init__(self):
        self.upload_dir = settings.UPLOAD_DIR
        self.logger = logging.getLogger(__name__)
        self._load_allowed_extensions()
        self.category_service = CategoryService()
        
        # Refactored services
        self.preprocessing_service = PreprocessingService()
        self.vector_service = VectorService()

        # SQLite ê¸°ë°˜ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì„œë¹„ìŠ¤
        self.file_metadata_service = FileMetadataService()
        self._ensure_data_dir()

    # --- ë¶„ë¦¬ëœ ì „ì²˜ë¦¬ ë° ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ---
    async def start_preprocessing(self, file_id: str, method: str = None):
        """íŒŒì¼ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        preprocessing_start_time = time.time()
        self.logger.info(f"ğŸ”„ === ì „ì²˜ë¦¬ ì‹œì‘: {file_id} ===")

        try:
            # methodê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if method is None:
                from .settings_service import settings_service
                system_settings = settings_service.get_section_settings("system")
                method = system_settings.get("preprocessing_method", "basic")
                self.logger.info(f"ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ì „ì²˜ë¦¬ ë°©ì‹: {method}")
            
            file_info = await self.get_file_info(file_id)
            if not file_info or not file_info.file_path or not os.path.exists(file_info.file_path):
                self.logger.error(f"íŒŒì¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_id}")
                await self._update_file_status(file_id, FileStatus.FAILED, error="File not found or path is invalid.")
                return {"success": False, "error": "File not found"}

            if file_info.status != FileStatus.UPLOADED:
                self.logger.warning(f"íŒŒì¼ì´ ì „ì²˜ë¦¬ ê°€ëŠ¥í•œ ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤: {file_id}, í˜„ì¬ ìƒíƒœ: {file_info.status}")
                return {"success": False, "error": "File is not in uploadable state"}

            await self._update_file_status(file_id, FileStatus.PREPROCESSING)

            # ì „ì²˜ë¦¬ ì‹¤í–‰
            self.logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‹œì‘ (ë°©ë²•: {method})...")
            text_content = await self.preprocessing_service.process_file(file_info.file_path, method)
            
            # ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
            await self._save_preprocessed_content(file_id, text_content, method)
            
            await self._update_file_status(file_id, FileStatus.PREPROCESSED)
            
            elapsed = time.time() - preprocessing_start_time
            self.logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text_content)} (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
            
            return {
                "success": True,
                "text_length": len(text_content),
                "processing_time": elapsed,
                "method": method
            }

        except Exception as e:
            self.logger.error(f"ğŸ’¥ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
            await self._update_file_status(file_id, FileStatus.FAILED, error=str(e))
            return {"success": False, "error": str(e)}

    async def start_vectorization(self, file_id: str):
        """ì „ì²˜ë¦¬ëœ íŒŒì¼ì˜ ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        vectorization_start_time = time.time()
        self.logger.info(f"ğŸš€ === ë²¡í„°í™” ì‹œì‘: {file_id} ===")

        try:
            file_info = await self.get_file_info(file_id)
            if not file_info:
                self.logger.error(f"íŒŒì¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                return {"success": False, "error": "File not found"}

            # PREPROCESSING ìƒíƒœë‚˜ FAILED ìƒíƒœì˜ íŒŒì¼ë„ ì¬ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í—ˆìš©
            if file_info.status not in [FileStatus.UPLOADED, FileStatus.PREPROCESSED, FileStatus.VECTORIZING, FileStatus.PREPROCESSING, FileStatus.FAILED]:
                self.logger.warning(f"íŒŒì¼ì´ ë²¡í„°í™” ê°€ëŠ¥í•œ ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤: {file_id}, í˜„ì¬ ìƒíƒœ: {file_info.status}")
                return {"success": False, "error": "File is not ready for vectorization"}
            
            # PREPROCESSING ë˜ëŠ” FAILED ìƒíƒœì¸ ê²½ìš° ê°•ì œ ì¬ì²˜ë¦¬ ì‹œì‘
            if file_info.status in [FileStatus.PREPROCESSING, FileStatus.FAILED]:
                self.logger.info(f"ğŸ”„ ìƒíƒœê°€ {file_info.status}ì¸ íŒŒì¼ì„ ê°•ì œ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤: {file_id}")
                # ìƒíƒœë¥¼ UPLOADEDë¡œ ì¬ì„¤ì •í•˜ì—¬ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘
                await self._update_file_status(file_id, FileStatus.UPLOADED)

            # UPLOADED ìƒíƒœë¼ë©´ ì „ì²˜ë¦¬ë¶€í„° ì‹œì‘
            if file_info.status == FileStatus.UPLOADED:
                self.logger.info(f"UPLOADED ìƒíƒœ íŒŒì¼ ì „ì²˜ë¦¬ ì‹œì‘: {file_id}")
                # ê¸°ë³¸ ì„¤ì •ì—ì„œ ì „ì²˜ë¦¬ ë°©ë²• ê°€ì ¸ì˜¤ê¸° (ì„¤ì •ì— ë”°ë¼ basic, docling, unstructured ì¤‘ ì„ íƒ)
                from .settings_service import settings_service
                system_settings = settings_service.get_section_settings("system")
                preprocessing_method = system_settings.get("preprocessing_method", "basic")
                self.logger.info(f"ì„¤ì •ëœ ì „ì²˜ë¦¬ ë°©ì‹: {preprocessing_method}")
                preprocess_result = await self.start_preprocessing(file_id, preprocessing_method)
                if not preprocess_result.get("success"):
                    self.logger.error(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {file_id}")
                    return {"success": False, "error": "Preprocessing failed"}
                
                # ì „ì²˜ë¦¬ ì™„ë£Œ í›„ íŒŒì¼ ì •ë³´ ë‹¤ì‹œ ë¡œë“œ
                file_info = await self.get_file_info(file_id)

            await self._update_file_status(file_id, FileStatus.VECTORIZING)

            # SSE ì´ë²¤íŠ¸ ì „ì†¡ (ë²¡í„°í™” ì‹œì‘)
            if SSE_AVAILABLE:
                try:
                    sse_manager = get_sse_manager()
                    await sse_manager.broadcast("vectorization_update", {
                        "file_id": file_id,
                        "filename": file_info.filename,
                        "status": "started",
                        "vectorized": False
                    })
                    self.logger.info(f"ğŸ“¡ SSE ë²¡í„°í™” ì‹œì‘ ì´ë²¤íŠ¸ ì „ì†¡: {file_id}")
                except Exception as sse_error:
                    self.logger.warning(f"SSE ì‹œì‘ ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {sse_error}")

            # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë¡œë“œ
            text_content = await self._load_preprocessed_content(file_id)
            if not text_content:
                self.logger.error(f"ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                await self._update_file_status(file_id, FileStatus.FAILED, error="Preprocessed content not found")
                return {"success": False, "error": "Preprocessed content not found"}

            # ë²¡í„°í™” ì‹¤í–‰
            self.logger.info(f"ì²­í‚¹ ë° ì„ë² ë”© ì‹œì‘...")
            vector_metadata = { 
                "filename": file_info.filename, 
                "category_id": file_info.category_id,
                "category_name": file_info.category_name,
                "preprocessing_method": file_info.preprocessing_method
            }
            result = await self.vector_service.chunk_and_embed_text(file_id, text_content, vector_metadata)

            if result.get("success"):
                self.logger.info(f"ğŸ”„ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘ - íŒŒì¼ ìƒíƒœë¥¼ COMPLETEDë¡œ ë³€ê²½")
                await self._update_file_status(file_id, FileStatus.COMPLETED, chunks_count=result.get('chunks_count'))
                
                # vectorized í•„ë“œë„ ë³„ë„ë¡œ ì—…ë°ì´íŠ¸
                self.logger.info(f"ğŸ”„ vectorized ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹œì‘")
                await self.update_file_vectorization_status(
                    file_id=file_id,
                    vectorized=True,
                    error_message=None,
                    chunk_count=result.get('chunks_count', 0)
                )
                
                elapsed = time.time() - vectorization_start_time
                self.logger.info(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ - ì²­í¬ ìˆ˜: {result.get('chunks_count', 0)}")
                self.logger.info(f"âœ… ë²¡í„°í™” ì„±ê³µ. ì²­í¬ ìˆ˜: {result.get('chunks_count', 0)} (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
                
                # SSE ì´ë²¤íŠ¸ ì „ì†¡ (ë²¡í„°í™” ì™„ë£Œ)
                if SSE_AVAILABLE:
                    try:
                        sse_manager = get_sse_manager()
                        await sse_manager.broadcast("vectorization_update", {
                            "file_id": file_id,
                            "filename": file_info.filename,
                            "status": "completed",
                            "vectorized": True,
                            "chunks_count": result.get('chunks_count', 0),
                            "processing_time": elapsed
                        })
                        self.logger.info(f"ğŸ“¡ SSE ë²¡í„°í™” ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡: {file_id}")
                    except Exception as sse_error:
                        self.logger.warning(f"SSE ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {sse_error}")
                
                return {
                    "success": True,
                    "chunks_count": result.get('chunks_count', 0),
                    "processing_time": elapsed
                }
            else:
                self.logger.error(f"âŒ ë²¡í„°í™” ì‹¤íŒ¨: {result.get('error')}")
                await self._update_file_status(file_id, FileStatus.FAILED, error=result.get('error'))
                
                # SSE ì´ë²¤íŠ¸ ì „ì†¡ (ë²¡í„°í™” ì‹¤íŒ¨)
                if SSE_AVAILABLE:
                    try:
                        sse_manager = get_sse_manager()
                        await sse_manager.broadcast("vectorization_update", {
                            "file_id": file_id,
                            "filename": file_info.filename,
                            "status": "failed",
                            "vectorized": False,
                            "error_message": result.get('error')
                        })
                        self.logger.info(f"ğŸ“¡ SSE ë²¡í„°í™” ì‹¤íŒ¨ ì´ë²¤íŠ¸ ì „ì†¡: {file_id}")
                    except Exception as sse_error:
                        self.logger.warning(f"SSE ì‹¤íŒ¨ ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {sse_error}")
                
                return {"success": False, "error": result.get('error')}

        except Exception as e:
            self.logger.error(f"ğŸ’¥ ë²¡í„°í™” ì˜¤ë¥˜: {e}", exc_info=True)
            await self._update_file_status(file_id, FileStatus.FAILED, error=str(e))
            
            # SSE ì´ë²¤íŠ¸ ì „ì†¡ (ë²¡í„°í™” ì˜ˆì™¸ ì˜¤ë¥˜)
            if SSE_AVAILABLE:
                try:
                    file_info = await self.get_file_info(file_id)
                    sse_manager = get_sse_manager()
                    await sse_manager.broadcast("vectorization_update", {
                        "file_id": file_id,
                        "filename": file_info.filename if file_info else "Unknown",
                        "status": "failed",
                        "vectorized": False,
                        "error_message": str(e)
                    })
                    self.logger.info(f"ğŸ“¡ SSE ë²¡í„°í™” ì˜ˆì™¸ ì˜¤ë¥˜ ì´ë²¤íŠ¸ ì „ì†¡: {file_id}")
                except Exception as sse_error:
                    self.logger.warning(f"SSE ì˜ˆì™¸ ì˜¤ë¥˜ ì´ë²¤íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {sse_error}")
            
            return {"success": False, "error": str(e)}

    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ë©”ì„œë“œ (deprecated)
    async def start_vectorization_pipeline(self, file_id: str):
        """ì „ì²´ ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤. (deprecated - ì „ì²˜ë¦¬ì™€ ë²¡í„°í™”ê°€ ë¶„ë¦¬ë¨)"""
        self.logger.warning(f"start_vectorization_pipelineì€ deprecated ë©ë‹ˆë‹¤. start_preprocessingê³¼ start_vectorizationì„ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        # ìë™ìœ¼ë¡œ ì „ì²˜ë¦¬ ë¨¼ì € ì‹¤í–‰
        preprocess_result = await self.start_preprocessing(file_id)
        if not preprocess_result.get("success"):
            return preprocess_result
            
        # ì „ì²˜ë¦¬ ì„±ê³µ ì‹œ ë²¡í„°í™” ì‹¤í–‰
        return await self.start_vectorization(file_id)

    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ preprocess_file ë©”ì„œë“œ
    async def preprocess_file(self, file_id: str, method: str = "basic"):
        """íŒŒì¼ ì „ì²˜ë¦¬ - start_preprocessingì˜ ë³„ì¹­ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self.start_preprocessing(file_id, method)

    async def _update_file_status(self, file_id: str, status: FileStatus, error: str = None, chunks_count: int = None):
        """íŒŒì¼ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        # ê¸°ì¡´ íŒŒì¼ ì •ë³´ ì¡°íšŒ
        file_metadata = self.file_metadata_service.get_file(file_id)
        if not file_metadata:
            self.logger.error(f"íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
            return
        
        old_status = file_metadata.status
        self.logger.info(f"ğŸ“ íŒŒì¼ ìƒíƒœ ë³€ê²½: {old_status} â†’ {status}")
        
        # ì—…ë°ì´íŠ¸í•  í•„ë“œë“¤
        update_fields = {}
        
        if chunks_count is not None:
            old_chunk_count = file_metadata.chunk_count
            update_fields['chunk_count'] = chunks_count
            self.logger.info(f"ğŸ“Š ì²­í¬ ìˆ˜ ì—…ë°ì´íŠ¸: {old_chunk_count} â†’ {chunks_count}ê°œ")
        
        if error:
            update_fields['error_message'] = error
            self.logger.error(f"âŒ ì‹¤íŒ¨ ìƒíƒœë¡œ ë³€ê²½, ì—ëŸ¬ ë©”ì‹œì§€: {error}")
        elif status == FileStatus.COMPLETED:
            update_fields['error_message'] = None
            self.logger.info(f"ğŸ”„ ì—ëŸ¬ ë©”ì‹œì§€ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # SQLite ìƒíƒœ ì—…ë°ì´íŠ¸
        success = self.file_metadata_service.update_status(
            file_id=file_id,
            status=status,
            **update_fields
        )
        
        if success:
            self.logger.info(f"âœ… SQLite ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
            # ì—…ë°ì´íŠ¸ í›„ í™•ì¸
            updated_file = self.file_metadata_service.get_file(file_id)
            if updated_file and status == FileStatus.COMPLETED:
                self.logger.info(f"ğŸ” ì—…ë°ì´íŠ¸ í›„ vectorized ê°’: {updated_file.vectorized}")
        else:
            self.logger.error(f"âŒ SQLite ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {file_id}")

    async def _save_preprocessed_content(self, file_id: str, text_content: str, method: str):
        """ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        preprocessed_dir = os.path.join(settings.DATA_DIR, "preprocessed")
        os.makedirs(preprocessed_dir, exist_ok=True)
        
        preprocessed_file = os.path.join(preprocessed_dir, f"{file_id}.txt")
        async with aiofiles.open(preprocessed_file, 'w', encoding='utf-8') as f:
            await f.write(text_content)
            
        # SQLiteì— ì „ì²˜ë¦¬ ë°©ë²• ì €ì¥
        self.file_metadata_service.update_file(
            file_id=file_id,
            preprocessing_method=method
        )

    async def _load_preprocessed_content(self, file_id: str) -> str:
        """ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        preprocessed_file = os.path.join(settings.DATA_DIR, "preprocessed", f"{file_id}.txt")
        if os.path.exists(preprocessed_file):
            async with aiofiles.open(preprocessed_file, 'r', encoding='utf-8') as f:
                return await f.read()
        return ""

    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ deprecated ë©”ì„œë“œ
    async def _update_vectorization_status(self, file_id: str, status: str, error: str = None, chunks_count: int = None):
        """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ deprecated ë©”ì„œë“œ"""
        self.logger.warning("_update_vectorization_statusëŠ” deprecatedë˜ì—ˆìŠµë‹ˆë‹¤. _update_file_statusë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        # ê¸°ì¡´ ìƒíƒœë¥¼ ìƒˆë¡œìš´ FileStatusë¡œ ë§¤í•‘
        status_mapping = {
            "processing": FileStatus.PREPROCESSING,
            "completed": FileStatus.COMPLETED,
            "failed": FileStatus.FAILED
        }
        new_status = status_mapping.get(status, FileStatus.FAILED)
        await self._update_file_status(file_id, new_status, error, chunks_count)

    async def update_file_vectorization_status(self, file_id: str, vectorized: bool, error_message: str = None, chunk_count: int = None):
        """íŒŒì¼ì˜ ë²¡í„°í™” ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤ (APIì—ì„œ í˜¸ì¶œìš©)"""
        try:
            # ì—…ë°ì´íŠ¸í•  í•„ë“œë“¤
            update_fields = {
                'vectorized': vectorized
            }
            
            if vectorized:
                update_fields['error_message'] = None
                if chunk_count is not None:
                    update_fields['chunk_count'] = chunk_count
                
                # ìƒíƒœë¥¼ COMPLETEDë¡œ ì—…ë°ì´íŠ¸
                success = self.file_metadata_service.update_status(
                    file_id=file_id,
                    status=FileStatus.COMPLETED,
                    **update_fields
                )
                
                if success:
                    self.logger.info(f"íŒŒì¼ {file_id} ë²¡í„°í™” ìƒíƒœë¥¼ ì„±ê³µìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ì²­í¬: {chunk_count}ê°œ)")
                    return True
            else:
                update_fields['error_message'] = error_message
                status = FileStatus.UPLOADED if not error_message else FileStatus.FAILED
                
                success = self.file_metadata_service.update_status(
                    file_id=file_id,
                    status=status,
                    **update_fields
                )
                
                if success:
                    self.logger.info(f"íŒŒì¼ {file_id} ë²¡í„°í™” ìƒíƒœë¥¼ ì¬ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸")
                    return True
            
            self.logger.error(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {file_id}")
            return False
                
        except Exception as e:
            self.logger.error(f"ë²¡í„°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {file_id}, ì˜¤ë¥˜: {str(e)}")
            return False

    # --- File Management Methods (Preserved) ---
    def _load_allowed_extensions(self):
        try:
            from .settings_service import settings_service
            system_settings = settings_service.get_section_settings("system")
            allowed_file_types = system_settings.get("allowedFileTypes", ["pdf"])
            self.allowed_extensions = {f".{ext}" if not ext.startswith('.') else ext for ext in allowed_file_types}
        except Exception as e:
            self.logger.warning(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ í™•ì¥ì ì‚¬ìš©: {str(e)}")
            self.allowed_extensions = {".pdf", ".docx", ".pptx", ".xlsx", ".txt"}
    
    # FileInfo ì†ì„± í•˜ìœ„ í˜¸í™˜ì„± (schemaì—ì„œ vectorized ì†ì„± ìš”êµ¬)
    def _convert_to_file_info(self, file_metadata: FileMetadata) -> FileInfo:
        """FileMetadataë¥¼ FileInfoë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        return FileInfo(
            file_id=file_metadata.file_id,
            filename=file_metadata.filename,
            saved_filename=file_metadata.saved_filename,
            file_path=file_metadata.file_path,
            file_size=file_metadata.file_size,
            file_hash=file_metadata.file_hash,
            category_id=file_metadata.category_id,
            category_name=file_metadata.category_name,
            status=file_metadata.status,
            upload_time=file_metadata.upload_time,
            preprocessing_started_at=file_metadata.preprocessing_started_at,
            preprocessing_completed_at=file_metadata.preprocessing_completed_at,
            vectorization_started_at=file_metadata.vectorization_started_at,
            vectorization_completed_at=file_metadata.vectorization_completed_at,
            error_message=file_metadata.error_message,
            chunk_count=file_metadata.chunk_count,
            preprocessing_method=file_metadata.preprocessing_method,
            vectorized=file_metadata.vectorized  # ì¶”ê°€ëœ í•„ë“œ
        )

    def _ensure_data_dir(self):
        os.makedirs(settings.DATA_DIR, exist_ok=True)

    async def upload_file(self, file: UploadFile, category_id: Optional[str] = None, allow_global_duplicates: bool = False, force_replace: bool = False) -> FileUploadResponse:
        try:
            file_extension = os.path.splitext(file.filename)[1].lower()
            if file_extension not in self.allowed_extensions:
                raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(self.allowed_extensions)}")
            
            from .settings_service import settings_service
            system_settings = settings_service.get_section_settings("system")
            max_file_size_mb = system_settings.get("maxFileSize", 10)
            max_file_size_bytes = max_file_size_mb * 1024 * 1024
            
            if file.size and file.size > max_file_size_bytes:
                raise HTTPException(status_code=400, detail=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ í¬ê¸°: {max_file_size_mb}MB")
            
            content = await file.read()
            file_size = len(content)
            
            import hashlib
            file_hash = hashlib.md5(content).hexdigest()
            
            # SQLiteì—ì„œ ì¤‘ë³µ íŒŒì¼ ê²€ì‚¬
            existing_file = self.file_metadata_service.get_file_by_hash(file_hash)
            if existing_file and (allow_global_duplicates or existing_file.category_id == category_id):
                if force_replace:
                    await self.delete_file(existing_file.file_id)
                else:
                    raise HTTPException(
                        status_code=409,
                        detail={"error": "duplicate_file", "message": "ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."}
                    )
            
            file_id = str(uuid.uuid4())
            saved_filename = f"{file_id}{file_extension}"
            file_path = os.path.join(self.upload_dir, saved_filename)
            
            category_name = None
            if category_id:
                category = await self.category_service.get_category(category_id)
                if not category:
                    raise HTTPException(status_code=400, detail="ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
                category_name = category.name
            
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(content)
            
            # ê¸°ë³¸ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            system_settings = settings_service.get_section_settings("system")
            default_preprocessing_method = system_settings.get("preprocessing_method", "basic")
            
            # SQLiteì— íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥
            file_metadata = FileMetadata(
                file_id=file_id,
                filename=file.filename,
                saved_filename=saved_filename,
                file_path=file_path,
                file_size=file_size,
                file_hash=file_hash,
                category_id=category_id,
                category_name=category_name,
                status=FileStatus.UPLOADED,
                upload_time=datetime.now(),
                preprocessing_method=default_preprocessing_method  # ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ê¸°ë³¸ê°’
            )
            
            success = self.file_metadata_service.create_file(file_metadata)
            if not success:
                # íŒŒì¼ ì‚­ì œ í›„ ì˜¤ë¥˜ ë°œìƒ
                if os.path.exists(file_path):
                    os.remove(file_path)
                raise HTTPException(status_code=500, detail="íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            return FileUploadResponse(
                file_id=file_id,
                filename=file.filename,
                status=FileStatus.UPLOADED,
                file_size=file_size,
                category_id=category_id,
                category_name=category_name,
                message="íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ëŠ” ë³„ë„ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤."
            )
            
        except HTTPException:
            raise
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    async def list_files(self, category_id: Optional[str] = None) -> List[FileInfo]:
        file_metadatas = self.file_metadata_service.list_files(
            category_id=category_id,
            include_deleted=False
        )
        
        files = []
        for file_metadata in file_metadatas:
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if os.path.exists(file_metadata.file_path):
                files.append(self._convert_to_file_info(file_metadata))
        
        return files

    async def get_file_info(self, file_id: str) -> Optional[FileInfo]:
        file_metadata = self.file_metadata_service.get_file(file_id)
        if not file_metadata:
            return None
        
        return self._convert_to_file_info(file_metadata)
    
    async def get_file_content(self, file_id: str) -> Dict[str, Any]:
        """íŒŒì¼ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜ (ì²­í‚¹ìš©)"""
        try:
            # íŒŒì¼ ì •ë³´ ì¡°íšŒ
            file_metadata = self.file_metadata_service.get_file(file_id)
            if not file_metadata:
                return {"success": False, "error": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
            
            # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            preprocessed_path = os.path.join(settings.DATA_DIR, "preprocessed", f"{file_id}.txt")
            
            # ì „ì²˜ë¦¬ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ í•´ë‹¹ ë‚´ìš© ì‚¬ìš©
            if os.path.exists(preprocessed_path):
                try:
                    with open(preprocessed_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    return {"success": True, "content": content}
                except Exception as e:
                    self.logger.error(f"ì „ì²˜ë¦¬ëœ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_id}: {e}")
            
            # ì „ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì›ë³¸ íŒŒì¼ì—ì„œ ì¶”ì¶œ
            original_path = file_metadata.file_path
            if not os.path.exists(original_path):
                return {"success": False, "error": "ì›ë³¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
            
            filename_lower = file_metadata.filename.lower()
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš° ì§ì ‘ ì½ê¸°
            if filename_lower.endswith(('.txt', '.md', '.html', '.json', '.xml', '.csv')):
                try:
                    with open(original_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    return {"success": True, "content": content}
                except UnicodeDecodeError:
                    try:
                        with open(original_path, 'r', encoding='cp949') as f:
                            content = f.read()
                        return {"success": True, "content": content}
                    except Exception as e:
                        return {"success": False, "error": f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"}
            
            # PPTX íŒŒì¼ì¸ ê²½ìš° - í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¹ ë¥¸ì²­í‚¹ìš©)
            elif filename_lower.endswith('.pptx'):
                try:
                    from pptx import Presentation
                    
                    prs = Presentation(original_path)
                    text_content = ""
                    
                    for i, slide in enumerate(prs.slides, 1):
                        slide_text = f"\n=== ìŠ¬ë¼ì´ë“œ {i} ===\n"
                        
                        # ìŠ¬ë¼ì´ë“œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        for shape in slide.shapes:
                            if hasattr(shape, "text") and shape.text.strip():
                                slide_text += shape.text + "\n"
                        
                        text_content += slide_text
                    
                    return {"success": True, "content": text_content.strip()}
                    
                except ImportError:
                    return {"success": False, "error": "PPTX í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìœ„í•´ python-pptx íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
                except Exception as e:
                    self.logger.error(f"PPTX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ {file_id}: {e}")
                    return {"success": False, "error": f"PPTX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
            
            # DOCX íŒŒì¼ì¸ ê²½ìš° - í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¹ ë¥¸ì²­í‚¹ìš©)
            elif filename_lower.endswith('.docx'):
                try:
                    from docx import Document
                    
                    doc = Document(original_path)
                    text_content = ""
                    
                    for paragraph in doc.paragraphs:
                        if paragraph.text.strip():
                            text_content += paragraph.text + "\n"
                    
                    return {"success": True, "content": text_content.strip()}
                    
                except ImportError:
                    return {"success": False, "error": "DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìœ„í•´ python-docx íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
                except Exception as e:
                    self.logger.error(f"DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ {file_id}: {e}")
                    return {"success": False, "error": f"DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
            
            # PDF íŒŒì¼ì¸ ê²½ìš° - í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¹ ë¥¸ì²­í‚¹ìš©)
            elif filename_lower.endswith('.pdf'):
                try:
                    import fitz  # PyMuPDF
                    
                    doc = fitz.open(original_path)
                    text_content = ""
                    
                    for page_num in range(len(doc)):
                        page = doc[page_num]
                        page_text = page.get_text()
                        if page_text.strip():
                            text_content += f"\n=== í˜ì´ì§€ {page_num + 1} ===\n"
                            text_content += page_text + "\n"
                    
                    doc.close()
                    return {"success": True, "content": text_content.strip()}
                    
                except ImportError:
                    return {"success": False, "error": "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìœ„í•´ PyMuPDF íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
                except Exception as e:
                    self.logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ {file_id}: {e}")
                    return {"success": False, "error": f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
            
            # ê¸°íƒ€ íŒŒì¼ì€ ì „ì²˜ë¦¬ê°€ í•„ìš”
            else:
                return {"success": False, "error": "íŒŒì¼ì´ ì•„ì§ ì „ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”."}
            
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {file_id}: {e}")
            return {"success": False, "error": f"íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    async def delete_file(self, file_id: str) -> bool:
        file_metadata = self.file_metadata_service.get_file(file_id)
        if not file_metadata:
            return False
        
        try:
            # ë¬¼ë¦¬ íŒŒì¼ ì‚­ì œ
            if os.path.exists(file_metadata.file_path):
                os.remove(file_metadata.file_path)
            
            # ë²¡í„° ë°ì´í„° ì‚­ì œ
            await self.vector_service.delete_document_vectors(file_id)
            
            # SQLiteì—ì„œ ì†Œí”„íŠ¸ ì‚­ì œ (statusë¥¼ deletedë¡œ ë³€ê²½)
            success = self.file_metadata_service.delete_file(file_id, soft_delete=True)
            return success
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}", exc_info=True)
            return False
    
    async def get_file_path(self, file_id: str) -> Optional[str]:
        """íŒŒì¼ ê²½ë¡œ ì¡°íšŒ"""
        file_metadata = self.file_metadata_service.get_file(file_id)
        if file_metadata and os.path.exists(file_metadata.file_path):
            return file_metadata.file_path
        return None
    
    
    
    async def retry_vectorization(self, file_id: str) -> bool:
        """ë²¡í„°í™” ì¬ì‹œë„"""
        try:
            file_metadata = self.file_metadata_service.get_file(file_id)
            if not file_metadata:
                return False
            
            # ìƒíƒœë¥¼ UPLOADEDë¡œ ì¬ì„¤ì •í•˜ê³  ë²¡í„°í™” ì¬ì‹œë„
            self.file_metadata_service.update_status(
                file_id=file_id,
                status=FileStatus.UPLOADED,
                vectorized=False,
                error_message=None
            )
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„°í™” ì‹œì‘
            import asyncio
            asyncio.create_task(self.start_vectorization(file_id))
            
            return True
            
        except Exception as e:
            self.logger.error(f"ë²¡í„°í™” ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
            return False
    
    async def set_search_flow(self, flow_id: str) -> bool:
        """ê²€ìƒ‰ Flow ì„¤ì • (í•˜ìœ„ í˜¸í™˜ì„±)"""
        self.logger.warning("set_search_flowëŠ” í˜„ì¬ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    async def delete_flow(self, flow_id: str) -> bool:
        """Flow ì‚­ì œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        self.logger.warning("delete_flowëŠ” í˜„ì¬ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    async def cleanup_orphaned_metadata(self) -> int:
        """ê³ ì•„ ë©”íƒ€ë°ì´í„° ì •ë¦¬"""
        try:
            files = self.file_metadata_service.list_files(include_deleted=False)
            orphaned_count = 0
            
            for file_metadata in files:
                # ì‹¤ì œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ìŒ
                if not os.path.exists(file_metadata.file_path):
                    success = self.file_metadata_service.delete_file(
                        file_id=file_metadata.file_id,
                        soft_delete=True
                    )
                    if success:
                        orphaned_count += 1
                        self.logger.info(f"ê³ ì•„ ë©”íƒ€ë°ì´í„° ì‚­ì œ: {file_metadata.file_id}")
            
            return orphaned_count
            
        except Exception as e:
            self.logger.error(f"ê³ ì•„ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0
    
    async def sync_vectorization_status(self) -> Dict[str, Any]:
        """ë²¡í„°í™” ìƒíƒœ ë™ê¸°í™”"""
        try:
            files = self.file_metadata_service.list_files(include_deleted=False)
            corrected_count = 0
            
            for file_metadata in files:
                # ë²¡í„° ì„œë¹„ìŠ¤ì—ì„œ ì‹¤ì œ ë²¡í„° ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                try:
                    has_vectors = await self.vector_service.has_document_vectors(file_metadata.file_id)
                    
                    # ë©”íƒ€ë°ì´í„°ì™€ ì‹¤ì œ ìƒíƒœê°€ ë‹¤ë¥¸ ê²½ìš° ìˆ˜ì •
                    if file_metadata.vectorized != has_vectors:
                        status = FileStatus.COMPLETED if has_vectors else FileStatus.UPLOADED
                        self.file_metadata_service.update_status(
                            file_id=file_metadata.file_id,
                            status=status,
                            vectorized=has_vectors
                        )
                        corrected_count += 1
                        self.logger.info(f"ë²¡í„°í™” ìƒíƒœ ë™ê¸°í™”: {file_metadata.file_id} -> {has_vectors}")
                        
                except Exception as e:
                    self.logger.warning(f"ë²¡í„° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨ {file_metadata.file_id}: {e}")
            
            return {
                "status_corrected": corrected_count,
                "message": f"{corrected_count}ê°œ íŒŒì¼ì˜ ìƒíƒœë¥¼ ë™ê¸°í™”í–ˆìŠµë‹ˆë‹¤."
            }
            
        except Exception as e:
            self.logger.error(f"ë²¡í„°í™” ìƒíƒœ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            return {"status_corrected": 0, "error": str(e)}
