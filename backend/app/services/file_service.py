
import os
import uuid
import aiofiles
import json
import time
import logging
import asyncio
from typing import List, Optional, Dict, Any
from fastapi import UploadFile, HTTPException
from datetime import datetime

from ..models.schemas import FileUploadResponse, FileInfo, FileProcessingOptions
from ..core.config import settings
from .category_service import CategoryService
from .preprocessing_service import PreprocessingService
from .vector_service import VectorService

class FileService:
    """íŒŒì¼ ê´€ë¦¬ ë° ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤."""
    def __init__(self):
        self.upload_dir = settings.UPLOAD_DIR
        self.logger = logging.getLogger(__name__)
        self._load_allowed_extensions()
        self.category_service = CategoryService()
        
        # Refactored services
        self.preprocessing_service = PreprocessingService()
        self.vector_service = VectorService()

        self.files_metadata_file = os.path.join(settings.DATA_DIR, "files_metadata.json")
        self._ensure_data_dir()
        self._load_files_metadata()

    # --- Vectorization Pipeline (Orchestrator) ---
    async def start_vectorization_pipeline(self, file_id: str):
        """ì „ì²´ ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤."""
        vectorization_start_time = time.time()
        self.logger.info(f"ğŸš€ === ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘: {file_id} ===")

        try:
            file_info = await self.get_file_info(file_id)
            if not file_info or not file_info.file_path or not os.path.exists(file_info.file_path):
                self.logger.error(f"íŒŒì¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_id}")
                await self._update_vectorization_status(file_id, "failed", error="File not found or path is invalid.")
                return

            await self._update_vectorization_status(file_id, "processing")

            # 1. ì „ì²˜ë¦¬ ë‹¨ê³„
            self.logger.info(f"[1/2] í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‹œì‘...")
            # í†µí•© ì„¤ì •ì—ì„œ ìš°ì„  ì „ì²˜ë¦¬ ë°©ì‹ ê²°ì •
            try:
                system_settings = settings_service.get_section_settings("system")
                preferred_method = system_settings.get("preprocessing_method", "basic")
                self.logger.info(f"ê¸°ë³¸ ì„¤ì •ì—ì„œ ì „ì²˜ë¦¬ ë°©ì‹ ë¡œë“œ: {preferred_method}")
            except Exception as e:
                self.logger.warning(f"ì „ì²˜ë¦¬ ë°©ì‹ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                preferred_method = "basic"
            
            text_content = await self.preprocessing_service.process_file(file_info.file_path, preferred_method)
            self.logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text_content)}")

            # 2. ì²­í‚¹ ë° ì„ë² ë”© ë‹¨ê³„
            self.logger.info(f"[2/2] ì²­í‚¹ ë° ì„ë² ë”© ì‹œì‘...")
            vector_metadata = { "filename": file_info.filename, "category_id": file_info.category_id }
            result = await self.vector_service.chunk_and_embed_text(file_id, text_content, vector_metadata)

            if result.get("success"):
                self.logger.info(f"âœ… ë²¡í„°í™” ì„±ê³µ. ì²­í¬ ìˆ˜: {result.get('chunks_count', 0)}")
                await self._update_vectorization_status(file_id, "completed", chunks_count=result.get('chunks_count'))
            else:
                self.logger.error(f"âŒ ë²¡í„°í™” ì‹¤íŒ¨: {result.get('error')}")
                await self._update_vectorization_status(file_id, "failed", error=result.get('error'))

        except Exception as e:
            self.logger.error(f"ğŸ’¥ ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì „ì²´ ì˜¤ë¥˜: {e}", exc_info=True)
            await self._update_vectorization_status(file_id, "failed", error=str(e))
        finally:
            elapsed = time.time() - vectorization_start_time
            self.logger.info(f"ğŸ === ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ: {file_id} (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ) ===")

    async def _update_vectorization_status(self, file_id: str, status: str, error: str = None, chunks_count: int = None):
        if file_id in self.files_metadata:
            self.files_metadata[file_id]["status"] = status
            if status == "completed":
                self.files_metadata[file_id]["vectorized"] = True
                self.files_metadata[file_id]["vectorized_at"] = datetime.now().isoformat()
                self.files_metadata[file_id]["error"] = None
                if chunks_count is not None:
                    self.files_metadata[file_id]["chunk_count"] = chunks_count
            elif status == "failed":
                self.files_metadata[file_id]["vectorized"] = False
                self.files_metadata[file_id]["error"] = error
            self._save_files_metadata()

    # --- File Management Methods (Preserved) ---
    def _load_allowed_extensions(self):
        try:
            from .settings_service import settings_service
            system_settings = settings_service.get_section_settings("system")
            allowed_file_types = system_settings.get("allowedFileTypes", ["pdf"])
            self.allowed_extensions = {f".{ext}" if not ext.startswith('.') else ext for ext in allowed_file_types}
        except Exception as e:
            self.logger.warning(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ í™•ì¥ì ì‚¬ìš©: {str(e)}")
            self.allowed_extensions = {".pdf", ".docx", ".pptx", ".xlsx", ".txt"}

    def _ensure_data_dir(self):
        os.makedirs(settings.DATA_DIR, exist_ok=True)

    def _load_files_metadata(self):
        if os.path.exists(self.files_metadata_file):
            try:
                with open(self.files_metadata_file, 'r', encoding='utf-8') as f:
                    self.files_metadata = json.load(f)
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.files_metadata = {}
        else:
            self.files_metadata = {}

    def _save_files_metadata(self):
        try:
            with open(self.files_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.files_metadata, f, ensure_ascii=False, indent=2, default=str)
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    async def upload_file(self, file: UploadFile, category_id: Optional[str] = None, allow_global_duplicates: bool = False, force_replace: bool = False, processing_options: Optional[FileProcessingOptions] = None) -> FileUploadResponse:
        try:
            file_extension = os.path.splitext(file.filename)[1].lower()
            if file_extension not in self.allowed_extensions:
                raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(self.allowed_extensions)}")
            
            from .settings_service import settings_service
            system_settings = settings_service.get_section_settings("system")
            max_file_size_mb = system_settings.get("maxFileSize", 10)
            max_file_size_bytes = max_file_size_mb * 1024 * 1024
            
            if file.size and file.size > max_file_size_bytes:
                raise HTTPException(status_code=400, detail=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ í¬ê¸°: {max_file_size_mb}MB")
            
            content = await file.read()
            file_size = len(content)
            
            import hashlib
            file_hash = hashlib.md5(content).hexdigest()
            
            for existing_file_id, existing_file_data in self.files_metadata.items():
                if existing_file_data.get("status") == "deleted":
                    continue
                
                if (existing_file_data.get("file_hash") == file_hash and 
                    (allow_global_duplicates or existing_file_data.get("category_id") == category_id)):
                    if force_replace:
                        await self.delete_file(existing_file_id)
                        continue
                    else:
                        raise HTTPException(
                            status_code=409,
                            detail={"error": "duplicate_file", "message": "ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."}
                        )
            
            file_id = str(uuid.uuid4())
            saved_filename = f"{file_id}{file_extension}"
            file_path = os.path.join(self.upload_dir, saved_filename)
            
            category_name = None
            if category_id:
                category = await self.category_service.get_category(category_id)
                if not category:
                    raise HTTPException(status_code=400, detail="ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
                category_name = category.name
            
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(content)
            
            file_info = {
                "file_id": file_id,
                "filename": file.filename,
                "saved_filename": saved_filename,
                "file_path": file_path,
                "file_size": file_size,
                "file_hash": file_hash,
                "category_id": category_id,
                "category_name": category_name,
                "status": "uploaded",
                "upload_time": datetime.now(),
                "vectorized": False
            }
            
            self.files_metadata[file_id] = file_info
            self._save_files_metadata()
            
            return FileUploadResponse(
                file_id=file_id,
                filename=file.filename,
                status="uploaded",
                file_size=file_size,
                category_id=category_id,
                category_name=category_name,
                message="íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„°í™”ëŠ” ë³„ë„ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤."
            )
            
        except HTTPException:
            raise
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    async def list_files(self, category_id: Optional[str] = None) -> List[FileInfo]:
        files = []
        for file_id, file_data in self.files_metadata.items():
            if file_data.get("status") == "deleted":
                continue
            if category_id is not None and file_data.get("category_id") != category_id:
                continue
            if os.path.exists(file_data.get("file_path", "")):
                files.append(FileInfo(**file_data))
        files.sort(key=lambda x: x.upload_time, reverse=True)
        return files

    async def get_file_info(self, file_id: str) -> Optional[FileInfo]:
        file_data = self.files_metadata.get(file_id)
        return FileInfo(**file_data) if file_data else None

    async def delete_file(self, file_id: str) -> bool:
        file_data = self.files_metadata.get(file_id)
        if not file_data:
            return False
        
        try:
            if os.path.exists(file_data["file_path"]):
                os.remove(file_data["file_path"])
            await self.vector_service.delete_document_vectors(file_id)
            self.files_metadata[file_id]["status"] = "deleted"
            self.files_metadata[file_id]["deleted_at"] = datetime.now().isoformat()
            self._save_files_metadata()
            return True
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}", exc_info=True)
            return False
