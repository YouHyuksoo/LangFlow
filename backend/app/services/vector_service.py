import os
import json
import asyncio
import time
import threading
import re
from datetime import datetime
from typing import Dict, Any, List, Optional
from ..core.config import settings
from .model_settings_service import get_current_model_config
from .docling_service import DoclingService
from ..models.schemas import DoclingOptions
from ..models.vector_models import VectorMetadata, VectorMetadataService

# ChromaDB ê´€ë ¨ íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì‹œë„ (í•„ìˆ˜: chromadbë§Œ í™•ì¸)
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install chromadb ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

from typing import List, Union, Callable, Any
import numpy as np

class EmbeddingFunction:
    """ì„ë² ë”© í•¨ìˆ˜ì˜ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self, base_function: Any, expected_dimension: int):
        """
        Args:
            base_function: ì‹¤ì œ ì„ë² ë”© í•¨ìˆ˜ (OpenAI, Google ë“±)
            expected_dimension: ì˜ˆìƒë˜ëŠ” ì„ë² ë”© ì°¨ì›
        """
        self.base_function = base_function
        self.expected_dimension = expected_dimension
    
    def __call__(self, input: List[str]) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ChromaDB 0.4.16+ ì¸í„°í˜ì´ìŠ¤)
        
        Args:
            input: ë³€í™˜í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[List[float]]: ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ì„ë² ë”© ì°¨ì›ì´ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ë•Œ
        """
        try:
            print(f"ğŸ”„ ì„ë² ë”© í•¨ìˆ˜ í˜¸ì¶œ - ì…ë ¥ í…ìŠ¤íŠ¸ ìˆ˜: {len(input)}")
            print(f"ğŸ” ì…ë ¥ ë°ì´í„° íƒ€ì…: {type(input)}")
            
            # ì…ë ¥ ê²€ì¦
            if not input or len(input) == 0:
                raise ValueError("ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # í† í° ìˆ˜ ì²´í¬ ë° ì œí•œ (8,192 í† í° í•œê³„ ê³ ë ¤)
            MAX_TOKENS = 6000  # ì•ˆì „ ë§ˆì§„ ë‚¨ê¸°ê¸°
            filtered_input = []
            
            for i, text in enumerate(input):
                # ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚° (ì˜ì–´ ê¸°ì¤€ 1 í† í° = 4ë¬¸ì, í•œê¸€ì€ ë” ì ìŒ)
                estimated_tokens = len(text) // 3  # í•œê¸€ ê³ ë ¤ ë³´ìˆ˜ì  ê³„ì‚°
                
                if estimated_tokens > MAX_TOKENS:
                    print(f"âš ï¸ í…ìŠ¤íŠ¸ {i} í† í° ì´ˆê³¼ ê°ì§€: {estimated_tokens} > {MAX_TOKENS}")
                    # í† í° í•œê³„ì— ë§ê²Œ í…ìŠ¤íŠ¸ ì˜ë¼ë‚´ê¸°
                    max_chars = MAX_TOKENS * 3  # ì•ˆì „í•œ ë¬¸ì ìˆ˜
                    truncated_text = text[:max_chars] + "... [í† í° ì œí•œìœ¼ë¡œ ì¸í•œ ìë¥¸ ë‚´ìš©]"
                    filtered_input.append(truncated_text)
                    print(f"âœ‚ï¸ í…ìŠ¤íŠ¸ ìë¥¸ ë‚´ìš©: {len(text)} -> {len(truncated_text)} ë¬¸ì")
                else:
                    filtered_input.append(text)
            
            print(f"âœ… í† í° ì²´í¬ ì™„ë£Œ - ì›ë³¸: {len(input)}ê°œ, í•„í„°ë§: {len(filtered_input)}ê°œ")
            input = filtered_input  # í•„í„°ë§ëœ ì…ë ¥ ì‚¬ìš©
            
            # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„± (ë” íš¨ìœ¨ì )
            if len(input) > 1:
                print(f"ğŸ“¦ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘... ({len(input)}ê°œ)")
                result = self.base_function(input)
                print(f"âœ… ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                print(f"ğŸ” API ì‘ë‹µ íƒ€ì…: {type(result)}")
            else:
                print(f"ğŸ” ë‹¨ì¼ ì„ë² ë”© ìƒì„± ì¤‘...")
                result = self.base_function(input)
                print(f"âœ… ë‹¨ì¼ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                print(f"ğŸ” API ì‘ë‹µ íƒ€ì…: {type(result)}")
            
            # íƒ€ì…ë³„ ì •ê·œí™” (í•­ìƒ List[List[float]] ë°˜í™˜)
            normalized_embeddings = self._normalize_embedding(result)
            
            # ì°¨ì› ê²€ì¦
            for i, embedding in enumerate(normalized_embeddings):
                if len(embedding) != self.expected_dimension:
                    raise ValueError(
                        f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜ (ì¸ë±ìŠ¤ {i}): ì˜ˆìƒ {self.expected_dimension}, "
                        f"ì‹¤ì œ {len(embedding)}"
                    )
            
            print(f"âœ… ì„ë² ë”© ì •ê·œí™” ë° ê²€ì¦ ì™„ë£Œ - {len(normalized_embeddings)}ê°œ ë²¡í„°")
            return normalized_embeddings
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            print(f"ğŸ” ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
            raise e  # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ throwí•˜ì—¬ ìƒìœ„ì—ì„œ ì²˜ë¦¬
    
    def _normalize_embedding(self, embedding: Any) -> List[List[float]]:
        """ë‹¤ì–‘í•œ íƒ€ì…ì˜ ì„ë² ë”©ì„ List[List[float]]ë¡œ ì •ê·œí™”"""
        print(f"ğŸ” ì„ë² ë”© ì •ê·œí™” ì‹œì‘ - íƒ€ì…: {type(embedding)}")
        print(f"ğŸ” ì„ë² ë”© ë‚´ìš©: {str(embedding)[:200]}...")  # ë””ë²„ê¹…ìš©
        
        # None ë˜ëŠ” ë¹ˆ ê°’ ì²´í¬
        if embedding is None:
            print("âŒ ì„ë² ë”©ì´ Noneì…ë‹ˆë‹¤")
            raise TypeError("ì„ë² ë”©ì´ Noneì…ë‹ˆë‹¤")
        
        # numpy array ì²˜ë¦¬
        if hasattr(embedding, 'tolist'):
            result = embedding.tolist()
            # 2ì°¨ì›ì¸ì§€ í™•ì¸
            if len(result) > 0 and isinstance(result[0], list):
                print(f"âœ… numpy array 2ì°¨ì› ì •ê·œí™” ì™„ë£Œ - {len(result)}ê°œ ë²¡í„°")
                return [[float(x) for x in vec] for vec in result]
            else:
                print(f"âœ… numpy array 1ì°¨ì› ì •ê·œí™” ì™„ë£Œ - 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜")
                return [[float(x) for x in result]]
        
        # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        if isinstance(embedding, list):
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²´í¬
            if len(embedding) == 0:
                print("âŒ ë¹ˆ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸")
                raise TypeError("ë¹ˆ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸")
            
            # ì²« ë²ˆì§¸ ìš”ì†Œê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ (2ì°¨ì›)
            if isinstance(embedding[0], list):
                print(f"âœ… 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ ì •ê·œí™” ì™„ë£Œ - {len(embedding)}ê°œ ë²¡í„°")
                return [[float(x) for x in vec] for vec in embedding]
            
            # ì²« ë²ˆì§¸ ìš”ì†Œê°€ ìˆ«ìì¸ì§€ í™•ì¸ (1ì°¨ì›)
            elif isinstance(embedding[0], (int, float)):
                print(f"âœ… 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ ì •ê·œí™” ì™„ë£Œ - 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜")
                return [[float(x) for x in embedding]]
            
            # ì²« ë²ˆì§¸ ìš”ì†Œê°€ numpy arrayì¸ ê²½ìš°
            elif hasattr(embedding[0], 'tolist'):
                print(f"âœ… ë¦¬ìŠ¤íŠ¸ ë‚´ numpy array ì •ê·œí™” ì¤‘... - {len(embedding)}ê°œ ë²¡í„°")
                return [arr.tolist() for arr in embedding]
            
            # ì²« ë²ˆì§¸ ìš”ì†Œê°€ ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš° (ì˜ˆ: ë¬¸ìì—´, ê°ì²´ ë“±)
            else:
                print(f"âŒ ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…: {type(embedding[0])}")
                print(f"ğŸ” ì²« ë²ˆì§¸ ìš”ì†Œ ë‚´ìš©: {embedding[0]}")
                # ChromaDB OpenAI í•¨ìˆ˜ì˜ íŠ¹ë³„í•œ ì‘ë‹µ í˜•ì‹ í™•ì¸
                if hasattr(embedding[0], '__dict__'):
                    print(f"ğŸ” ê°ì²´ ì†ì„±: {vars(embedding[0])}")
                raise TypeError(f"ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…: {type(embedding[0])}")
        
        # ê·¸ ì™¸ì˜ ê²½ìš° ì˜¤ë¥˜ ë°œìƒ
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…: {type(embedding)}")
        if hasattr(embedding, '__dict__'):
            print(f"ğŸ” ê°ì²´ ì†ì„±: {vars(embedding)}")
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…: {type(embedding)}")

async def _create_embedding_function() -> Union[EmbeddingFunction, None]:
    """í˜„ì¬ ëª¨ë¸ ì„¤ì •ì— ë”°ë¼ í‘œì¤€í™”ëœ ì„ë² ë”© í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ë¹„ë™ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì„¤ì • ë¡œë“œ
        model_config = await get_current_model_config()
        
        embedding_config = model_config.get("embedding", {})
        
        provider = embedding_config.get("provider", "openai")
        model = embedding_config.get("model", "text-embedding-3-small")
        api_key = embedding_config.get("api_key", "")
        dimension = embedding_config.get("dimension", 384)
        
        print(f"ì„ë² ë”© í•¨ìˆ˜ ìƒì„±: {provider} - {model} ({dimension}ì°¨ì›)")
        
        if provider == "openai" and api_key:
            try:
                from chromadb.utils import embedding_functions
                print(f"ğŸ”‘ OpenAI API í‚¤ ê¸¸ì´: {len(api_key)} ë¬¸ì")
                print(f"ğŸ”‘ API í‚¤ ì‹œì‘: {api_key[:10]}...")
                
                base_function = embedding_functions.OpenAIEmbeddingFunction(
                    api_key=api_key,
                    model_name=model,
                    dimensions=dimension  # OpenAI ì°¨ì› ì„¤ì •
                )
                print(f"âœ… OpenAI ì„ë² ë”© í•¨ìˆ˜ ìƒì„± ì„±ê³µ")
                return EmbeddingFunction(base_function, dimension)
            except Exception as openai_error:
                print(f"âŒ OpenAI ì„ë² ë”© í•¨ìˆ˜ ìƒì„± ì‹¤íŒ¨: {str(openai_error)}")
                print(f"ğŸ” ì—ëŸ¬ íƒ€ì…: {type(openai_error).__name__}")
                return None
            
        elif provider == "google" and api_key:
            # Google ì„ë² ë”© í•¨ìˆ˜ (í–¥í›„ í™•ì¥)
            print("Google ì„ë² ë”©ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
            
        elif provider == "ollama":
            # Ollama ì„ë² ë”© í•¨ìˆ˜ (í–¥í›„ í™•ì¥)  
            print("Ollama ì„ë² ë”©ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
            
        else:
            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ì œê³µì—…ì²´ì´ê±°ë‚˜ API í‚¤ê°€ ì—†ìŒ: {provider}")
            return None
            
    except Exception as e:
        print(f"ì„ë² ë”© í•¨ìˆ˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

class VectorService:
    """ChromaDB ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ (Thread-Safe ì‹±ê¸€í†¤)"""
    
    _instance = None
    _initialized = False
    _client = None
    _collection = None
    _lock = threading.Lock()  # Thread-safe ì‹±ê¸€í†¤ì„ ìœ„í•œ ë½
    
    def __new__(cls):
        # Double-checked locking pattern for thread-safety
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(VectorService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # Thread-safe ì´ˆê¸°í™”
        with VectorService._lock:
            if VectorService._initialized:
                return
                
            self.vector_dir = os.path.join(settings.DATA_DIR, 'vectors')
            self.metadata_dir = os.path.join(settings.DATA_DIR, 'vector_metadata')
            
            os.makedirs(self.vector_dir, exist_ok=True)
            os.makedirs(self.metadata_dir, exist_ok=True)
            
            # Docling ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            self.docling_service = DoclingService()
            
            # SQLite ë©”íƒ€ë°ì´í„° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì§€ì—° ì‚¬ìš©)
            self.metadata_service = VectorMetadataService()
            
            # ì§€ì—° ì´ˆê¸°í™” - ì‹¤ì œ ë²¡í„°í™” ì‘ì—…ì—ì„œë§Œ ChromaDB ì—°ê²°ì„ ìˆ˜í–‰
            # íŒŒì¼ ì—…ë¡œë“œ ë“± ì¼ë°˜ì ì¸ ì‘ì—…ì—ì„œëŠ” ChromaDBë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
            print("VectorService ì´ˆê¸°í™” ì™„ë£Œ (ChromaDBëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ ì§€ì—° ë¡œë”©)")
            VectorService._initialized = True
    
    async def _ensure_client(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ìë™ ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤."""
        try:
            # í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ì´ ëª¨ë‘ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if VectorService._client is None or VectorService._collection is None:
                print("ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ìë™ ì—°ê²° ì‹œë„")
                
                # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
                if not os.path.exists(chroma_db_path):
                    raise RuntimeError(
                        f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {chroma_db_path}\n"
                        f"ì„¤ì •ì—ì„œ 'ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±'ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                    )
                
                # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²° ì‹œë„
                await self._connect_to_chromadb()
            
            # ì»¬ë ‰ì…˜ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
            try:
                count = VectorService._collection.count()
                print(f"ChromaDB ì»¬ë ‰ì…˜ ì—°ê²° í™•ì¸: ì´ ë²¡í„° ê°œìˆ˜ {count}")
            except Exception as e:
                print(f"ChromaDB ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
                raise RuntimeError(f"ChromaDB ì»¬ë ‰ì…˜ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            
            return VectorService._client
        
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    
    async def create_chromadb_database(self) -> bool:
        """ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ê³¼ ê¸°ë³¸ êµ¬ì¡°ë§Œ ìƒì„±í•©ë‹ˆë‹¤ (ì„¤ì •ì—ì„œë§Œ ì‚¬ìš©)."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        try:
            print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹œì‘...")
            
            import chromadb
            from chromadb.config import Settings
            
            # ì„ì‹œ í´ë¼ì´ì–¸íŠ¸ë¡œ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ê³¼ êµ¬ì¡° ìƒì„±
            temp_client = None
            temp_collection = None
            
            try:
                # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                temp_client = chromadb.PersistentClient(
                    path=self.vector_dir,
                    settings=Settings(
                        anonymized_telemetry=False,
                        allow_reset=True
                    )
                )
                
                # ê¸°ë³¸ ì»¬ë ‰ì…˜ ìƒì„±
                self.collection_name = "langflow"
                
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
                try:
                    existing_collection = temp_client.get_collection(name=self.collection_name)
                    print(f"ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë°œê²¬: {self.collection_name}")
                except Exception:
                    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                    from ..core.config import settings
                    
                    # ë™ì  ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
                    embedding_function = await _create_embedding_function()
                    if embedding_function:
                        print("ë™ì  ì„ë² ë”© í•¨ìˆ˜ ì„¤ì • ì™„ë£Œ")
                    else:
                        print("ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
                    
                    temp_collection = temp_client.create_collection(
                        name=self.collection_name,
                        embedding_function=embedding_function,
                        metadata={"description": "LangFlow ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ (OpenAI ì„ë² ë”© ì‚¬ìš©)"}
                    )
                    print(f"ìƒˆ ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name}")
                
                # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
                chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
                if os.path.exists(chroma_db_path):
                    print(f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ìƒì„±ë¨: {chroma_db_path}")
                    return True
                else:
                    print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return False
                    
            finally:
                # ì„ì‹œ ì—°ê²° í•´ì œ - ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¦‰ì‹œ í•´ì œ
                temp_client = None
                temp_collection = None
                print("ChromaDB ì„ì‹œ ì—°ê²° í•´ì œë¨")
                
        except Exception as e:
            print(f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def _connect_to_chromadb(self):
        """ê¸°ì¡´ ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì— ì—°ê²°í•©ë‹ˆë‹¤ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜)."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
        chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
        if not os.path.exists(chroma_db_path):
            raise RuntimeError(f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {chroma_db_path}\nì„¤ì •ì—ì„œ ë¨¼ì € 'ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±'ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        try:
            import chromadb
            from chromadb.config import Settings
            
            # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°
            try:
                VectorService._client = chromadb.PersistentClient(
                    path=self.vector_dir,
                    settings=Settings(
                        anonymized_telemetry=False,
                        allow_reset=True
                    )
                )
            except Exception:
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                VectorService._client = chromadb.PersistentClient(path=self.vector_dir)
            
            # ê¸°ë³¸ ì»¬ë ‰ì…˜ ì—°ê²° ë˜ëŠ” ìƒì„±
            self.collection_name = "langflow"
            try:
                VectorService._collection = VectorService._client.get_collection(name=self.collection_name)
                print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}' ì—°ê²° ì„±ê³µ")
            except Exception:
                print(f"ì»¬ë ‰ì…˜ '{self.collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                # ë™ì  ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
                embedding_function = await _create_embedding_function()
                if not embedding_function:
                    print("ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
                
                VectorService._collection = VectorService._client.create_collection(
                    name=self.collection_name,
                    embedding_function=embedding_function,
                    metadata={"description": "LangFlow ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"}
                )
                print(f"ìƒˆ ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            count = VectorService._collection.count()
            print(f"ChromaDB ì—°ê²° ì„±ê³µ - ë²¡í„° ê°œìˆ˜: {count}")
            
        except Exception as e:
            VectorService._client = None
            VectorService._collection = None
            raise RuntimeError(f"ChromaDB ì—°ê²° ì‹¤íŒ¨: {str(e)}")
    
    async def add_document_chunks(self, file_id: str, chunks: List[str], metadata: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì²­í¬ë¥¼ ChromaDBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            # ChromaDBê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            client = await self._ensure_client()
            if not CHROMADB_AVAILABLE or not VectorService._collection:
                print("âŒ ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except RuntimeError as e:
            print(f"âŒ ChromaDB ì´ˆê¸°í™” í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            return False
        
        try:
            # ëª¨ë¸ ì„¤ì •ì—ì„œ ë°°ì¹˜ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
            model_config = await get_current_model_config()
            dynamic_batch_size = model_config.get("settings", {}).get("batch_size", settings.BATCH_SIZE)
            total_chunks = len(chunks)
            
            # ì„ë² ë”© ìƒì„± ì‹œì—ëŠ” ì„¤ì •ëœ ë°°ì¹˜ í¬ê¸° ì‚¬ìš© (í† í° ì œí•œ ê³ ë ¤)
            batch_size = dynamic_batch_size
            
            print(f"ChromaDBì— {total_chunks}ê°œ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ì„ë² ë”© ì•ˆì •ì„±ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ì¡°ì •)")
            
            for batch_start in range(0, total_chunks, batch_size):
                batch_end = min(batch_start + batch_size, total_chunks)
                batch_chunks = chunks[batch_start:batch_end]
                
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {batch_start + 1}-{batch_end}/{total_chunks}")
                print(f"ğŸ” ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ í™•ì¸ ì¤‘...")
                
                # ChromaDB ìƒíƒœ í™•ì¸
                await self._ensure_client()
                if not VectorService._collection:
                    raise RuntimeError("ChromaDB ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"âœ… ChromaDB í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
                
                # ë°°ì¹˜ì˜ ê° ì²­í¬ì— ê³ ìœ  ID ìƒì„±
                print(f"ğŸ“ ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„ ì¤‘... ({len(batch_chunks)}ê°œ ì²­í¬)")
                chunk_ids = []
                chunk_texts = []
                chunk_metadatas = []
                
                for i, chunk in enumerate(batch_chunks):
                    actual_index = batch_start + i
                    chunk_id = f"{file_id}_chunk_{actual_index}"
                    chunk_ids.append(chunk_id)
                    chunk_texts.append(chunk)
                    
                    # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                    chunk_metadata = {
                        "file_id": file_id,
                        "chunk_index": actual_index,
                        "filename": metadata.get("filename", ""),
                        "category_id": metadata.get("category_id", ""),
                        "category_name": metadata.get("category_name", ""),
                        "flow_id": metadata.get("flow_id", ""),
                        "vectorization_method": "chromadb_batch",
                        "created_at": datetime.now().isoformat()
                    }
                    chunk_metadatas.append(chunk_metadata)
                
                try:
                    # ì„ë² ë”© ìƒì„±
                    embedding_function = await _create_embedding_function()
                    if not embedding_function:
                        print("ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - í…ìŠ¤íŠ¸ë§Œ ì €ì¥")
                        # ì„ë² ë”© ì—†ì´ ì €ì¥ (ChromaDBê°€ ë‚´ë¶€ í•¨ìˆ˜ ì‚¬ìš©)
                        VectorService._collection.add(
                            ids=chunk_ids,
                            documents=chunk_texts,
                            metadatas=chunk_metadatas
                        )
                    else:
                        # ì„ë² ë”© ìƒì„±í•˜ì—¬ ì €ì¥
                        print(f"ë°°ì¹˜ {batch_start + 1}-{batch_end} ì„ë² ë”© ìƒì„± ì¤‘...")
                        
                        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± (íƒ€ì„ì•„ì›ƒê³¼ ì¬ì‹œë„ í¬í•¨)
                        batch_texts = [chunk_text.strip() for chunk_text in chunk_texts]
                        
                        try:
                            # ì„ë² ë”© ìƒì„± (60ì´ˆ íƒ€ì„ì•„ì›ƒ)
                            print(f"ğŸ“¡ OpenAI API í˜¸ì¶œ ì‹œì‘... (ë°°ì¹˜ í¬ê¸°: {len(batch_texts)}, ìµœëŒ€ 60ì´ˆ ëŒ€ê¸°)")
                            print(f"ğŸ” API í‚¤ í™•ì¸: {'ìˆìŒ' if embedding_function.base_function.api_key else 'ì—†ìŒ'}")
                            print(f"ğŸ” ëª¨ë¸: {embedding_function.base_function.model_name}")
                            print(f"ğŸ” ì°¨ì›: {embedding_function.expected_dimension}")
                            
                            # ì„ë² ë”© ìƒì„± ì „ ì¶”ê°€ ìƒíƒœ ì²´í¬
                            start_time = time.time()
                            print(f"â±ï¸ ì„ë² ë”© ìƒì„± ì‹œì‘: {start_time}")
                            
                            # ì²« ë²ˆì§¸ ë°°ì¹˜ì¸ ê²½ìš° ì—°ê²° í…ŒìŠ¤íŠ¸
                            if batch_start == 0:
                                print("ğŸ§ª ì²« ë²ˆì§¸ ë°°ì¹˜ - ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
                                try:
                                    test_embedding = await asyncio.wait_for(
                                        asyncio.to_thread(embedding_function, ["ì—°ê²° í…ŒìŠ¤íŠ¸"]),
                                        timeout=30.0
                                    )
                                    print(f"âœ… ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ (ì„ë² ë”© ì°¨ì›: {len(test_embedding[0])})")
                                except Exception as test_error:
                                    print(f"âŒ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(test_error)}")
                                    raise test_error
                            
                            chunk_embeddings = await asyncio.wait_for(
                                asyncio.to_thread(embedding_function, batch_texts),
                                timeout=60.0
                            )
                            
                            end_time = time.time()
                            elapsed = end_time - start_time
                            print(f"âœ… ë°°ì¹˜ {batch_start + 1}-{batch_end} ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
                            
                        except asyncio.TimeoutError:
                            print(f"â° ë°°ì¹˜ {batch_start + 1}-{batch_end} ì„ë² ë”© ìƒì„± íƒ€ì„ì•„ì›ƒ (60ì´ˆ) - ì ì§„ì  ì¬ì‹œë„ ì¤‘...")
                            
                            # ì ì§„ì  ì¬ì‹œë„: ë°°ì¹˜ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì—¬ì„œ ì¬ì‹œë„
                            if len(batch_texts) > 1:
                                print(f"ğŸ”„ ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ ì¬ì‹œë„ (í¬ê¸°: {len(batch_texts)} -> {len(batch_texts)//2})")
                                mid_point = len(batch_texts) // 2
                                chunk_embeddings = []
                                
                                # ì²« ë²ˆì§¸ ì ˆë°˜ ì²˜ë¦¬
                                try:
                                    first_half = await asyncio.wait_for(
                                        asyncio.to_thread(embedding_function, batch_texts[:mid_point]),
                                        timeout=45.0
                                    )
                                    chunk_embeddings.extend(first_half)
                                    print(f"âœ… ì²« ë²ˆì§¸ ì ˆë°˜ ì™„ë£Œ ({mid_point}ê°œ)")
                                except Exception as first_error:
                                    print(f"âŒ ì²« ë²ˆì§¸ ì ˆë°˜ ì‹¤íŒ¨: {str(first_error)}")
                                    raise first_error
                                
                                # ë‘ ë²ˆì§¸ ì ˆë°˜ ì²˜ë¦¬
                                try:
                                    second_half = await asyncio.wait_for(
                                        asyncio.to_thread(embedding_function, batch_texts[mid_point:]),
                                        timeout=45.0
                                    )
                                    chunk_embeddings.extend(second_half)
                                    print(f"âœ… ë‘ ë²ˆì§¸ ì ˆë°˜ ì™„ë£Œ ({len(batch_texts) - mid_point}ê°œ)")
                                except Exception as second_error:
                                    print(f"âŒ ë‘ ë²ˆì§¸ ì ˆë°˜ ì‹¤íŒ¨: {str(second_error)}")
                                    raise second_error
                            else:
                                # ë‹¨ì¼ ì²­í¬ì¸ ê²½ìš° ë” ê¸´ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¬ì‹œë„
                                print(f"ğŸ”„ ë‹¨ì¼ ì²­í¬ ì¬ì‹œë„ (90ì´ˆ íƒ€ì„ì•„ì›ƒ)")
                                chunk_embeddings = await asyncio.wait_for(
                                    asyncio.to_thread(embedding_function, batch_texts),
                                    timeout=90.0
                                )
                                    
                        except Exception as embed_error:
                            print(f"âŒ ë°°ì¹˜ {batch_start + 1}-{batch_end} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(embed_error)}")
                            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {type(embed_error).__name__}")
                            raise embed_error
                        
                        # ChromaDBì— ë°°ì¹˜ ì¶”ê°€ (ì„ë² ë”© í¬í•¨)
                        VectorService._collection.add(
                            ids=chunk_ids,
                            embeddings=chunk_embeddings,
                            documents=chunk_texts,
                            metadatas=chunk_metadatas
                        )
                    print(f"ë°°ì¹˜ {batch_start + 1}-{batch_end} ì €ì¥ ì™„ë£Œ")
                    
                except Exception as batch_error:
                    print(f"ë°°ì¹˜ {batch_start + 1}-{batch_end} ì €ì¥ ì‹¤íŒ¨: {str(batch_error)}")
                    raise batch_error
            
            # SQLiteì— ë©”íƒ€ë°ì´í„° ì €ì¥
            await self._save_metadata_to_sqlite(file_id, metadata, len(chunks))
            
            print(f"âœ… ChromaDB ë²¡í„° ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_id}, ì´ ì²­í¬ ìˆ˜: {total_chunks}")
            return True
            
        except Exception as e:
            error_message = str(e)
            print(f"âŒ ChromaDB ë²¡í„° ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {error_message}")
            return False
    
    async def search_similar_chunks(self, query: str, top_k: int = 5, category_ids: List[str] = None) -> List[Dict[str, Any]]:
        """ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ì²­í¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” í™•ì¸ ë° ì¬ì‹œë„
        try:
            await self._ensure_client()
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {str(e)}")
        
        if not VectorService._collection:
            print("ChromaDB ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        try:
            # ê²€ìƒ‰ í•„í„° ì¤€ë¹„
            where_filter = None
            if category_ids:
                where_filter = {"category_id": {"$in": category_ids}}
            
            # ChromaDB ê²€ìƒ‰ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™”)
            print(f"ChromaDB ê²€ìƒ‰ ì‹œì‘: '{query}', top_k={top_k}")
            start_time = time.time()
            
            results = VectorService._collection.query(
                query_texts=[query],
                n_results=top_k,
                where=where_filter,
                include=["documents", "metadatas", "distances"]  # í•„ìš”í•œ ë°ì´í„°ë§Œ ìš”ì²­
            )
            
            search_time = time.time() - start_time
            print(f"ChromaDB ê²€ìƒ‰ ì™„ë£Œ: {search_time:.2f}ì´ˆ")
            
            # ê²°ê³¼ í˜•ì‹ ë³€í™˜
            search_results = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    # ChromaDB distanceë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨ -> ë†’ì„ìˆ˜ë¡ ìœ ì‚¬í•¨)
                    distance = results['distances'][0][i] if 'distances' in results else 0.5
                    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜: 1 - (distance / max_distance)
                    # ì„¤ì •ì—ì„œ ì •ê·œí™” íŒ©í„° ê°€ì ¸ì˜¤ê¸° (OpenAI ì„ë² ë”©ì€ ì¼ë°˜ì ìœ¼ë¡œ 0.0~2.0)
                    similarity_score = max(0.0, 1.0 - (distance / settings.DISTANCE_NORMALIZATION_FACTOR))
                    
                    search_results.append({
                        "chunk_id": results['ids'][0][i],
                        "text": doc,
                        "metadata": results['metadatas'][0][i],
                        "score": similarity_score,
                        "distance": distance  # ì›ë³¸ ê±°ë¦¬ë„ ë³´ê´€
                    })
                    
                    print(f"ê²€ìƒ‰ ê²°ê³¼ {i+1}: ê±°ë¦¬={distance:.3f}, ì ìˆ˜={similarity_score:.3f}, íŒŒì¼={results['metadatas'][0][i].get('filename', 'unknown')}")
            
            print(f"âœ… ChromaDB ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            print(f"âŒ ChromaDB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise RuntimeError(f"ChromaDB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def get_document_chunks(self, file_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ë¥¼ ChromaDBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” í™•ì¸ ë° ì¬ì‹œë„
        try:
            await self._ensure_client()
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {str(e)}")
        
        if not VectorService._collection:
            print("ChromaDB ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        try:
            # ChromaDBì—ì„œ íŒŒì¼ IDë¡œ í•„í„°ë§í•˜ì—¬ ì¡°íšŒ
            results = VectorService._collection.get(
                where={"file_id": file_id}
            )
            
            chunks = []
            if results['documents']:
                for i, doc in enumerate(results['documents']):
                    chunks.append({
                        "chunk_id": results['ids'][i],
                        "text": doc,
                        "metadata": results['metadatas'][i],
                        "file_id": file_id
                    })
            
            print(f"âœ… ChromaDBì—ì„œ íŒŒì¼ ë²¡í„° ì¡°íšŒ ì™„ë£Œ: {file_id}, {len(chunks)}ê°œ ë²¡í„°")
            return chunks
            
        except Exception as e:
            print(f"âŒ ChromaDB ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise RuntimeError(f"ChromaDB ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def delete_document_vectors(self, file_id: str) -> bool:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ë²¡í„°ë¥¼ ChromaDBì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” í™•ì¸ ë° ì¬ì‹œë„
        try:
            await self._ensure_client()
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {str(e)}")
        
        if not VectorService._collection:
            print("ChromaDB ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        try:
            # ChromaDBì—ì„œ íŒŒì¼ IDë¡œ í•„í„°ë§í•˜ì—¬ ì‚­ì œ
            VectorService._collection.delete(
                where={"file_id": file_id}
            )
            
            # SQLiteì—ì„œ ë©”íƒ€ë°ì´í„° ì‚­ì œ
            self.metadata_service.delete_metadata(file_id)
            
            print(f"âœ… ChromaDBì—ì„œ ë¬¸ì„œ ë²¡í„° ì‚­ì œ ì™„ë£Œ: {file_id}")
            return True
            
        except Exception as e:
            print(f"âŒ ChromaDB ë¬¸ì„œ ë²¡í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise RuntimeError(f"ChromaDB ë¬¸ì„œ ë²¡í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def _update_metadata_index(self, file_id: str, metadata: Dict[str, Any]):
        """ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. (DEPRECATED: ChromaDB ë©”íƒ€ë°ì´í„° ì‚¬ìš©)"""
        try:
            index_file_path = os.path.join(self.metadata_dir, 'index.json')
            
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ì½ê¸°
            index_data = {}
            if os.path.exists(index_file_path):
                with open(index_file_path, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            
            # ìƒˆ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            index_data[file_id] = {
                **metadata,
                "updated_at": datetime.now().isoformat()
            }
            
            # ì¸ë±ìŠ¤ ì €ì¥
            with open(index_file_path, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def _remove_from_metadata_index(self, file_id: str):
        """ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ì—ì„œ íŒŒì¼ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤. (DEPRECATED: ChromaDB ë©”íƒ€ë°ì´í„° ì‚¬ìš©)"""
        try:
            metadata_file_path = os.path.join(self.metadata_dir, "index.json")
            
            if os.path.exists(metadata_file_path):
                with open(metadata_file_path, 'r', encoding='utf-8') as f:
                    metadata_index = json.load(f)
                
                # íŒŒì¼ IDë¡œ ì œê±°
                if file_id in metadata_index:
                    del metadata_index[file_id]
                    
                    with open(metadata_file_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata_index, f, ensure_ascii=False, indent=2)
                    
                    print(f"ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ì—ì„œ ì œê±°: {file_id}")
            
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ ì œê±° ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def _save_metadata_to_sqlite(self, file_id: str, metadata: Dict[str, Any], chunk_count: int):
        """SQLiteì— ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            vector_metadata = VectorMetadata(
                file_id=file_id,
                filename=metadata.get("filename", ""),
                category_id=metadata.get("category_id"),
                category_name=metadata.get("category_name"),
                flow_id=metadata.get("flow_id"),
                processing_method=metadata.get("processing_method", "basic_text"),
                processing_time=metadata.get("processing_time", 0.0),
                chunk_count=chunk_count,
                file_size=metadata.get("file_size", 0),
                page_count=metadata.get("page_count"),
                table_count=metadata.get("table_count"),
                image_count=metadata.get("image_count")
            )
            
            # Docling ì˜µì…˜ì´ ìˆìœ¼ë©´ ì €ì¥
            if metadata.get("docling_options"):
                vector_metadata.set_docling_options(metadata["docling_options"])
            
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒì„±
            existing = self.metadata_service.get_metadata(file_id)
            if existing:
                self.metadata_service.update_metadata(
                    file_id,
                    filename=vector_metadata.filename,
                    category_id=vector_metadata.category_id,
                    category_name=vector_metadata.category_name,
                    flow_id=vector_metadata.flow_id,
                    processing_method=vector_metadata.processing_method,
                    processing_time=vector_metadata.processing_time,
                    chunk_count=vector_metadata.chunk_count,
                    file_size=vector_metadata.file_size,
                    page_count=vector_metadata.page_count,
                    table_count=vector_metadata.table_count,
                    image_count=vector_metadata.image_count,
                    docling_options=vector_metadata.docling_options
                )
            else:
                self.metadata_service.create_metadata(vector_metadata)
                
        except Exception as e:
            print(f"SQLite ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    async def _get_file_metadata(self, file_id: str) -> Optional[Dict[str, Any]]:
        """SQLiteì—ì„œ íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            metadata = self.metadata_service.get_metadata(file_id)
            if metadata:
                return {
                    "file_id": metadata.file_id,
                    "filename": metadata.filename,
                    "category_id": metadata.category_id,
                    "category_name": metadata.category_name,
                    "flow_id": metadata.flow_id,
                    "processing_method": metadata.processing_method,
                    "processing_time": metadata.processing_time,
                    "chunk_count": metadata.chunk_count,
                    "file_size": metadata.file_size,
                    "page_count": metadata.page_count,
                    "table_count": metadata.table_count,
                    "image_count": metadata.image_count,
                    "docling_options": metadata.get_docling_options(),
                    "created_at": metadata.created_at.isoformat(),
                    "updated_at": metadata.updated_at.isoformat()
                }
            return None
            
        except Exception as e:
            print(f"SQLite ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def reset_chromadb(self):
        """ChromaDB ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì™„ì „íˆ ë¦¬ì…‹í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ ì—°ê²° í•´ì œ
            VectorService._client = None
            VectorService._collection = None
            
            # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë°±ì—… ë° ì‚­ì œ
            import shutil
            if os.path.exists(self.vector_dir):
                backup_path = f"{self.vector_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                shutil.move(self.vector_dir, backup_path)
                print(f"ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ {backup_path}ë¡œ ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")
            
            # ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.vector_dir, exist_ok=True)
            print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì…‹ ì™„ë£Œ")
            
            # ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            if await self.create_chromadb_database():
                print("ìƒˆ ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            else:
                print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            
        except Exception as e:
            print(f"ChromaDB ë¦¬ì…‹ ì‹¤íŒ¨: {str(e)}")
    
    def get_chromadb_status(self) -> Dict[str, Any]:
        """ChromaDB ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ëŒ€ì‹œë³´ë“œìš© - ì´ˆê¸°í™” ì‹œë„ ì—†ì´ í˜„ì¬ ìƒíƒœë§Œ ì¡°íšŒ)."""
        # ê¸°ë³¸ ìƒíƒœ
        status = {
            "chromadb_available": CHROMADB_AVAILABLE,
            "collection_name": "langflow",
            "collection_count": 0,
            "client_initialized": VectorService._client is not None,
            "collection_initialized": VectorService._collection is not None,
            "vector_dir": self.vector_dir,
            "metadata_dir": self.metadata_dir,
        }
        
        # ChromaDBê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
        if not CHROMADB_AVAILABLE:
            status["status"] = "unavailable"
            status["message"] = "ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return status
        
        # í˜„ì¬ ì´ˆê¸°í™” ìƒíƒœë§Œ í™•ì¸ (ì´ˆê¸°í™” ì‹œë„í•˜ì§€ ì•ŠìŒ)
        if VectorService._client is None:
            status["status"] = "not_initialized"
            status["message"] = "ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
            if os.path.exists(chroma_db_path):
                status["has_existing_data"] = True
                status["message"] = "ChromaDB ìˆìŒ. ë²¡í„°í™”ì‹œ ìë™ ë¡œë“œ."
            else:
                status["has_existing_data"] = False
            
            return status
        
        if VectorService._collection is None:
            status["status"] = "client_only"
            status["message"] = "ChromaDB í´ë¼ì´ì–¸íŠ¸ëŠ” ìˆì§€ë§Œ ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return status
        
        # í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°ì—ë§Œ ì»¬ë ‰ì…˜ ì¹´ìš´íŠ¸ ì¡°íšŒ
        try:
            count = VectorService._collection.count()
            status["collection_count"] = count
            status["status"] = "healthy"
            status["message"] = f"ChromaDBê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤. ì´ {count}ê°œì˜ ë²¡í„°ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        except Exception as e:
            error_msg = str(e).lower()
            status["collection_count"] = 0
            status["collection_error"] = str(e)
            
            # ìŠ¤í‚¤ë§ˆ ì˜¤ë¥˜ ê°ì§€
            if "no such column" in error_msg:
                status["status"] = "schema_error"
                status["requires_migration"] = True
                status["migration_reason"] = "schema_mismatch"
                status["error"] = "ChromaDB ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜"
                status["message"] = "ChromaDB ìŠ¤í‚¤ë§ˆê°€ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¦¬ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤."
                status["solution"] = "ChromaDB ë¦¬ì…‹ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìŠ¤í‚¤ë§ˆë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”."
            else:
                status["status"] = "error"
                status["error"] = str(e)
                status["message"] = f"ChromaDB ì˜¤ë¥˜: {str(e)}"
        
        # ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
        try:
            if os.path.exists(self.vector_dir):
                status["vector_dir_exists"] = True
                status["vector_dir_files"] = len(os.listdir(self.vector_dir))
            else:
                status["vector_dir_exists"] = False
                status["vector_dir_files"] = 0
        except Exception as e:
            status["directory_error"] = str(e)
            status["vector_dir_exists"] = False
            status["vector_dir_files"] = 0
        
        return status
    
    def get_metadata_stats(self) -> Dict[str, Any]:
        """SQLite ë©”íƒ€ë°ì´í„° í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            return self.metadata_service.get_stats()
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "total_files": 0,
                "total_chunks": 0,
                "processing_methods": {},
                "database_path": "error"
            }
    
    def debug_chromadb_status(self):
        """ChromaDB ìƒíƒœë¥¼ ìì„¸íˆ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n=== ChromaDB ìƒì„¸ ìƒíƒœ ===")
        
        # ê¸°ë³¸ ìƒíƒœ
        status = self.get_chromadb_status()
        for key, value in status.items():
            print(f"{key}: {value}")
        
        # ì»¬ë ‰ì…˜ ì •ë³´
        if VectorService._collection:
            try:
                count = VectorService._collection.count()
                print(f"\nğŸ“Š ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {count}")
                
                # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
                if count > 0:
                    sample = VectorService._collection.peek(limit=1)
                    print(f"ğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ:")
                    print(f"  - ID: {sample['ids'][0]}")
                    print(f"  - ë©”íƒ€ë°ì´í„°: {sample['metadatas'][0]}")
                    print(f"  - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample['documents'][0])} ë¬¸ì")
                    
            except Exception as e:
                print(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ì •ë³´
        print(f"\nğŸ“ ë²¡í„° ë””ë ‰í† ë¦¬: {self.vector_dir}")
        if os.path.exists(self.vector_dir):
            files = os.listdir(self.vector_dir)
            print(f"ğŸ“‚ íŒŒì¼/í´ë” ìˆ˜: {len(files)}")
            for item in files:
                item_path = os.path.join(self.vector_dir, item)
                if os.path.isdir(item_path):
                    size = sum(os.path.getsize(os.path.join(item_path, f)) for f in os.listdir(item_path))
                    print(f"  ğŸ“ {item}/ (ë²¡í„° ë°ì´í„°: {size/1024/1024:.1f}MB)")
                else:
                    size = os.path.getsize(item_path)
                    print(f"  ğŸ“„ {item} ({size/1024:.1f}KB)")
        
        print("=== ChromaDB ìƒíƒœ í™•ì¸ ì™„ë£Œ ===\n")
    
    async def initialize_chromadb_manually(self) -> Dict[str, Any]:
        """ChromaDBë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë˜ëŠ” ì—°ê²°)."""
        result = {
            "success": False,
            "message": "",
            "error": None
        }
        
        try:
            # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if VectorService._client is not None and VectorService._collection is not None:
                result["success"] = True
                result["message"] = "ChromaDBê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                return result
            
            print("ChromaDB ìˆ˜ë™ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
            chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
            
            if os.path.exists(chroma_db_path):
                # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°
                print("ê¸°ì¡´ ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ë°œê²¬ - ì—°ê²° ì‹œë„")
                await self._connect_to_chromadb()
            else:
                # ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
                print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
                if not await self.create_chromadb_database():
                    result["error"] = "ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    return result
                
                # ìƒì„±ëœ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°
                await self._connect_to_chromadb()
            
            # ì´ˆê¸°í™” ì„±ê³µ í™•ì¸
            if VectorService._client is not None and VectorService._collection is not None:
                result["success"] = True
                result["message"] = "ChromaDB ì´ˆê¸°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                print("âœ… ChromaDB ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                result["error"] = "ChromaDB ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                print("âŒ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨")
                
        except Exception as e:
            error_msg = str(e)
            result["error"] = error_msg
            result["message"] = f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
            print(f"âŒ ChromaDB ì´ˆê¸°í™” ì˜¤ë¥˜: {error_msg}")
        
        return result
    
    async def migrate_from_deprecated_config(self) -> bool:
        """Deprecated ì„¤ì •ì—ì„œ ìƒˆ ì„¤ì •ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ (í•„ìš”í•œ ê²½ìš°)."""
        try:
            # ì´ë¯¸ ìƒˆ í˜•ì‹ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶ˆí•„ìš”
            if VectorService._client is not None:
                print("ChromaDBê°€ ì´ë¯¸ ìƒˆ í˜•ì‹ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return True
            
            # ChromaDB ìˆ˜ë™ ì´ˆê¸°í™” ì‹œë„
            print("ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œë„ ì¤‘...")
            result = self.initialize_chromadb_manually()
            
            if result["success"]:
                print("ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
                return True
            else:
                print(f"ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                return False
                
        except Exception as e:
            print(f"ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def _safe_ensure_client(self):
        """ì•ˆì „í•œ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¡œê¹… ìµœì†Œí™”)"""
        try:
            if VectorService._client is None or VectorService._collection is None:
                await self._ensure_client()
            
            # ì´ˆê¸°í™” í™•ì¸
            if VectorService._client is None or VectorService._collection is None:
                return False
            
            # ì»¬ë ‰ì…˜ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸ (ì˜¤ë¥˜ë§Œ ë¡œê·¸)
            try:
                count = VectorService._collection.count()
                return True
            except Exception as e:
                print(f"ChromaDB ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
                return False
            
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def process_document_with_docling(
        self, 
        file_path: str, 
        file_id: str, 
        metadata: Dict[str, Any],
        docling_options: Optional[DoclingOptions] = None,
        use_parallel: bool = False
    ) -> Dict[str, Any]:
        """
        Doclingì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê³ ê¸‰ ì „ì²˜ë¦¬í•˜ê³  ë²¡í„°í™”í•©ë‹ˆë‹¤.
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
            file_id: íŒŒì¼ ê³ ìœ  ID
            metadata: íŒŒì¼ ë©”íƒ€ë°ì´í„°
            docling_options: Docling ì²˜ë¦¬ ì˜µì…˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ì •ë³´
        """
        try:
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            print(f"ğŸ”§ Docling ë¬¸ì„œ ì²˜ë¦¬ ìš”ì²­: {os.path.basename(file_path)} ({file_size / 1024 / 1024:.2f} MB)")
            
            # Doclingì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            print("ğŸ” Docling ì„œë¹„ìŠ¤ ê°€ìš©ì„± í™•ì¸ ì¤‘...")
            if not self.docling_service.is_available:
                print("âš ï¸ Doclingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                print("â†ªï¸ í´ë°± ì²˜ë¦¬ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return await self._fallback_text_processing(file_path, file_id, metadata)
            else:
                print("âœ… Docling ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥")
            
            # íŒŒì¼ í˜•ì‹ ì§€ì› ì—¬ë¶€ í™•ì¸
            print("ğŸ“‹ íŒŒì¼ í˜•ì‹ ì§€ì› ì—¬ë¶€ í™•ì¸ ì¤‘...")
            is_supported = await self.docling_service.is_supported_format(file_path)
            if not is_supported:
                print(f"âš ï¸ Doclingì´ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
                print("â†ªï¸ í´ë°± ì²˜ë¦¬ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return await self._fallback_text_processing(file_path, file_id, metadata)
            else:
                print(f"âœ… ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: {os.path.splitext(file_path)[1]}")
            
            # Docling ì˜µì…˜ ì„¤ì •
            print("âš™ï¸ Docling ì˜µì…˜ êµ¬ì„± ì¤‘...")
            if docling_options is None:
                docling_options = DoclingOptions(
                    output_format="markdown",
                    extract_tables=True,
                    extract_images=True,
                    ocr_enabled=False
                )
                print("ğŸ“‹ ê¸°ë³¸ Docling ì˜µì…˜ ì‚¬ìš©")
            else:
                print("ğŸ“‹ ì‚¬ìš©ì ì •ì˜ Docling ì˜µì…˜ ì ìš©")
            
            print(f"   - ì¶œë ¥ í˜•ì‹: {docling_options.output_format}")
            print(f"   - í…Œì´ë¸” ì¶”ì¶œ: {docling_options.extract_tables}")
            print(f"   - ì´ë¯¸ì§€ ì¶”ì¶œ: {docling_options.extract_images}")
            print(f"   - OCR í™œì„±í™”: {docling_options.ocr_enabled}")
            
            print(f"ğŸ”„ Doclingìœ¼ë¡œ ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹œì‘: {file_path}")
            
            # Doclingìœ¼ë¡œ ë¬¸ì„œ ì²˜ë¦¬
            print("ğŸš€ Docling ë¬¸ì„œ ì²˜ë¦¬ ìš”ì²­ ì‹œì‘...")
            docling_start_time = time.time()
            docling_result = await self.docling_service.process_document(file_path, docling_options)
            docling_elapsed = time.time() - docling_start_time
            
            if not docling_result.success:
                error_msg = docling_result.error
                is_timeout = docling_result.metadata.get("timeout", False)
                
                print(f"âŒ Docling ì²˜ë¦¬ ì‹¤íŒ¨ ({docling_elapsed:.2f}ì´ˆ ì†Œìš”): {error_msg}")
                
                if is_timeout:
                    print("â° íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ ì‹¤íŒ¨ - íŒŒì¼ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ë³µì¡í•¨")
                    print("ğŸ’¡ í•´ê²°ë°©ë²•: íŒŒì¼ í¬ê¸° ì¶•ì†Œ ë˜ëŠ” OCR ë¹„í™œì„±í™” ì‹œë„")
                else:
                    print(f"ğŸ” ì‹¤íŒ¨ ì›ì¸: {error_msg}")
                
                print("â†ªï¸ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ í´ë°± ì‹œë„ ì¤‘...")
                return await self._fallback_text_processing(file_path, file_id, metadata)
            
            print(f"âœ… Docling ì²˜ë¦¬ ì„±ê³µ ({docling_elapsed:.2f}ì´ˆ ì†Œìš”)")
            print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {len(docling_result.content.get('text', ''))} ê¸€ì ì¶”ì¶œ")
            
            # ì²˜ë¦¬ëœ ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            print("âœ‚ï¸ ì²­í¬ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
            chunk_start_time = time.time()
            chunks = await self._create_enhanced_chunks(docling_result, docling_options)
            chunk_elapsed = time.time() - chunk_start_time
            
            if not chunks:
                print(f"âš ï¸ ì²˜ë¦¬ëœ ì½˜í…ì¸ ì—ì„œ ìœ íš¨í•œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({chunk_elapsed:.2f}ì´ˆ ì†Œìš”)")
                return {
                    "success": False,
                    "error": "ì²­í¬ ìƒì„± ì‹¤íŒ¨",
                    "chunks_count": 0
                }
            
            print(f"âœ… ì²­í¬ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ({chunk_elapsed:.2f}ì´ˆ ì†Œìš”)")
            if chunks:
                avg_chunk_size = sum(len(chunk) for chunk in chunks) / len(chunks)
                print(f"ğŸ“Š ì²­í¬ í†µê³„: í‰ê·  ê¸¸ì´ {avg_chunk_size:.0f} ê¸€ì")
            
            # ë©”íƒ€ë°ì´í„°ì— Docling ì •ë³´ ì¶”ê°€
            print("ğŸ“‹ ë©”íƒ€ë°ì´í„° êµ¬ì„± ì¤‘...")
            enhanced_metadata = {
                **metadata,
                "processing_method": "docling",
                "docling_options": docling_options.dict(),
                "processing_time": docling_result.processing_time,
                "page_count": docling_result.metadata.get("page_count", 0),
                "table_count": docling_result.metadata.get("table_count", 0),
                "image_count": docling_result.metadata.get("image_count", 0)
            }
            print(f"âœ… ë©”íƒ€ë°ì´í„° êµ¬ì„± ì™„ë£Œ (í˜ì´ì§€: {enhanced_metadata['page_count']}, í…Œì´ë¸”: {enhanced_metadata['table_count']}, ì´ë¯¸ì§€: {enhanced_metadata['image_count']})")
            
            # ChromaDBì— ë²¡í„° ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬ ì˜µì…˜)
            print(f"ğŸ’¾ ChromaDBì— ë²¡í„° ì €ì¥ ì‹œì‘... ({len(chunks)}ê°œ ì²­í¬)")
            vector_start_time = time.time()
            
            # ì‹œìŠ¤í…œ ì„¤ì •ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€ í™•ì¸
            try:
                from ..api.settings import load_settings
                system_settings = load_settings()
                parallel_enabled = system_settings.get("enableParallelProcessing", True)
            except:
                parallel_enabled = True
                
            if use_parallel and parallel_enabled and len(chunks) > settings.BATCH_SIZE * 2:
                # ë³‘ë ¬ ì²˜ë¦¬ ì ìš© (í° íŒŒì¼ì—ë§Œ)
                print(f"ğŸš€ ë³‘ë ¬ ë²¡í„°í™” ëª¨ë“œ ì ìš© - {len(chunks)}ê°œ ì²­í¬ (ì„¤ì •ì—ì„œ í™œì„±í™”ë¨)")
                try:
                    from .parallel_vector_service import get_parallel_vector_service
                    parallel_service = get_parallel_vector_service()
                    
                    result = await parallel_service.vectorize_document_parallel(
                        file_id=file_id,
                        chunks=chunks,
                        metadata=enhanced_metadata
                    )
                    
                    success = result.get("success", False)
                    if success:
                        vector_elapsed = result.get("processing_time", time.time() - vector_start_time)
                        print(f"âœ… ë³‘ë ¬ ë²¡í„°í™” ì™„ë£Œ - ìºì‹œ íˆíŠ¸ìœ¨: {result.get('cache_hit_rate', 0):.1%}")
                        print(f"âš¡ ì„±ëŠ¥ í†µê³„: {result.get('performance_stats', {})}")
                    else:
                        print(f"âŒ ë³‘ë ¬ ë²¡í„°í™” ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ fallback: {result.get('error', '')}")
                        success = await self.add_document_chunks(file_id, chunks, enhanced_metadata)
                        vector_elapsed = time.time() - vector_start_time
                        
                except Exception as parallel_error:
                    print(f"âš ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì˜¤ë¥˜, ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ fallback: {parallel_error}")
                    success = await self.add_document_chunks(file_id, chunks, enhanced_metadata)
                    vector_elapsed = time.time() - vector_start_time
            else:
                # ê¸°ë³¸ ìˆœì°¨ ì²˜ë¦¬
                success = await self.add_document_chunks(file_id, chunks, enhanced_metadata)
                vector_elapsed = time.time() - vector_start_time
            
            if success:
                print(f"âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ ({vector_elapsed:.2f}ì´ˆ ì†Œìš”)")
                total_time = docling_elapsed + chunk_elapsed + vector_elapsed
                print(f"ğŸ‰ Docling ê¸°ë°˜ ë²¡í„°í™” ì „ì²´ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ (ì´ {total_time:.2f}ì´ˆ)")
                print(f"â±ï¸  ì‹œê°„ ë¶„ì„: Docling {docling_elapsed:.2f}ì´ˆ, ì²­í‚¹ {chunk_elapsed:.2f}ì´ˆ, ë²¡í„°í™” {vector_elapsed:.2f}ì´ˆ")
                return {
                    "success": True,
                    "chunks_count": len(chunks),
                    "processing_method": "docling",
                    "processing_time": docling_result.processing_time,
                    "docling_metadata": docling_result.metadata,
                    "total_pipeline_time": total_time
                }
            else:
                print(f"âŒ ë²¡í„° ì €ì¥ ì‹¤íŒ¨ ({vector_elapsed:.2f}ì´ˆ ì†Œìš”)")
                return {
                    "success": False,
                    "error": "ë²¡í„° ì €ì¥ ì‹¤íŒ¨",
                    "chunks_count": len(chunks)
                }
                
        except Exception as e:
            print(f"âŒ Docling ê¸°ë°˜ ë²¡í„°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "chunks_count": 0
            }
    
    async def _create_enhanced_chunks(
        self, 
        docling_result, 
        options: DoclingOptions
    ) -> List[str]:
        """
        Docling ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥ìƒëœ ì²­í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        chunks = []
        
        try:
            content = docling_result.content
            
            # ì£¼ìš” ì½˜í…ì¸  ì„ íƒ (ìš°ì„ ìˆœìœ„: markdown > text)
            if options.output_format == "markdown" and content.get("markdown"):
                main_content = content["markdown"]
                content_type = "markdown"
            elif content.get("text"):
                main_content = content["text"]
                content_type = "text"
            else:
                print("âš ï¸ ìœ íš¨í•œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return chunks
            
            print(f"ğŸ“ {content_type} í˜•ì‹ìœ¼ë¡œ ì²­í‚¹ ì‹œì‘ (ê¸¸ì´: {len(main_content)}ì)")
            
            # ëª¨ë¸ ì„¤ì •ì—ì„œ ì²­í‚¹ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            model_config = await get_current_model_config()
            chunk_size = model_config.get("settings", {}).get("chunk_size", settings.DEFAULT_CHUNK_SIZE)
            overlap_size = model_config.get("settings", {}).get("chunk_overlap", settings.DEFAULT_CHUNK_OVERLAP)
            
            # ë¬¸ë‹¨ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
            if content_type == "markdown":
                chunks.extend(await self._smart_markdown_chunking(main_content, chunk_size, overlap_size))
            else:
                chunks.extend(await self._smart_text_chunking(main_content, chunk_size, overlap_size))
            
            # í…Œì´ë¸” ì½˜í…ì¸  ì¶”ê°€
            if options.extract_tables and docling_result.tables:
                table_chunks = await self._create_table_chunks(docling_result.tables)
                chunks.extend(table_chunks)
                print(f"ğŸ“Š í…Œì´ë¸” ì²­í¬ {len(table_chunks)}ê°œ ì¶”ê°€")
            
            # ì´ë¯¸ì§€ ìº¡ì…˜ ì¶”ê°€ (Vision ëª¨ë¸ ì§€ì›)
            if options.extract_images and docling_result.images:
                image_chunks = await self._create_image_caption_chunks(docling_result.images)
                chunks.extend(image_chunks)
                print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ìº¡ì…˜ ì²­í¬ {len(image_chunks)}ê°œ ì¶”ê°€")
            
            # êµ¬ì¡° ì •ë³´ ê¸°ë°˜ ì²­í¬ (ì œëª©, ì„¹ì…˜ ë“±)
            if content.get("structure"):
                structure_chunks = await self._create_structure_chunks(content["structure"], main_content)
                chunks.extend(structure_chunks)
                print(f"ğŸ—ï¸ êµ¬ì¡° ê¸°ë°˜ ì²­í¬ {len(structure_chunks)}ê°œ ì¶”ê°€")
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            chunks = await self._deduplicate_chunks(chunks)
            
            print(f"âœ… ì²­í‚¹ ì™„ë£Œ: ì´ {len(chunks)}ê°œ ì²­í¬")
            return chunks
            
        except Exception as e:
            print(f"âŒ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return chunks
    
    async def _smart_markdown_chunking(self, content: str, chunk_size: int, overlap_size: int) -> List[str]:
        """Markdown í˜•ì‹ì— ìµœì í™”ëœ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (ê°œì„ ëœ ë²„ì „)"""
        chunks = []
        
        # Markdown í—¤ë”(##, ###, #### ë“±)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
        # í—¤ë” ìì²´ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ì •ê·œì‹ì˜ ìº¡ì²˜ ê·¸ë£¹ ì‚¬ìš©
        # í—¤ë”ê°€ ì—†ëŠ” ê¸´ í…ìŠ¤íŠ¸ë„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë¬¸ë‹¨ ë¶„í• ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
        
        all_paragraphs = content.split('\n\n')
        current_chunk = ""

        for paragraph in all_paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
            if len(current_chunk) + len(paragraph) + 2 <= chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk)
                
                # ë¬¸ë‹¨ ìì²´ê°€ ì²­í¬ í¬ê¸°ë³´ë‹¤ í° ê²½ìš°, ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¬ë¶„í• 
                if len(paragraph) > chunk_size:
                    sentence_chunks = await self._split_long_paragraph(paragraph, chunk_size, overlap_size)
                    chunks.extend(sentence_chunks)
                    current_chunk = "" # ë‹¤ìŒ ë¬¸ë‹¨ì„ ìƒˆ ì²­í¬ì—ì„œ ì‹œì‘
                else:
                    current_chunk = paragraph

        # ë§ˆì§€ë§‰ ë‚¨ì€ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(current_chunk)
        
        return [chunk for chunk in chunks if chunk.strip()]

    async def _split_long_paragraph(self, paragraph: str, chunk_size: int, overlap_size: int) -> List[str]:
        """ê¸´ ë¬¸ë‹¨ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        sentences = re.split(r'(?<=[.!?])\s+', paragraph)
        chunks = []
        current_chunk = ""

        for sentence in sentences:
            if len(current_chunk) + len(sentence) + 1 <= chunk_size:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)

        return chunks
    
    async def _smart_text_chunking(self, content: str, chunk_size: int, overlap_size: int) -> List[str]:
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (ë¬¸ë‹¨ ê²½ê³„ ê³ ë ¤)"""
        chunks = []
        
        # ë¬¸ë‹¨ë³„ë¡œ ë¶„í• 
        paragraphs = content.split('\n\n')
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
            if len(current_chunk + '\n\n' + paragraph) <= chunk_size:
                current_chunk += ('\n\n' if current_chunk else '') + paragraph
            else:
                # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk)
                
                # ë¬¸ë‹¨ì´ ì²­í¬ í¬ê¸°ë³´ë‹¤ í° ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                if len(paragraph) > chunk_size:
                    sentences = paragraph.split('. ')
                    sentence_chunk = ""
                    
                    for sentence in sentences:
                        if not sentence.endswith('.'):
                            sentence += '.'
                        
                        if len(sentence_chunk + ' ' + sentence) <= chunk_size:
                            sentence_chunk += (' ' if sentence_chunk else '') + sentence
                        else:
                            if sentence_chunk:
                                chunks.append(sentence_chunk)
                            sentence_chunk = sentence
                    
                    if sentence_chunk:
                        current_chunk = sentence_chunk
                    else:
                        current_chunk = ""
                else:
                    current_chunk = paragraph
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    async def _create_table_chunks(self, tables: List[Dict[str, Any]]) -> List[str]:
        """í…Œì´ë¸” ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë³€í™˜"""
        chunks = []
        
        for table in tables:
            table_content = f"[í…Œì´ë¸” {table.get('id', 'unknown')}]\n"
            table_content += f"í˜ì´ì§€: {table.get('page', 'unknown')}\n"
            
            if table.get('html'):
                table_content += f"HTML: {table['html']}\n"
            
            if table.get('content'):
                table_content += f"ë‚´ìš©: {table['content']}"
            
            chunks.append(table_content)
        
        return chunks
    
    async def _create_image_caption_chunks(self, images: List[Dict[str, Any]]) -> List[str]:
        """ì´ë¯¸ì§€ ìº¡ì…˜ì„ ì²­í¬ë¡œ ë³€í™˜ (Vision ëª¨ë¸ ì§€ì›ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° í¬í•¨)"""
        chunks = []
        
        for image in images:
            # ì´ë¯¸ì§€ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            image_path = image.get('image_path')
            if not image_path:
                continue
                
            # ì´ë¯¸ì§€ ìº¡ì…˜ ì²­í¬ ìƒì„±
            caption = image.get('caption', image.get('description', ''))
            page = image.get('page', 0)
            image_id = image.get('id', 'unknown')
            
            # ìº¡ì…˜ í…ìŠ¤íŠ¸ì— ì´ë¯¸ì§€ ê²½ë¡œ í¬í•¨ (Vision ëª¨ë¸ì´ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡)
            caption_text = f"[ì´ë¯¸ì§€: {image_path}] {caption}"
            
            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í¬í•¨
            if page > 0:
                caption_text += f" (í˜ì´ì§€ {page})"
            
            chunks.append(caption_text)
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±: {image_id} -> {len(caption_text)} ê¸€ì")
        
        return chunks
    
    async def _create_structure_chunks(self, structure: List[Dict[str, Any]], content: str) -> List[str]:
        """ë¬¸ì„œ êµ¬ì¡° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ ìƒì„±"""
        chunks = []
        
        # ì œëª©/í—¤ë”©ë§Œ ë³„ë„ë¡œ ì¸ë±ì‹±
        headings = [item for item in structure if 'title' in item.get('type', '').lower() or 'heading' in item.get('type', '').lower()]
        
        for heading in headings[:10]:  # ìµœëŒ€ 10ê°œì˜ ì£¼ìš” í—¤ë”©ë§Œ
            heading_text = f"[êµ¬ì¡°: {heading.get('type', 'unknown')}] {heading.get('text_preview', '')}"
            chunks.append(heading_text)
        
        return chunks
    
    async def _deduplicate_chunks(self, chunks: List[str]) -> List[str]:
        """ì¤‘ë³µ ì²­í¬ ì œê±°"""
        seen = set()
        unique_chunks = []
        
        for chunk in chunks:
            # ê³µë°± ì •ê·œí™” í›„ ì¤‘ë³µ í™•ì¸
            normalized = ' '.join(chunk.split())
            chunk_hash = hash(normalized)
            
            if chunk_hash not in seen and len(normalized) > 50:  # ìµœì†Œ 50ì ì´ìƒ
                seen.add(chunk_hash)
                unique_chunks.append(chunk)
        
        return unique_chunks
    
    async def _fallback_text_processing(self, file_path: str, file_id: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Docling ë¹„í™œì„±í™” ì‹œ ë˜ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œì˜ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        try:
            import os
            file_extension = os.path.splitext(file_path)[1].lower()
            filename = os.path.basename(file_path)
            
            print(f"ğŸ“„ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œì‘: {filename} ({file_extension})")
            start_time = time.time()
            
            # FileService í†µí•© í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‚¬ìš© (í•œê¸€ ì²˜ë¦¬ì— ìµœì í™”ëœ ë°©ì‹)
            from .file_service import FileService
            file_service = FileService()
            
            # ëª¨ë“  íŒŒì¼ í˜•ì‹ì„ FileServiceì—ì„œ ì²˜ë¦¬
            print(f"ğŸ“„ FileService í†µí•© í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‚¬ìš©: {file_extension}")
            content = await file_service.extract_text_from_file(file_path)
            
            processing_time = time.time() - start_time
            
            # ì¶”ì¶œ ê²°ê³¼ ê²€ì¦
            if not content or content.strip() == "":
                return {
                    "success": False,
                    "error": f"íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({file_extension})",
                    "chunks_count": 0,
                    "processing_method": "basic_text",
                    "processing_time": processing_time
                }
            
            print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(content):,}ì ({processing_time:.2f}ì´ˆ ì†Œìš”)")
            
            # ëª¨ë¸ ì„¤ì •ì—ì„œ ì²­í‚¹ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            model_config = await get_current_model_config()
            chunk_size = model_config.get("settings", {}).get("chunk_size", settings.DEFAULT_CHUNK_SIZE)
            overlap_size = model_config.get("settings", {}).get("chunk_overlap", settings.DEFAULT_CHUNK_OVERLAP)
            
            # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (ë™ì  ì„¤ì •ê°’ ì‚¬ìš©)
            chunks = await self._smart_text_chunking(content, chunk_size, overlap_size)
            
            if not chunks:
                return {
                    "success": False,
                    "error": "ìœ íš¨í•œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "chunks_count": 0,
                    "processing_method": "basic_text",
                    "processing_time": processing_time
                }
            
            print(f"ğŸ“ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            fallback_metadata = {
                **metadata,
                "processing_method": "basic_text_optimized",
                "processing_time": processing_time,
                "file_type": file_extension,
                "text_extraction_method": self._get_extraction_method_name(file_extension)
            }
            
            # ë²¡í„° ì €ì¥
            print("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
            success = await self.add_document_chunks(file_id, chunks, fallback_metadata)
            
            if success:
                print(f"âœ… ë²¡í„°í™” ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ì €ì¥ë¨")
            else:
                print("âŒ ë²¡í„° ì €ì¥ ì‹¤íŒ¨")
            
            return {
                "success": success,
                "chunks_count": len(chunks),
                "processing_method": "basic_text_optimized",
                "processing_time": processing_time,
                "text_length": len(content),
                "extraction_method": self._get_extraction_method_name(file_extension)
            }
            
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            import traceback
            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "chunks_count": 0,
                "processing_method": "basic_text_failed"
            }
    
    def _get_extraction_method_name(self, file_extension: str) -> str:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì¶”ì¶œ ë°©ë²• ì´ë¦„ ë°˜í™˜"""
        method_map = {
            '.pdf': 'pdfminer.six + pypdf',
            '.docx': 'python-docx',
            '.pptx': 'python-pptx', 
            '.xlsx': 'openpyxl',
            '.doc': 'python-docx (legacy)',
            '.ppt': 'python-pptx (legacy)',
            '.xls': 'openpyxl (legacy)',
            '.txt': 'direct_read',
            '.md': 'direct_read',
            '.csv': 'direct_read',
            '.html': 'beautifulsoup4',
            '.htm': 'beautifulsoup4'
        }
        return method_map.get(file_extension, 'direct_read_fallback')
    
    async def vectorize_with_docling_pipeline(
        self, 
        file_path: str, 
        file_id: str, 
        metadata: Dict[str, Any],
        enable_docling: bool = True,
        docling_options: Optional[DoclingOptions] = None,
        use_parallel: bool = True
    ) -> Dict[str, Any]:
        """
        í†µí•©ëœ ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ (Docling í™œìš© ê°€ëŠ¥)
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            file_id: íŒŒì¼ ID
            metadata: ë©”íƒ€ë°ì´í„°
            enable_docling: Docling ì‚¬ìš© ì—¬ë¶€
            docling_options: Docling ì˜µì…˜
            
        Returns:
            ë²¡í„°í™” ê²°ê³¼
        """
        try:
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            filename = metadata.get("filename", os.path.basename(file_path))
            docling_status = "í™œì„±í™”" if enable_docling else "ë¹„í™œì„±í™”"
            print(f"ğŸ”§ ë²¡í„°í™” ì‹œì  Docling {docling_status} (ì„¤ì • ê¸°ë°˜): {filename}")
            print(f"ğŸš€ í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘: {file_path}")
            print(f"ğŸ“Š íŒŒì¼ ì •ë³´: {file_size / 1024 / 1024:.2f} MB, Docling í™œì„±í™”: {enable_docling}")
            
            # Docling ì„¤ì • ìƒíƒœë¥¼ ëª…í™•í•˜ê²Œ ì¶œë ¥
            print(f"ğŸ” Docling ì„¤ì • ìƒíƒœ:")
            print(f"   - enable_docling: {enable_docling}")
            print(f"   - docling_service.is_available: {self.docling_service.is_available}")
            print(f"   - docling_options: {docling_options.dict() if docling_options else 'None'}")
            
            if enable_docling and self.docling_service.is_available:
                print("ğŸ”§ Docling í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘...")
                # Doclingì„ ìš°ì„ ì ìœ¼ë¡œ ì‹œë„
                result = await self.process_document_with_docling(
                    file_path, file_id, metadata, docling_options, use_parallel
                )
                
                if result["success"]:
                    print("âœ… Docling ê¸°ë°˜ ë²¡í„°í™” ì„±ê³µ")
                    return result
                else:
                    print(f"âš ï¸ Docling ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    print("â†ªï¸ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ì „í™˜ ì¤‘...")
            elif not enable_docling:
                print("ğŸ“ Docling ë¹„í™œì„±í™”ë¨ (ì‚¬ìš©ì ì„¤ì •)")
            else:
                print("ğŸ“ Docling ì‚¬ìš© ë¶ˆê°€ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ ë˜ëŠ” ì˜¤ë¥˜)")
            
            # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ í´ë°±
            print("ğŸ“ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œì‘...")
            fallback_start_time = time.time()
            result = await self._fallback_text_processing(file_path, file_id, metadata)
            fallback_elapsed = time.time() - fallback_start_time
            
            if result["success"]:
                print(f"âœ… ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„±ê³µ ({fallback_elapsed:.2f}ì´ˆ ì†Œìš”)")
                print(f"âš™ï¸ ì²˜ë¦¬ ë°©ë²•: {result.get('processing_method', 'ê¸°ë³¸')}")
            else:
                print(f"âŒ ëª¨ë“  ì²˜ë¦¬ ë°©ë²• ì‹¤íŒ¨ ({fallback_elapsed:.2f}ì´ˆ ì†Œìš”): {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
            return result
            
        except Exception as e:
            print(f"âŒ ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "chunks_count": 0
            }
    

    
    def _backup_and_reset_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ë°±ì—…í•˜ê³  ë¦¬ì…‹í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ ì—°ê²° í•´ì œ
            VectorService._client = None
            VectorService._collection = None
            
            import shutil
            import time
            
            # ì ê¹ ëŒ€ê¸°í•˜ì—¬ íŒŒì¼ ì ê¸ˆ í•´ì œ
            time.sleep(0.5)
            
            if os.path.exists(self.vector_dir):
                try:
                    backup_path = f"{self.vector_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    shutil.move(self.vector_dir, backup_path)
                    print(f"ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ {backup_path}ë¡œ ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")
                except Exception as backup_error:
                    print(f"ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤íŒ¨: {str(backup_error)}")
                    print("ë°±ì—… ì—†ì´ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...")
                    # ë°±ì—… ì‹¤íŒ¨ ì‹œ ê°œë³„ íŒŒì¼ ì •ë¦¬ ì‹œë„
                    try:
                        for file in os.listdir(self.vector_dir):
                            file_path = os.path.join(self.vector_dir, file)
                            try:
                                if os.path.isfile(file_path):
                                    os.unlink(file_path)
                                    print(f"ì‚­ì œë¨: {file}")
                                elif os.path.isdir(file_path):
                                    shutil.rmtree(file_path)
                                    print(f"ë””ë ‰í† ë¦¬ ì‚­ì œë¨: {file}")
                            except Exception as file_error:
                                print(f"ì‚­ì œ ì‹¤íŒ¨: {file} - {str(file_error)} (ê³„ì† ì§„í–‰)")
                    except Exception as cleanup_error:
                        print(f"ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {str(cleanup_error)}")
            
            # ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.vector_dir, exist_ok=True)
            print("ìƒˆë¡œìš´ ChromaDB ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì…‹ ê³¼ì •ì—ì„œ ì˜¤ë¥˜: {str(e)}")
            print("ChromaDBë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    
    async def _create_fresh_chromadb(self):
        """ìƒˆë¡œìš´ ChromaDB í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            import chromadb
            
            # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            try:
                VectorService._client = chromadb.PersistentClient(
                    path=self.vector_dir,
                    settings=Settings(
                        anonymized_telemetry=False,
                        allow_reset=True
                    )
                )
            except Exception:
                try:
                    VectorService._client = chromadb.PersistentClient(path=self.vector_dir)
                except Exception as fallback_error:
                    print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(fallback_error)}")
                    return False
            
            # ì»¬ë ‰ì…˜ ìƒì„±
            self.collection_name = "langflow"
            
            try:
                VectorService._collection = VectorService._client.get_collection(name=self.collection_name)
                print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚¬ìš©")
            except Exception:
                # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                # ë™ì  ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
                embedding_function = await _create_embedding_function()
                if not embedding_function:
                    print("ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
                
                VectorService._collection = VectorService._client.create_collection(
                    name=self.collection_name,
                    embedding_function=embedding_function,
                    metadata={"description": "LangFlow ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"}
                )
                print(f"ìƒˆ ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            print(f"ìƒˆ ChromaDB ìƒì„± ì‹¤íŒ¨: {str(e)}")
            VectorService._client = None
            VectorService._collection = None
            return False

    async def find_orphaned_vectors(self) -> Dict[str, Any]:
        """ê³ ì•„ ë²¡í„°(íŒŒì¼ì´ ì‚­ì œë˜ì—ˆì§€ë§Œ ë²¡í„°ëŠ” ë‚¨ì•„ìˆëŠ” ê²½ìš°)ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        try:
            await self._ensure_client()
            
            # ChromaDBì—ì„œ ëª¨ë“  ë²¡í„° ì¡°íšŒ
            all_results = VectorService._collection.get()
            
            # íŒŒì¼ ì„œë¹„ìŠ¤ì—ì„œ í˜„ì¬ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            from ..services.file_service import FileService
            file_service = FileService()
            current_files = await file_service.list_files()
            current_file_ids = {f.file_id for f in current_files}
            
            orphaned_vectors = []
            total_vectors = len(all_results['ids']) if all_results['ids'] else 0
            
            # ë²¡í„° ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì •ìƒì ì¸ ì‘ë‹µ
            if total_vectors == 0:
                return {
                    'total_vectors': 0,
                    'orphaned_vectors': [],
                    'orphaned_count': 0,
                    'current_files_count': len(current_files),
                    'message': 'ë²¡í„°í™”ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë²¡í„°í™”ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”.'
                }
            
            if all_results['metadatas']:
                for i, metadata in enumerate(all_results['metadatas']):
                    if metadata and 'file_id' in metadata:
                        file_id = metadata['file_id']
                        if file_id not in current_file_ids:
                            orphaned_vectors.append({
                                'vector_id': all_results['ids'][i],
                                'file_id': file_id,
                                'text': all_results['documents'][i] if all_results['documents'] else '',
                                'metadata': metadata
                            })
            
            return {
                'total_vectors': total_vectors,
                'orphaned_vectors': orphaned_vectors,
                'orphaned_count': len(orphaned_vectors),
                'current_files_count': len(current_files),
                'message': f'ì´ {total_vectors}ê°œì˜ ë²¡í„° ì¤‘ {len(orphaned_vectors)}ê°œì˜ ê³ ì•„ ë²¡í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.'
            }
            
        except Exception as e:
            print(f"ê³ ì•„ ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                'error': str(e),
                'total_vectors': 0,
                'orphaned_vectors': [],
                'orphaned_count': 0,
                'current_files_count': 0,
                'message': f'ê³ ì•„ ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
            }
    
    async def cleanup_orphaned_vectors(self) -> Dict[str, Any]:
        """ê³ ì•„ ë²¡í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        try:
            orphaned_info = await self.find_orphaned_vectors()

            if 'error' in orphaned_info:
                return orphaned_info

            if orphaned_info['orphaned_count'] == 0:
                return {
                    'message': orphaned_info.get('message', 'ì •ë¦¬í•  ê³ ì•„ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'),
                    'cleaned_count': 0,
                    'total_vectors': orphaned_info.get('total_vectors', 0)
                }

            # ê³ ì•„ ë²¡í„° ì‚­ì œ
            orphaned_ids = [v['vector_id'] for v in orphaned_info['orphaned_vectors']]
            VectorService._collection.delete(ids=orphaned_ids)

            print(f"âœ… {orphaned_info['orphaned_count']}ê°œì˜ ê³ ì•„ ë²¡í„°ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")

            return {
                'message': f"{orphaned_info['orphaned_count']}ê°œì˜ ê³ ì•„ ë²¡í„°ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.",
                'cleaned_count': orphaned_info['orphaned_count'],
                'remaining_vectors': orphaned_info['total_vectors'] - orphaned_info['orphaned_count'],
                'total_vectors': orphaned_info['total_vectors']
            }
        except Exception as e:
            print(f"ê³ ì•„ ë²¡í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                'error': str(e),
                'cleaned_count': 0,
                'message': f'ê³ ì•„ ë²¡í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
            }

    async def clear_chromadb_documents_only(self) -> Dict[str, Any]:
        """ChromaDB ì»¬ë ‰ì…˜ êµ¬ì¡°ëŠ” ìœ ì§€í•˜ê³ , ë¬¸ì„œ(ë²¡í„°)ë§Œ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤."""
        try:
            await self._ensure_client()
            if not VectorService._collection:
                return {"success": False, "error": "ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

            # ì „ì²´ ì‚­ì œ: where ì¡°ê±´ ì—†ì´ ids=Noneìœ¼ë¡œ delete í˜¸ì¶œ ì‹œ ëª¨ë“  í•­ëª© ì œê±°
            try:
                # ì¼ë¶€ ë²„ì „ì€ ids ë˜ëŠ” whereê°€ í•„ìš” â†’ ëª¨ë“  idsë¥¼ ê°€ì ¸ì™€ ì‚­ì œ
                all_ids = []
                data = VectorService._collection.get()
                if data and data.get('ids'):
                    all_ids = data['ids']
                if all_ids:
                    VectorService._collection.delete(ids=all_ids)
                return {"success": True, "deleted_count": len(all_ids)}
            except Exception as e:
                return {"success": False, "error": str(e)}
        except Exception as e:
            return {"success": False, "error": str(e)}