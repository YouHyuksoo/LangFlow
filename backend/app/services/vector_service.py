import os
import json
import asyncio
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from ..core.config import settings
from .model_settings_service import get_current_model_config
from .docling_service import DoclingService
from ..models.schemas import DoclingOptions

# ChromaDB ê´€ë ¨ íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì‹œë„ (í•„ìˆ˜: chromadbë§Œ í™•ì¸)
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install chromadb ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

def _create_embedding_function():
    """í˜„ì¬ ëª¨ë¸ ì„¤ì •ì— ë”°ë¼ ì„ë² ë”© í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ë™ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì„¤ì • ë¡œë“œ
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            model_config = loop.run_until_complete(get_current_model_config())
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            model_config = loop.run_until_complete(get_current_model_config())
        
        embedding_config = model_config.get("embedding", {})
        
        provider = embedding_config.get("provider", "openai")
        model = embedding_config.get("model", "text-embedding-3-small")
        api_key = embedding_config.get("api_key", "")
        
        print(f"ì„ë² ë”© í•¨ìˆ˜ ìƒì„±: {provider} - {model}")
        
        if provider == "openai" and api_key:
            from chromadb.utils import embedding_functions
            return embedding_functions.OpenAIEmbeddingFunction(
                api_key=api_key,
                model_name=model
            )
        elif provider == "google" and api_key:
            # Google ì„ë² ë”© í•¨ìˆ˜ (í–¥í›„ í™•ì¥)
            from chromadb.utils import embedding_functions
            # Googleìš© ì„ë² ë”© í•¨ìˆ˜ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€
            return None
        elif provider == "ollama":
            # Ollama ì„ë² ë”© í•¨ìˆ˜ (í–¥í›„ í™•ì¥)
            return None
        else:
            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ì œê³µì—…ì²´ì´ê±°ë‚˜ API í‚¤ê°€ ì—†ìŒ: {provider}")
            return None
            
    except Exception as e:
        print(f"ì„ë² ë”© í•¨ìˆ˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

class VectorService:
    """ChromaDB ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ (ê°œì„ ëœ ì‹±ê¸€í†¤)"""
    
    _instance = None
    _initialized = False
    _client = None
    _collection = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(VectorService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if VectorService._initialized:
            return
            
        self.vector_dir = os.path.join(settings.DATA_DIR, 'vectors')
        self.metadata_dir = os.path.join(settings.DATA_DIR, 'vector_metadata')
        
        os.makedirs(self.vector_dir, exist_ok=True)
        os.makedirs(self.metadata_dir, exist_ok=True)
        
        # Docling ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.docling_service = DoclingService()
        
        # ì§€ì—° ì´ˆê¸°í™” - ì‹¤ì œ ë²¡í„°í™” ì‘ì—…ì—ì„œë§Œ ChromaDB ì—°ê²°ì„ ìˆ˜í–‰
        # íŒŒì¼ ì—…ë¡œë“œ ë“± ì¼ë°˜ì ì¸ ì‘ì—…ì—ì„œëŠ” ChromaDBë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
        print("VectorService ì´ˆê¸°í™” ì™„ë£Œ (ChromaDBëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ ì§€ì—° ë¡œë”©)")
        VectorService._initialized = True
    
    async def _ensure_client(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ìë™ ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤."""
        try:
            # í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ì´ ëª¨ë‘ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if VectorService._client is None or VectorService._collection is None:
                print("ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ìë™ ì—°ê²° ì‹œë„")
                
                # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
                if not os.path.exists(chroma_db_path):
                    raise RuntimeError(
                        f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {chroma_db_path}\n"
                        f"ì„¤ì •ì—ì„œ 'ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±'ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                    )
                
                # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²° ì‹œë„
                await self._connect_to_chromadb()
            
            # ì»¬ë ‰ì…˜ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
            try:
                count = VectorService._collection.count()
                print(f"ChromaDB ì»¬ë ‰ì…˜ ì—°ê²° í™•ì¸: ì´ ë²¡í„° ê°œìˆ˜ {count}")
            except Exception as e:
                print(f"ChromaDB ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
                raise RuntimeError(f"ChromaDB ì»¬ë ‰ì…˜ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            
            return VectorService._client
        
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    
    async def create_chromadb_database(self) -> bool:
        """ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ê³¼ ê¸°ë³¸ êµ¬ì¡°ë§Œ ìƒì„±í•©ë‹ˆë‹¤ (ì„¤ì •ì—ì„œë§Œ ì‚¬ìš©)."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        try:
            print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹œì‘...")
            
            import chromadb
            from chromadb.config import Settings
            
            # ì„ì‹œ í´ë¼ì´ì–¸íŠ¸ë¡œ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ê³¼ êµ¬ì¡° ìƒì„±
            temp_client = None
            temp_collection = None
            
            try:
                # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                temp_client = chromadb.PersistentClient(
                    path=self.vector_dir,
                    settings=Settings(
                        anonymized_telemetry=False,
                        allow_reset=True
                    )
                )
                
                # ê¸°ë³¸ ì»¬ë ‰ì…˜ ìƒì„±
                self.collection_name = "langflow"
                
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
                try:
                    existing_collection = temp_client.get_collection(name=self.collection_name)
                    print(f"ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë°œê²¬: {self.collection_name}")
                except Exception:
                    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                    from ..core.config import settings
                    
                    # ë™ì  ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
                    embedding_function = await _create_embedding_function()
                    if embedding_function:
                        print("ë™ì  ì„ë² ë”© í•¨ìˆ˜ ì„¤ì • ì™„ë£Œ")
                    else:
                        print("ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
                    
                    temp_collection = temp_client.create_collection(
                        name=self.collection_name,
                        embedding_function=embedding_function,
                        metadata={"description": "LangFlow ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ (OpenAI ì„ë² ë”© ì‚¬ìš©)"}
                    )
                    print(f"ìƒˆ ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name}")
                
                # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
                chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
                if os.path.exists(chroma_db_path):
                    print(f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ìƒì„±ë¨: {chroma_db_path}")
                    return True
                else:
                    print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return False
                    
            finally:
                # ì„ì‹œ ì—°ê²° í•´ì œ - ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¦‰ì‹œ í•´ì œ
                temp_client = None
                temp_collection = None
                print("ChromaDB ì„ì‹œ ì—°ê²° í•´ì œë¨")
                
        except Exception as e:
            print(f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def _connect_to_chromadb(self):
        """ê¸°ì¡´ ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì— ì—°ê²°í•©ë‹ˆë‹¤ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜)."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
        chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
        if not os.path.exists(chroma_db_path):
            raise RuntimeError(f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {chroma_db_path}\nì„¤ì •ì—ì„œ ë¨¼ì € 'ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±'ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        try:
            import chromadb
            from chromadb.config import Settings
            
            # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°
            try:
                VectorService._client = chromadb.PersistentClient(
                    path=self.vector_dir,
                    settings=Settings(
                        anonymized_telemetry=False,
                        allow_reset=True
                    )
                )
            except Exception:
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                VectorService._client = chromadb.PersistentClient(path=self.vector_dir)
            
            # ê¸°ë³¸ ì»¬ë ‰ì…˜ ì—°ê²° ë˜ëŠ” ìƒì„±
            self.collection_name = "langflow"
            try:
                VectorService._collection = VectorService._client.get_collection(name=self.collection_name)
                print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}' ì—°ê²° ì„±ê³µ")
            except Exception:
                print(f"ì»¬ë ‰ì…˜ '{self.collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                # ë™ì  ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
                embedding_function = await _create_embedding_function()
                if not embedding_function:
                    print("ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
                
                VectorService._collection = VectorService._client.create_collection(
                    name=self.collection_name,
                    embedding_function=embedding_function,
                    metadata={"description": "LangFlow ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"}
                )
                print(f"ìƒˆ ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            count = VectorService._collection.count()
            print(f"ChromaDB ì—°ê²° ì„±ê³µ - ë²¡í„° ê°œìˆ˜: {count}")
            
        except Exception as e:
            VectorService._client = None
            VectorService._collection = None
            raise RuntimeError(f"ChromaDB ì—°ê²° ì‹¤íŒ¨: {str(e)}")
    
    async def add_document_chunks(self, file_id: str, chunks: List[str], metadata: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì²­í¬ë¥¼ ChromaDBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            # ChromaDBê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            client = await self._ensure_client()
            if not CHROMADB_AVAILABLE or not VectorService._collection:
                print("âŒ ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except RuntimeError as e:
            print(f"âŒ ChromaDB ì´ˆê¸°í™” í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            return False
        
        try:
            # ë©”ëª¨ë¦¬ ë¶€ì¡±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            batch_size = 10  # í•œ ë²ˆì— 10ê°œì”© ì²˜ë¦¬
            total_chunks = len(chunks)
            
            print(f"ChromaDBì— {total_chunks}ê°œ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            
            for batch_start in range(0, total_chunks, batch_size):
                batch_end = min(batch_start + batch_size, total_chunks)
                batch_chunks = chunks[batch_start:batch_end]
                
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {batch_start + 1}-{batch_end}/{total_chunks}")
                
                # ë°°ì¹˜ì˜ ê° ì²­í¬ì— ê³ ìœ  ID ìƒì„±
                chunk_ids = []
                chunk_texts = []
                chunk_metadatas = []
                
                for i, chunk in enumerate(batch_chunks):
                    actual_index = batch_start + i
                    chunk_id = f"{file_id}_chunk_{actual_index}"
                    chunk_ids.append(chunk_id)
                    chunk_texts.append(chunk)
                    
                    # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                    chunk_metadata = {
                        "file_id": file_id,
                        "chunk_index": actual_index,
                        "filename": metadata.get("filename", ""),
                        "category_id": metadata.get("category_id", ""),
                        "category_name": metadata.get("category_name", ""),
                        "flow_id": metadata.get("flow_id", ""),
                        "vectorization_method": "chromadb_batch",
                        "created_at": datetime.now().isoformat()
                    }
                    chunk_metadatas.append(chunk_metadata)
                
                try:
                    # ChromaDBì— ë°°ì¹˜ ì¶”ê°€
                    VectorService._collection.add(
                        ids=chunk_ids,
                        documents=chunk_texts,
                        metadatas=chunk_metadatas
                    )
                    print(f"ë°°ì¹˜ {batch_start + 1}-{batch_end} ì €ì¥ ì™„ë£Œ")
                    
                except Exception as batch_error:
                    print(f"ë°°ì¹˜ {batch_start + 1}-{batch_end} ì €ì¥ ì‹¤íŒ¨: {str(batch_error)}")
                    raise batch_error
            
            # ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            await self._update_metadata_index(file_id, metadata)
            
            print(f"âœ… ChromaDB ë²¡í„° ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_id}, ì´ ì²­í¬ ìˆ˜: {total_chunks}")
            return True
            
        except Exception as e:
            error_message = str(e)
            print(f"âŒ ChromaDB ë²¡í„° ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {error_message}")
            return False
    
    async def search_similar_chunks(self, query: str, top_k: int = 5, category_ids: List[str] = None) -> List[Dict[str, Any]]:
        """ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ì²­í¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” í™•ì¸ ë° ì¬ì‹œë„
        try:
            await self._ensure_client()
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {str(e)}")
        
        if not VectorService._collection:
            print("ChromaDB ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        try:
            # ê²€ìƒ‰ í•„í„° ì¤€ë¹„
            where_filter = None
            if category_ids:
                where_filter = {"category_id": {"$in": category_ids}}
            
            # ChromaDB ê²€ìƒ‰ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™”)
            print(f"ChromaDB ê²€ìƒ‰ ì‹œì‘: '{query}', top_k={top_k}")
            start_time = time.time()
            
            results = VectorService._collection.query(
                query_texts=[query],
                n_results=top_k,
                where=where_filter,
                include=["documents", "metadatas", "distances"]  # í•„ìš”í•œ ë°ì´í„°ë§Œ ìš”ì²­
            )
            
            search_time = time.time() - start_time
            print(f"ChromaDB ê²€ìƒ‰ ì™„ë£Œ: {search_time:.2f}ì´ˆ")
            
            # ê²°ê³¼ í˜•ì‹ ë³€í™˜
            search_results = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    # ChromaDB distanceë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨ -> ë†’ì„ìˆ˜ë¡ ìœ ì‚¬í•¨)
                    distance = results['distances'][0][i] if 'distances' in results else 0.5
                    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜: 1 - (distance / max_distance)
                    # ì¼ë°˜ì ìœ¼ë¡œ 0.0~2.0 ì‚¬ì´ ê°’ì´ë¯€ë¡œ 2.0ìœ¼ë¡œ ì •ê·œí™”
                    similarity_score = max(0.0, 1.0 - (distance / 2.0))
                    
                    search_results.append({
                        "chunk_id": results['ids'][0][i],
                        "text": doc,
                        "metadata": results['metadatas'][0][i],
                        "score": similarity_score,
                        "distance": distance  # ì›ë³¸ ê±°ë¦¬ë„ ë³´ê´€
                    })
                    
                    print(f"ê²€ìƒ‰ ê²°ê³¼ {i+1}: ê±°ë¦¬={distance:.3f}, ì ìˆ˜={similarity_score:.3f}, íŒŒì¼={results['metadatas'][0][i].get('filename', 'unknown')}")
            
            print(f"âœ… ChromaDB ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            print(f"âŒ ChromaDB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise RuntimeError(f"ChromaDB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def get_document_chunks(self, file_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ë¥¼ ChromaDBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” í™•ì¸ ë° ì¬ì‹œë„
        try:
            await self._ensure_client()
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {str(e)}")
        
        if not VectorService._collection:
            print("ChromaDB ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        try:
            # ChromaDBì—ì„œ íŒŒì¼ IDë¡œ í•„í„°ë§í•˜ì—¬ ì¡°íšŒ
            results = VectorService._collection.get(
                where={"file_id": file_id}
            )
            
            chunks = []
            if results['documents']:
                for i, doc in enumerate(results['documents']):
                    chunks.append({
                        "chunk_id": results['ids'][i],
                        "text": doc,
                        "metadata": results['metadatas'][i],
                        "file_id": file_id
                    })
            
            print(f"âœ… ChromaDBì—ì„œ íŒŒì¼ ë²¡í„° ì¡°íšŒ ì™„ë£Œ: {file_id}, {len(chunks)}ê°œ ë²¡í„°")
            return chunks
            
        except Exception as e:
            print(f"âŒ ChromaDB ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise RuntimeError(f"ChromaDB ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def delete_document_vectors(self, file_id: str) -> bool:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ë²¡í„°ë¥¼ ChromaDBì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install chromadb langchain-chroma langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” í™•ì¸ ë° ì¬ì‹œë„
        try:
            await self._ensure_client()
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise RuntimeError(f"ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {str(e)}")
        
        if not VectorService._collection:
            print("ChromaDB ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise RuntimeError("ChromaDBë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ChromaDBê°€ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        try:
            # ChromaDBì—ì„œ íŒŒì¼ IDë¡œ í•„í„°ë§í•˜ì—¬ ì‚­ì œ
            VectorService._collection.delete(
                where={"file_id": file_id}
            )
            
            # ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ì—ì„œë„ ì œê±°
            await self._remove_from_metadata_index(file_id)
            
            print(f"âœ… ChromaDBì—ì„œ ë¬¸ì„œ ë²¡í„° ì‚­ì œ ì™„ë£Œ: {file_id}")
            return True
            
        except Exception as e:
            print(f"âŒ ChromaDB ë¬¸ì„œ ë²¡í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise RuntimeError(f"ChromaDB ë¬¸ì„œ ë²¡í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def _update_metadata_index(self, file_id: str, metadata: Dict[str, Any]):
        """ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            index_file_path = os.path.join(self.metadata_dir, 'index.json')
            
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ì½ê¸°
            index_data = {}
            if os.path.exists(index_file_path):
                with open(index_file_path, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            
            # ìƒˆ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            index_data[file_id] = {
                **metadata,
                "updated_at": datetime.now().isoformat()
            }
            
            # ì¸ë±ìŠ¤ ì €ì¥
            with open(index_file_path, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def _remove_from_metadata_index(self, file_id: str):
        """ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ì—ì„œ íŒŒì¼ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        try:
            metadata_file_path = os.path.join(self.metadata_dir, "index.json")
            
            if os.path.exists(metadata_file_path):
                with open(metadata_file_path, 'r', encoding='utf-8') as f:
                    metadata_index = json.load(f)
                
                # íŒŒì¼ IDë¡œ ì œê±°
                if file_id in metadata_index:
                    del metadata_index[file_id]
                    
                    with open(metadata_file_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata_index, f, ensure_ascii=False, indent=2)
                    
                    print(f"ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ì—ì„œ ì œê±°: {file_id}")
            
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ ì œê±° ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def _get_file_metadata(self, file_id: str) -> Optional[Dict[str, Any]]:
        """íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            metadata_file_path = os.path.join(self.metadata_dir, "index.json")
            
            if os.path.exists(metadata_file_path):
                with open(metadata_file_path, 'r', encoding='utf-8') as f:
                    metadata_index = json.load(f)
                
                return metadata_index.get(file_id)
            
            return None
            
        except Exception as e:
            print(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def reset_chromadb(self):
        """ChromaDB ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì™„ì „íˆ ë¦¬ì…‹í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ ì—°ê²° í•´ì œ
            VectorService._client = None
            VectorService._collection = None
            
            # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë°±ì—… ë° ì‚­ì œ
            import shutil
            if os.path.exists(self.vector_dir):
                backup_path = f"{self.vector_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                shutil.move(self.vector_dir, backup_path)
                print(f"ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ {backup_path}ë¡œ ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")
            
            # ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.vector_dir, exist_ok=True)
            print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì…‹ ì™„ë£Œ")
            
            # ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
            if await self.create_chromadb_database():
                print("ìƒˆ ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
            else:
                print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            
        except Exception as e:
            print(f"ChromaDB ë¦¬ì…‹ ì‹¤íŒ¨: {str(e)}")
    
    def get_chromadb_status(self) -> Dict[str, Any]:
        """ChromaDB ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ëŒ€ì‹œë³´ë“œìš© - ì´ˆê¸°í™” ì‹œë„ ì—†ì´ í˜„ì¬ ìƒíƒœë§Œ ì¡°íšŒ)."""
        # ê¸°ë³¸ ìƒíƒœ
        status = {
            "chromadb_available": CHROMADB_AVAILABLE,
            "collection_name": "langflow",
            "collection_count": 0,
            "client_initialized": VectorService._client is not None,
            "collection_initialized": VectorService._collection is not None,
            "vector_dir": self.vector_dir,
            "metadata_dir": self.metadata_dir,
        }
        
        # ChromaDBê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
        if not CHROMADB_AVAILABLE:
            status["status"] = "unavailable"
            status["message"] = "ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return status
        
        # í˜„ì¬ ì´ˆê¸°í™” ìƒíƒœë§Œ í™•ì¸ (ì´ˆê¸°í™” ì‹œë„í•˜ì§€ ì•ŠìŒ)
        if VectorService._client is None:
            status["status"] = "not_initialized"
            status["message"] = "ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
            if os.path.exists(chroma_db_path):
                status["has_existing_data"] = True
                status["message"] = "ChromaDB ìˆìŒ. ë²¡í„°í™”ì‹œ ìë™ ë¡œë“œ."
            else:
                status["has_existing_data"] = False
            
            return status
        
        if VectorService._collection is None:
            status["status"] = "client_only"
            status["message"] = "ChromaDB í´ë¼ì´ì–¸íŠ¸ëŠ” ìˆì§€ë§Œ ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return status
        
        # í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°ì—ë§Œ ì»¬ë ‰ì…˜ ì¹´ìš´íŠ¸ ì¡°íšŒ
        try:
            count = VectorService._collection.count()
            status["collection_count"] = count
            status["status"] = "healthy"
            status["message"] = f"ChromaDBê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤. ì´ {count}ê°œì˜ ë²¡í„°ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        except Exception as e:
            error_msg = str(e).lower()
            status["collection_count"] = 0
            status["collection_error"] = str(e)
            
            # ìŠ¤í‚¤ë§ˆ ì˜¤ë¥˜ ê°ì§€
            if "no such column" in error_msg:
                status["status"] = "schema_error"
                status["requires_migration"] = True
                status["migration_reason"] = "schema_mismatch"
                status["error"] = "ChromaDB ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜"
                status["message"] = "ChromaDB ìŠ¤í‚¤ë§ˆê°€ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¦¬ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤."
                status["solution"] = "ChromaDB ë¦¬ì…‹ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìŠ¤í‚¤ë§ˆë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”."
            else:
                status["status"] = "error"
                status["error"] = str(e)
                status["message"] = f"ChromaDB ì˜¤ë¥˜: {str(e)}"
        
        # ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
        try:
            if os.path.exists(self.vector_dir):
                status["vector_dir_exists"] = True
                status["vector_dir_files"] = len(os.listdir(self.vector_dir))
            else:
                status["vector_dir_exists"] = False
                status["vector_dir_files"] = 0
        except Exception as e:
            status["directory_error"] = str(e)
            status["vector_dir_exists"] = False
            status["vector_dir_files"] = 0
        
        return status
    
    def debug_chromadb_status(self):
        """ChromaDB ìƒíƒœë¥¼ ìì„¸íˆ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n=== ChromaDB ìƒì„¸ ìƒíƒœ ===")
        
        # ê¸°ë³¸ ìƒíƒœ
        status = self.get_chromadb_status()
        for key, value in status.items():
            print(f"{key}: {value}")
        
        # ì»¬ë ‰ì…˜ ì •ë³´
        if VectorService._collection:
            try:
                count = VectorService._collection.count()
                print(f"\nğŸ“Š ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {count}")
                
                # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
                if count > 0:
                    sample = VectorService._collection.peek(limit=1)
                    print(f"ğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ:")
                    print(f"  - ID: {sample['ids'][0]}")
                    print(f"  - ë©”íƒ€ë°ì´í„°: {sample['metadatas'][0]}")
                    print(f"  - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample['documents'][0])} ë¬¸ì")
                    
            except Exception as e:
                print(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ì •ë³´
        print(f"\nğŸ“ ë²¡í„° ë””ë ‰í† ë¦¬: {self.vector_dir}")
        if os.path.exists(self.vector_dir):
            files = os.listdir(self.vector_dir)
            print(f"ğŸ“‚ íŒŒì¼/í´ë” ìˆ˜: {len(files)}")
            for item in files:
                item_path = os.path.join(self.vector_dir, item)
                if os.path.isdir(item_path):
                    size = sum(os.path.getsize(os.path.join(item_path, f)) for f in os.listdir(item_path))
                    print(f"  ğŸ“ {item}/ (ë²¡í„° ë°ì´í„°: {size/1024/1024:.1f}MB)")
                else:
                    size = os.path.getsize(item_path)
                    print(f"  ğŸ“„ {item} ({size/1024:.1f}KB)")
        
        print("=== ChromaDB ìƒíƒœ í™•ì¸ ì™„ë£Œ ===\n")
    
    async def initialize_chromadb_manually(self) -> Dict[str, Any]:
        """ChromaDBë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë˜ëŠ” ì—°ê²°)."""
        result = {
            "success": False,
            "message": "",
            "error": None
        }
        
        try:
            # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if VectorService._client is not None and VectorService._collection is not None:
                result["success"] = True
                result["message"] = "ChromaDBê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                return result
            
            print("ChromaDB ìˆ˜ë™ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
            chroma_db_path = os.path.join(self.vector_dir, "chroma.sqlite3")
            
            if os.path.exists(chroma_db_path):
                # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°
                print("ê¸°ì¡´ ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ë°œê²¬ - ì—°ê²° ì‹œë„")
                await self._connect_to_chromadb()
            else:
                # ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
                print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
                if not await self.create_chromadb_database():
                    result["error"] = "ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    return result
                
                # ìƒì„±ëœ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°
                await self._connect_to_chromadb()
            
            # ì´ˆê¸°í™” ì„±ê³µ í™•ì¸
            if VectorService._client is not None and VectorService._collection is not None:
                result["success"] = True
                result["message"] = "ChromaDB ì´ˆê¸°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                print("âœ… ChromaDB ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                result["error"] = "ChromaDB ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                print("âŒ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨")
                
        except Exception as e:
            error_msg = str(e)
            result["error"] = error_msg
            result["message"] = f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
            print(f"âŒ ChromaDB ì´ˆê¸°í™” ì˜¤ë¥˜: {error_msg}")
        
        return result
    
    async def migrate_from_deprecated_config(self) -> bool:
        """Deprecated ì„¤ì •ì—ì„œ ìƒˆ ì„¤ì •ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ (í•„ìš”í•œ ê²½ìš°)."""
        try:
            # ì´ë¯¸ ìƒˆ í˜•ì‹ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶ˆí•„ìš”
            if VectorService._client is not None:
                print("ChromaDBê°€ ì´ë¯¸ ìƒˆ í˜•ì‹ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return True
            
            # ChromaDB ìˆ˜ë™ ì´ˆê¸°í™” ì‹œë„
            print("ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œë„ ì¤‘...")
            result = self.initialize_chromadb_manually()
            
            if result["success"]:
                print("ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
                return True
            else:
                print(f"ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                return False
                
        except Exception as e:
            print(f"ChromaDB ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def _safe_ensure_client(self):
        """ì•ˆì „í•œ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¡œê¹… ìµœì†Œí™”)"""
        try:
            if VectorService._client is None or VectorService._collection is None:
                await self._ensure_client()
            
            # ì´ˆê¸°í™” í™•ì¸
            if VectorService._client is None or VectorService._collection is None:
                return False
            
            # ì»¬ë ‰ì…˜ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸ (ì˜¤ë¥˜ë§Œ ë¡œê·¸)
            try:
                count = VectorService._collection.count()
                return True
            except Exception as e:
                print(f"ChromaDB ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
                return False
            
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def process_document_with_docling(
        self, 
        file_path: str, 
        file_id: str, 
        metadata: Dict[str, Any],
        docling_options: Optional[DoclingOptions] = None
    ) -> Dict[str, Any]:
        """
        Doclingì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê³ ê¸‰ ì „ì²˜ë¦¬í•˜ê³  ë²¡í„°í™”í•©ë‹ˆë‹¤.
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
            file_id: íŒŒì¼ ê³ ìœ  ID
            metadata: íŒŒì¼ ë©”íƒ€ë°ì´í„°
            docling_options: Docling ì²˜ë¦¬ ì˜µì…˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ì •ë³´
        """
        try:
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            print(f"ğŸ”§ Docling ë¬¸ì„œ ì²˜ë¦¬ ìš”ì²­: {os.path.basename(file_path)} ({file_size / 1024 / 1024:.2f} MB)")
            
            # Doclingì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            print("ğŸ” Docling ì„œë¹„ìŠ¤ ê°€ìš©ì„± í™•ì¸ ì¤‘...")
            if not self.docling_service.is_available:
                print("âš ï¸ Doclingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                print("â†ªï¸ í´ë°± ì²˜ë¦¬ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return await self._fallback_text_processing(file_path, file_id, metadata)
            else:
                print("âœ… Docling ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥")
            
            # íŒŒì¼ í˜•ì‹ ì§€ì› ì—¬ë¶€ í™•ì¸
            print("ğŸ“‹ íŒŒì¼ í˜•ì‹ ì§€ì› ì—¬ë¶€ í™•ì¸ ì¤‘...")
            is_supported = await self.docling_service.is_supported_format(file_path)
            if not is_supported:
                print(f"âš ï¸ Doclingì´ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
                print("â†ªï¸ í´ë°± ì²˜ë¦¬ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return await self._fallback_text_processing(file_path, file_id, metadata)
            else:
                print(f"âœ… ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: {os.path.splitext(file_path)[1]}")
            
            # Docling ì˜µì…˜ ì„¤ì •
            print("âš™ï¸ Docling ì˜µì…˜ êµ¬ì„± ì¤‘...")
            if docling_options is None:
                docling_options = DoclingOptions(
                    output_format="markdown",
                    extract_tables=True,
                    extract_images=True,
                    ocr_enabled=False
                )
                print("ğŸ“‹ ê¸°ë³¸ Docling ì˜µì…˜ ì‚¬ìš©")
            else:
                print("ğŸ“‹ ì‚¬ìš©ì ì •ì˜ Docling ì˜µì…˜ ì ìš©")
            
            print(f"   - ì¶œë ¥ í˜•ì‹: {docling_options.output_format}")
            print(f"   - í…Œì´ë¸” ì¶”ì¶œ: {docling_options.extract_tables}")
            print(f"   - ì´ë¯¸ì§€ ì¶”ì¶œ: {docling_options.extract_images}")
            print(f"   - OCR í™œì„±í™”: {docling_options.ocr_enabled}")
            
            print(f"ğŸ”„ Doclingìœ¼ë¡œ ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹œì‘: {file_path}")
            
            # Doclingìœ¼ë¡œ ë¬¸ì„œ ì²˜ë¦¬
            print("ğŸš€ Docling ë¬¸ì„œ ì²˜ë¦¬ ìš”ì²­ ì‹œì‘...")
            docling_start_time = time.time()
            docling_result = await self.docling_service.process_document(file_path, docling_options)
            docling_elapsed = time.time() - docling_start_time
            
            if not docling_result.success:
                error_msg = docling_result.error
                is_timeout = docling_result.metadata.get("timeout", False)
                
                print(f"âŒ Docling ì²˜ë¦¬ ì‹¤íŒ¨ ({docling_elapsed:.2f}ì´ˆ ì†Œìš”): {error_msg}")
                
                if is_timeout:
                    print("â° íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ ì‹¤íŒ¨ - íŒŒì¼ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ë³µì¡í•¨")
                    print("ğŸ’¡ í•´ê²°ë°©ë²•: íŒŒì¼ í¬ê¸° ì¶•ì†Œ ë˜ëŠ” OCR ë¹„í™œì„±í™” ì‹œë„")
                else:
                    print(f"ğŸ” ì‹¤íŒ¨ ì›ì¸: {error_msg}")
                
                print("â†ªï¸ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ í´ë°± ì‹œë„ ì¤‘...")
                return await self._fallback_text_processing(file_path, file_id, metadata)
            
            print(f"âœ… Docling ì²˜ë¦¬ ì„±ê³µ ({docling_elapsed:.2f}ì´ˆ ì†Œìš”)")
            print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {len(docling_result.content.get('text', ''))} ê¸€ì ì¶”ì¶œ")
            
            # ì²˜ë¦¬ëœ ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            print("âœ‚ï¸ ì²­í¬ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
            chunk_start_time = time.time()
            chunks = await self._create_enhanced_chunks(docling_result, docling_options)
            chunk_elapsed = time.time() - chunk_start_time
            
            if not chunks:
                print(f"âš ï¸ ì²˜ë¦¬ëœ ì½˜í…ì¸ ì—ì„œ ìœ íš¨í•œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({chunk_elapsed:.2f}ì´ˆ ì†Œìš”)")
                return {
                    "success": False,
                    "error": "ì²­í¬ ìƒì„± ì‹¤íŒ¨",
                    "chunks_count": 0
                }
            
            print(f"âœ… ì²­í¬ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ({chunk_elapsed:.2f}ì´ˆ ì†Œìš”)")
            if chunks:
                avg_chunk_size = sum(len(chunk) for chunk in chunks) / len(chunks)
                print(f"ğŸ“Š ì²­í¬ í†µê³„: í‰ê·  ê¸¸ì´ {avg_chunk_size:.0f} ê¸€ì")
            
            # ë©”íƒ€ë°ì´í„°ì— Docling ì •ë³´ ì¶”ê°€
            print("ğŸ“‹ ë©”íƒ€ë°ì´í„° êµ¬ì„± ì¤‘...")
            enhanced_metadata = {
                **metadata,
                "processing_method": "docling",
                "docling_options": docling_options.dict(),
                "processing_time": docling_result.processing_time,
                "page_count": docling_result.metadata.get("page_count", 0),
                "table_count": docling_result.metadata.get("table_count", 0),
                "image_count": docling_result.metadata.get("image_count", 0)
            }
            print(f"âœ… ë©”íƒ€ë°ì´í„° êµ¬ì„± ì™„ë£Œ (í˜ì´ì§€: {enhanced_metadata['page_count']}, í…Œì´ë¸”: {enhanced_metadata['table_count']}, ì´ë¯¸ì§€: {enhanced_metadata['image_count']})")
            
            # ChromaDBì— ë²¡í„° ì €ì¥
            print(f"ğŸ’¾ ChromaDBì— ë²¡í„° ì €ì¥ ì‹œì‘... ({len(chunks)}ê°œ ì²­í¬)")
            vector_start_time = time.time()
            success = await self.add_document_chunks(file_id, chunks, enhanced_metadata)
            vector_elapsed = time.time() - vector_start_time
            
            if success:
                print(f"âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ ({vector_elapsed:.2f}ì´ˆ ì†Œìš”)")
                total_time = docling_elapsed + chunk_elapsed + vector_elapsed
                print(f"ğŸ‰ Docling ê¸°ë°˜ ë²¡í„°í™” ì „ì²´ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ (ì´ {total_time:.2f}ì´ˆ)")
                print(f"â±ï¸  ì‹œê°„ ë¶„ì„: Docling {docling_elapsed:.2f}ì´ˆ, ì²­í‚¹ {chunk_elapsed:.2f}ì´ˆ, ë²¡í„°í™” {vector_elapsed:.2f}ì´ˆ")
                return {
                    "success": True,
                    "chunks_count": len(chunks),
                    "processing_method": "docling",
                    "processing_time": docling_result.processing_time,
                    "docling_metadata": docling_result.metadata,
                    "total_pipeline_time": total_time
                }
            else:
                print(f"âŒ ë²¡í„° ì €ì¥ ì‹¤íŒ¨ ({vector_elapsed:.2f}ì´ˆ ì†Œìš”)")
                return {
                    "success": False,
                    "error": "ë²¡í„° ì €ì¥ ì‹¤íŒ¨",
                    "chunks_count": len(chunks)
                }
                
        except Exception as e:
            print(f"âŒ Docling ê¸°ë°˜ ë²¡í„°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "chunks_count": 0
            }
    
    async def _create_enhanced_chunks(
        self, 
        docling_result, 
        options: DoclingOptions
    ) -> List[str]:
        """
        Docling ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥ìƒëœ ì²­í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        chunks = []
        
        try:
            content = docling_result.content
            
            # ì£¼ìš” ì½˜í…ì¸  ì„ íƒ (ìš°ì„ ìˆœìœ„: markdown > text)
            if options.output_format == "markdown" and content.get("markdown"):
                main_content = content["markdown"]
                content_type = "markdown"
            elif content.get("text"):
                main_content = content["text"]
                content_type = "text"
            else:
                print("âš ï¸ ìœ íš¨í•œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return chunks
            
            print(f"ğŸ“ {content_type} í˜•ì‹ìœ¼ë¡œ ì²­í‚¹ ì‹œì‘ (ê¸¸ì´: {len(main_content)}ì)")
            
            # ê¸°ë³¸ ì²­í‚¹ (1500ì ë‹¨ìœ„, 200ì ì˜¤ë²„ë©)
            chunk_size = 1500
            overlap_size = 200
            
            # ë¬¸ë‹¨ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
            if content_type == "markdown":
                chunks.extend(await self._smart_markdown_chunking(main_content, chunk_size, overlap_size))
            else:
                chunks.extend(await self._smart_text_chunking(main_content, chunk_size, overlap_size))
            
            # í…Œì´ë¸” ì½˜í…ì¸  ì¶”ê°€
            if options.extract_tables and docling_result.tables:
                table_chunks = await self._create_table_chunks(docling_result.tables)
                chunks.extend(table_chunks)
                print(f"ğŸ“Š í…Œì´ë¸” ì²­í¬ {len(table_chunks)}ê°œ ì¶”ê°€")
            
            # êµ¬ì¡° ì •ë³´ ê¸°ë°˜ ì²­í¬ (ì œëª©, ì„¹ì…˜ ë“±)
            if content.get("structure"):
                structure_chunks = await self._create_structure_chunks(content["structure"], main_content)
                chunks.extend(structure_chunks)
                print(f"ğŸ—ï¸ êµ¬ì¡° ê¸°ë°˜ ì²­í¬ {len(structure_chunks)}ê°œ ì¶”ê°€")
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            chunks = await self._deduplicate_chunks(chunks)
            
            print(f"âœ… ì²­í‚¹ ì™„ë£Œ: ì´ {len(chunks)}ê°œ ì²­í¬")
            return chunks
            
        except Exception as e:
            print(f"âŒ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return chunks
    
    async def _smart_markdown_chunking(self, content: str, chunk_size: int, overlap_size: int) -> List[str]:
        """Markdown í˜•ì‹ì— ìµœì í™”ëœ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹"""
        chunks = []
        
        # Markdown ì„¹ì…˜ë³„ë¡œ ë¶„í• 
        sections = content.split('\n## ')
        
        for i, section in enumerate(sections):
            if i > 0:  # ì²« ë²ˆì§¸ ì„¹ì…˜ì´ ì•„ë‹ˆë©´ í—¤ë” ë³µì›
                section = '## ' + section
            
            if len(section) <= chunk_size:
                # ì„¹ì…˜ì´ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                chunks.append(section.strip())
            else:
                # ì„¹ì…˜ì´ í° ê²½ìš° í•˜ìœ„ ë¶„í• 
                subsections = section.split('\n### ')
                current_chunk = ""
                
                for j, subsection in enumerate(subsections):
                    if j > 0:
                        subsection = '### ' + subsection
                    
                    if len(current_chunk + subsection) <= chunk_size:
                        current_chunk += ('\n' if current_chunk else '') + subsection
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        
                        # ì„œë¸Œì„¹ì…˜ë„ í° ê²½ìš° í…ìŠ¤íŠ¸ ì²­í‚¹
                        if len(subsection) > chunk_size:
                            text_chunks = await self._smart_text_chunking(subsection, chunk_size, overlap_size)
                            chunks.extend(text_chunks)
                            current_chunk = ""
                        else:
                            current_chunk = subsection
                
                if current_chunk:
                    chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if chunk.strip()]
    
    async def _smart_text_chunking(self, content: str, chunk_size: int, overlap_size: int) -> List[str]:
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (ë¬¸ë‹¨ ê²½ê³„ ê³ ë ¤)"""
        chunks = []
        
        # ë¬¸ë‹¨ë³„ë¡œ ë¶„í• 
        paragraphs = content.split('\n\n')
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
            if len(current_chunk + '\n\n' + paragraph) <= chunk_size:
                current_chunk += ('\n\n' if current_chunk else '') + paragraph
            else:
                # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk)
                
                # ë¬¸ë‹¨ì´ ì²­í¬ í¬ê¸°ë³´ë‹¤ í° ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                if len(paragraph) > chunk_size:
                    sentences = paragraph.split('. ')
                    sentence_chunk = ""
                    
                    for sentence in sentences:
                        if not sentence.endswith('.'):
                            sentence += '.'
                        
                        if len(sentence_chunk + ' ' + sentence) <= chunk_size:
                            sentence_chunk += (' ' if sentence_chunk else '') + sentence
                        else:
                            if sentence_chunk:
                                chunks.append(sentence_chunk)
                            sentence_chunk = sentence
                    
                    if sentence_chunk:
                        current_chunk = sentence_chunk
                    else:
                        current_chunk = ""
                else:
                    current_chunk = paragraph
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    async def _create_table_chunks(self, tables: List[Dict[str, Any]]) -> List[str]:
        """í…Œì´ë¸” ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë³€í™˜"""
        chunks = []
        
        for table in tables:
            table_content = f"[í…Œì´ë¸” {table.get('id', 'unknown')}]\n"
            table_content += f"í˜ì´ì§€: {table.get('page', 'unknown')}\n"
            
            if table.get('html'):
                table_content += f"HTML: {table['html']}\n"
            
            if table.get('content'):
                table_content += f"ë‚´ìš©: {table['content']}"
            
            chunks.append(table_content)
        
        return chunks
    
    async def _create_structure_chunks(self, structure: List[Dict[str, Any]], content: str) -> List[str]:
        """ë¬¸ì„œ êµ¬ì¡° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ ìƒì„±"""
        chunks = []
        
        # ì œëª©/í—¤ë”©ë§Œ ë³„ë„ë¡œ ì¸ë±ì‹±
        headings = [item for item in structure if 'title' in item.get('type', '').lower() or 'heading' in item.get('type', '').lower()]
        
        for heading in headings[:10]:  # ìµœëŒ€ 10ê°œì˜ ì£¼ìš” í—¤ë”©ë§Œ
            heading_text = f"[êµ¬ì¡°: {heading.get('type', 'unknown')}] {heading.get('text_preview', '')}"
            chunks.append(heading_text)
        
        return chunks
    
    async def _deduplicate_chunks(self, chunks: List[str]) -> List[str]:
        """ì¤‘ë³µ ì²­í¬ ì œê±°"""
        seen = set()
        unique_chunks = []
        
        for chunk in chunks:
            # ê³µë°± ì •ê·œí™” í›„ ì¤‘ë³µ í™•ì¸
            normalized = ' '.join(chunk.split())
            chunk_hash = hash(normalized)
            
            if chunk_hash not in seen and len(normalized) > 50:  # ìµœì†Œ 50ì ì´ìƒ
                seen.add(chunk_hash)
                unique_chunks.append(chunk)
        
        return unique_chunks
    
    async def _fallback_text_processing(self, file_path: str, file_id: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Doclingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œì˜ í´ë°± ì²˜ë¦¬"""
        try:
            print(f"ğŸ“„ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ì§„í–‰: {file_path}")
            
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            import os
            file_extension = os.path.splitext(file_path)[1].lower()
            
            if file_extension == '.pdf':
                # PDF íŒŒì¼ ì²˜ë¦¬ - FileServiceì˜ extract_text_from_pdf ì‚¬ìš©
                from .file_service import FileService
                file_service = FileService()
                content = await file_service.extract_text_from_pdf(file_path)
            elif file_extension in ['.doc', '.docx', '.ppt', '.pptx', '.xls', '.xlsx']:
                # Office íŒŒì¼ ì²˜ë¦¬
                from .file_service import FileService
                file_service = FileService()
                content = await file_service.extract_text_from_office(file_path)
            else:
                # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ (txt, md ë“±)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    # UTF-8ë¡œ ì½ì„ ìˆ˜ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
                    try:
                        with open(file_path, 'r', encoding='cp949') as f:
                            content = f.read()
                    except:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            content = f.read()
            
            if not content or content.strip() == "":
                return {
                    "success": False,
                    "error": "íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "chunks_count": 0
                }
            
            # ê¸°ë³¸ ì²­í‚¹
            chunks = await self._smart_text_chunking(content, 1500, 200)
            
            if not chunks:
                return {
                    "success": False,
                    "error": "ìœ íš¨í•œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "chunks_count": 0
                }
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            fallback_metadata = {
                **metadata,
                "processing_method": "basic_text",
                "processing_time": 0.1,
                "file_type": file_extension
            }
            
            # ë²¡í„° ì €ì¥
            success = await self.add_document_chunks(file_id, chunks, fallback_metadata)
            
            return {
                "success": success,
                "chunks_count": len(chunks),
                "processing_method": "basic_text",
                "processing_time": 0.1
            }
            
        except Exception as e:
            print(f"âŒ í´ë°± í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "chunks_count": 0
            }
    
    async def vectorize_with_docling_pipeline(
        self, 
        file_path: str, 
        file_id: str, 
        metadata: Dict[str, Any],
        enable_docling: bool = True,
        docling_options: Optional[DoclingOptions] = None
    ) -> Dict[str, Any]:
        """
        í†µí•©ëœ ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ (Docling í™œìš© ê°€ëŠ¥)
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            file_id: íŒŒì¼ ID
            metadata: ë©”íƒ€ë°ì´í„°
            enable_docling: Docling ì‚¬ìš© ì—¬ë¶€
            docling_options: Docling ì˜µì…˜
            
        Returns:
            ë²¡í„°í™” ê²°ê³¼
        """
        try:
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            print(f"ğŸš€ í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘: {file_path}")
            print(f"ğŸ“Š íŒŒì¼ ì •ë³´: {file_size / 1024 / 1024:.2f} MB, Docling í™œì„±í™”: {enable_docling}")
            
            if enable_docling:
                print("ğŸ”§ Docling ê¸°ë°˜ ì²˜ë¦¬ ì‹œì‘...")
                # Doclingì„ ìš°ì„ ì ìœ¼ë¡œ ì‹œë„
                result = await self.process_document_with_docling(
                    file_path, file_id, metadata, docling_options
                )
                
                if result["success"]:
                    print("âœ… Docling ê¸°ë°˜ ë²¡í„°í™” ì„±ê³µ")
                    return result
                else:
                    print(f"âš ï¸ Docling ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                    print("â†ªï¸ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ì „í™˜ ì¤‘...")
            else:
                print("ğŸ“ Docling ë¹„í™œì„±í™” - ê¸°ë³¸ ì²˜ë¦¬ ì‚¬ìš©")
            
            # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ í´ë°±
            print("ğŸ“ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œì‘...")
            fallback_start_time = time.time()
            result = await self._fallback_text_processing(file_path, file_id, metadata)
            fallback_elapsed = time.time() - fallback_start_time
            
            if result["success"]:
                print(f"âœ… ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„±ê³µ ({fallback_elapsed:.2f}ì´ˆ ì†Œìš”)")
                print(f"âš™ï¸ ì²˜ë¦¬ ë°©ë²•: {result.get('processing_method', 'ê¸°ë³¸')}")
            else:
                print(f"âŒ ëª¨ë“  ì²˜ë¦¬ ë°©ë²• ì‹¤íŒ¨ ({fallback_elapsed:.2f}ì´ˆ ì†Œìš”): {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
            return result
            
        except Exception as e:
            print(f"âŒ ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "chunks_count": 0
            }
    

    
    def _backup_and_reset_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ë°±ì—…í•˜ê³  ë¦¬ì…‹í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ ì—°ê²° í•´ì œ
            VectorService._client = None
            VectorService._collection = None
            
            import shutil
            import time
            
            # ì ê¹ ëŒ€ê¸°í•˜ì—¬ íŒŒì¼ ì ê¸ˆ í•´ì œ
            time.sleep(0.5)
            
            if os.path.exists(self.vector_dir):
                try:
                    backup_path = f"{self.vector_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    shutil.move(self.vector_dir, backup_path)
                    print(f"ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ {backup_path}ë¡œ ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")
                except Exception as backup_error:
                    print(f"ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤íŒ¨: {str(backup_error)}")
                    print("ë°±ì—… ì—†ì´ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...")
                    # ë°±ì—… ì‹¤íŒ¨ ì‹œ ê°œë³„ íŒŒì¼ ì •ë¦¬ ì‹œë„
                    try:
                        for file in os.listdir(self.vector_dir):
                            file_path = os.path.join(self.vector_dir, file)
                            try:
                                if os.path.isfile(file_path):
                                    os.unlink(file_path)
                                    print(f"ì‚­ì œë¨: {file}")
                                elif os.path.isdir(file_path):
                                    shutil.rmtree(file_path)
                                    print(f"ë””ë ‰í† ë¦¬ ì‚­ì œë¨: {file}")
                            except Exception as file_error:
                                print(f"ì‚­ì œ ì‹¤íŒ¨: {file} - {str(file_error)} (ê³„ì† ì§„í–‰)")
                    except Exception as cleanup_error:
                        print(f"ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {str(cleanup_error)}")
            
            # ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.vector_dir, exist_ok=True)
            print("ìƒˆë¡œìš´ ChromaDB ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì…‹ ê³¼ì •ì—ì„œ ì˜¤ë¥˜: {str(e)}")
            print("ChromaDBë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    
    def _create_fresh_chromadb(self):
        """ìƒˆë¡œìš´ ChromaDB í´ë¼ì´ì–¸íŠ¸ì™€ ì»¬ë ‰ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            import chromadb
            
            # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            try:
                VectorService._client = chromadb.PersistentClient(
                    path=self.vector_dir,
                    settings=Settings(
                        anonymized_telemetry=False,
                        allow_reset=True
                    )
                )
            except Exception:
                try:
                    VectorService._client = chromadb.PersistentClient(path=self.vector_dir)
                except Exception as fallback_error:
                    print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(fallback_error)}")
                    return False
            
            # ì»¬ë ‰ì…˜ ìƒì„±
            self.collection_name = "langflow"
            
            try:
                VectorService._collection = VectorService._client.get_collection(name=self.collection_name)
                print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚¬ìš©")
            except Exception:
                # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                # ë™ì  ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
                embedding_function = _create_embedding_function()
                if not embedding_function:
                    print("ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰")
                
                VectorService._collection = VectorService._client.create_collection(
                    name=self.collection_name,
                    embedding_function=embedding_function,
                    metadata={"description": "LangFlow ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"}
                )
                print(f"ìƒˆ ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            print(f"ìƒˆ ChromaDB ìƒì„± ì‹¤íŒ¨: {str(e)}")
            VectorService._client = None
            VectorService._collection = None
            return False

    async def find_orphaned_vectors(self) -> Dict[str, Any]:
        """ê³ ì•„ ë²¡í„°(íŒŒì¼ì´ ì‚­ì œë˜ì—ˆì§€ë§Œ ë²¡í„°ëŠ” ë‚¨ì•„ìˆëŠ” ê²½ìš°)ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        try:
            await self._ensure_client()
            
            # ChromaDBì—ì„œ ëª¨ë“  ë²¡í„° ì¡°íšŒ
            all_results = VectorService._collection.get()
            
            # íŒŒì¼ ì„œë¹„ìŠ¤ì—ì„œ í˜„ì¬ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            from ..services.file_service import FileService
            file_service = FileService()
            current_files = await file_service.list_files()
            current_file_ids = {f.file_id for f in current_files}
            
            orphaned_vectors = []
            total_vectors = len(all_results['ids']) if all_results['ids'] else 0
            
            # ë²¡í„° ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì •ìƒì ì¸ ì‘ë‹µ
            if total_vectors == 0:
                return {
                    'total_vectors': 0,
                    'orphaned_vectors': [],
                    'orphaned_count': 0,
                    'current_files_count': len(current_files),
                    'message': 'ë²¡í„°í™”ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë²¡í„°í™”ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”.'
                }
            
            if all_results['metadatas']:
                for i, metadata in enumerate(all_results['metadatas']):
                    if metadata and 'file_id' in metadata:
                        file_id = metadata['file_id']
                        if file_id not in current_file_ids:
                            orphaned_vectors.append({
                                'vector_id': all_results['ids'][i],
                                'file_id': file_id,
                                'text': all_results['documents'][i] if all_results['documents'] else '',
                                'metadata': metadata
                            })
            
            return {
                'total_vectors': total_vectors,
                'orphaned_vectors': orphaned_vectors,
                'orphaned_count': len(orphaned_vectors),
                'current_files_count': len(current_files),
                'message': f'ì´ {total_vectors}ê°œì˜ ë²¡í„° ì¤‘ {len(orphaned_vectors)}ê°œì˜ ê³ ì•„ ë²¡í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.'
            }
            
        except Exception as e:
            print(f"ê³ ì•„ ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                'error': str(e),
                'total_vectors': 0,
                'orphaned_vectors': [],
                'orphaned_count': 0,
                'current_files_count': 0,
                'message': f'ê³ ì•„ ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
            }
    
    async def cleanup_orphaned_vectors(self) -> Dict[str, Any]:
        """ê³ ì•„ ë²¡í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        try:
            orphaned_info = await self.find_orphaned_vectors()
            
            if 'error' in orphaned_info:
                return orphaned_info
            
            if orphaned_info['orphaned_count'] == 0:
                return {
                    'message': orphaned_info.get('message', 'ì •ë¦¬í•  ê³ ì•„ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'),
                    'cleaned_count': 0,
                    'total_vectors': orphaned_info.get('total_vectors', 0)
                }
            
            # ê³ ì•„ ë²¡í„° ì‚­ì œ
            orphaned_ids = [v['vector_id'] for v in orphaned_info['orphaned_vectors']]
            VectorService._collection.delete(ids=orphaned_ids)
            
            print(f"âœ… {orphaned_info['orphaned_count']}ê°œì˜ ê³ ì•„ ë²¡í„°ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            
            return {
                'message': f"{orphaned_info['orphaned_count']}ê°œì˜ ê³ ì•„ ë²¡í„°ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.",
                'cleaned_count': orphaned_info['orphaned_count'],
                'remaining_vectors': orphaned_info['total_vectors'] - orphaned_info['orphaned_count'],
                'total_vectors': orphaned_info['total_vectors']
            }
            
        except Exception as e:
            print(f"ê³ ì•„ ë²¡í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                'error': str(e),
                'cleaned_count': 0,
                'message': f'ê³ ì•„ ë²¡í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
            } 