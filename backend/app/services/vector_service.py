
import os
import json
import asyncio
import time
import threading
import re
import sys
from datetime import datetime
from typing import Dict, Any, List, Optional, Union

# ìœˆë„ìš° í™˜ê²½ì—ì„œ ìœ ë‹ˆì½”ë“œ ì¶œë ¥ ì§€ì›
if sys.platform == "win32":
    import codecs
    try:
        if hasattr(sys.stdout, 'detach'):
            sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
        if hasattr(sys.stderr, 'detach'):
            sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())
    except (AttributeError, OSError):
        # detachê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
        pass

import numpy as np
from ..core.config import settings
from .settings_service import settings_service
from ..models.schemas import DoclingOptions
from ..models.vector_models import VectorMetadata, VectorMetadataService

# PRD2 ê°œì„ : ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì„œë¹„ìŠ¤ ì„í¬íŠ¸ (í—¤ë”© í—¤ë” ì„ë² ë”©ìš©)
try:
    from .chunking_service import ChunkProposal
except ImportError:
    ChunkProposal = None

# ChromaDB ê´€ë ¨ íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì‹œë„
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install chromadb ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

# --- Embedding Function Wrapper ---
class EmbeddingFunction:
    """ChromaDBì™€ í˜¸í™˜ë˜ëŠ” ì„ë² ë”© í•¨ìˆ˜ ë˜í¼ (OpenAI + HuggingFace ì§€ì›)"""
    
    def __init__(self, embedding_model: str = None):
        # ì„¤ì •ì—ì„œ ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        model_settings = settings_service.get_section_settings("models")
        self.embedding_model = embedding_model or model_settings.get("embedding_model", "text-embedding-ada-002")
        self.embedding_provider = model_settings.get("embedding_provider", "openai")
        
        self._openai_client = None
        self._hf_model = None
        self._hf_tokenizer = None
        
        print(f"ğŸ”§ ì„ë² ë”© ì„¤ì • ë¡œë“œ: {self.embedding_provider} - {self.embedding_model}")
        
        # ì„¤ì •ì— ë”°ë¥¸ ëª¨ë¸ íƒ€ì… ê²°ì •
        if self.embedding_provider == "openai":
            self.model_type = "openai"
            print(f"âœ… OpenAI ì„ë² ë”© ì‚¬ìš©: {self.embedding_model}")
        elif self.embedding_provider == "huggingface":
            self.model_type = "huggingface"
            print(f"âœ… HuggingFace ì„ë² ë”© ì‚¬ìš©: {self.embedding_model}")
        else:
            # ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ìë™ ê°ì§€
            self.model_type = self._detect_model_type(self.embedding_model)
            print(f"âœ… ìë™ ê°ì§€ëœ ì„ë² ë”© íƒ€ì…: {self.model_type} - {self.embedding_model}")
    
    def _detect_model_type(self, model_name: str) -> str:
        """ëª¨ë¸ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ íƒ€ì… ê°ì§€ (OpenAI ìš°ì„  ì ìš©)"""
        # OpenAI ëª¨ë¸ ê°•ì œ ì ìš© - ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •ëœ ê²½ìš° ìš°ì„ ê¶Œì„ ê°€ì§
        openai_models = ["text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"]
        
        if model_name in openai_models or model_name.startswith("text-embedding-") or "openai" in model_name.lower():
            print(f"ğŸ”¹ OpenAI ì„ë² ë”© ëª¨ë¸ ê°ì§€: {model_name}")
            return "openai"
        elif "/" in model_name or model_name.startswith("huggingface"):
            print(f"ğŸ”¹ HuggingFace ì„ë² ë”© ëª¨ë¸ ê°ì§€: {model_name}")
            return "huggingface"
        else:
            # ê¸°ë³¸ê°’ì€ OpenAI (1536ì°¨ì›ìœ¼ë¡œ í†µì¼)
            print(f"ğŸ”¹ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸, OpenAIë¡œ ì²˜ë¦¬: {model_name}")
            return "openai"
    
    def get_embedding_dimension(self) -> int:
        """í˜„ì¬ ì„ë² ë”© ëª¨ë¸ì˜ ì°¨ì› ìˆ˜ë¥¼ ë°˜í™˜"""
        if self.model_type == "openai":
            # OpenAI ëª¨ë¸ë³„ ì°¨ì› ì„¤ì •
            if "text-embedding-3-small" in self.embedding_model:
                return 1536
            elif "text-embedding-3-large" in self.embedding_model:
                return 3072
            elif "text-embedding-ada-002" in self.embedding_model:
                return 1536
            else:
                return 1536  # ê¸°ë³¸ê°’
        elif self.model_type == "huggingface":
            # HuggingFace ëª¨ë¸ì˜ ê²½ìš° ë™ì ìœ¼ë¡œ ê°ì§€í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ê°’ ì‚¬ìš©
            if "bge" in self.embedding_model.lower():
                return 768  # BGE ëª¨ë¸ë“¤ì˜ ì¼ë°˜ì ì¸ ì°¨ì›
            else:
                # 1536ì°¨ì›ìœ¼ë¡œ í†µì¼ (ì°¨ì› ë¶ˆì¼ì¹˜ ë°©ì§€)
                return 1536
        else:
            return 1536  # ê¸°ë³¸ê°’
    
    def _get_openai_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if self._openai_client is None:
            try:
                import openai
                # ì„¤ì •ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
                model_settings = settings_service.get_section_settings("models")
                api_key = model_settings.get("embedding_api_key", "")
                
                if not api_key:
                    print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return None
                
                self._openai_client = openai.OpenAI(api_key=api_key)
            except ImportError:
                print("OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
        
        return self._openai_client
    
    def _get_huggingface_model(self):
        """HuggingFace ëª¨ë¸ ì´ˆê¸°í™”"""
        if self._hf_model is None or self._hf_tokenizer is None:
            try:
                from sentence_transformers import SentenceTransformer
                import torch
                import traceback
                import os
                
                # GPU ì‚¬ìš© ë¹„í™œì„±í™” - CPUë§Œ ì‚¬ìš©
                os.environ["CUDA_VISIBLE_DEVICES"] = ""
                torch.cuda.is_available = lambda: False
                
                print(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì¤‘: {self.embedding_model}")
                print(f"PyTorch ë²„ì „: {torch.__version__}")
                print(f"CPU ì „ìš© ëª¨ë“œë¡œ ì‹¤í–‰")
                
                # CPUë§Œ ì‚¬ìš©í•˜ë„ë¡ ëª…ì‹œì  ì„¤ì •
                self._hf_model = SentenceTransformer(
                    self.embedding_model, 
                    device='cpu',
                    cache_folder='./model_cache'  # ë¡œì»¬ ìºì‹œ í´ë” ì§€ì •
                )
                
                # ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ CPUë¡œ ì´ë™
                self._hf_model = self._hf_model.to('cpu')
                    
                print(f"âœ… í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ (CPU ëª¨ë“œ): {self.embedding_model}")
                
            except ImportError as ie:
                print("sentence-transformers íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("pip install sentence-transformers ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
                print(f"ìƒì„¸ ì˜¤ë¥˜: {ie}")
                return None
            except Exception as e:
                print(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                print("ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
                traceback.print_exc()
                return None
        
        return self._hf_model
    
    def __call__(self, input):
        """ChromaDBì—ì„œ í˜¸ì¶œë˜ëŠ” ì„ë² ë”© í•¨ìˆ˜ (ChromaDB v0.4.16+ í˜¸í™˜)"""
        # ChromaDB 0.4.16+ ë²„ì „ì€ ì •í™•íˆ (self, input) ì‹œê·¸ë‹ˆì²˜ë§Œ í—ˆìš©
        if not input:
            return []
        
        if self.model_type == "huggingface":
            return self._create_huggingface_embeddings(input)
        else:
            return self._create_openai_embeddings(input)
    
    def _create_openai_embeddings(self, input_texts):
        """OpenAI ì„ë² ë”© ìƒì„±"""
        client = self._get_openai_client()
        if not client:
            print("OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë”ë¯¸ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            dim = self.get_embedding_dimension()
            return [[0.0] * dim for _ in input_texts]
        
        try:
            # OpenAI ì„ë² ë”© API í˜¸ì¶œ
            response = client.embeddings.create(
                model=self.embedding_model,
                input=input_texts
            )
            
            # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
            embeddings = []
            for item in response.data:
                embeddings.append(item.embedding)
            
            return embeddings
            
        except Exception as e:
            print(f"OpenAI ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ì„ë² ë”© ë°˜í™˜
            dim = self.get_embedding_dimension()
            return [[0.0] * dim for _ in input_texts]
    
    def _create_huggingface_embeddings(self, input_texts):
        """HuggingFace ë¡œì»¬ ì„ë² ë”© ìƒì„±"""
        model = self._get_huggingface_model()
        if not model:
            print("í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë”ë¯¸ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            dim = self.get_embedding_dimension()
            return [[0.0] * dim for _ in input_texts]
        
        try:
            # CPUì—ì„œ ì„ë² ë”© ìƒì„± (í…ì„œ ë³€í™˜ ë¹„í™œì„±í™”)
            embeddings = model.encode(
                input_texts, 
                convert_to_tensor=False,
                convert_to_numpy=True,
                device='cpu',
                show_progress_bar=False
            )
            
            # numpy arrayë¥¼ listë¡œ ë³€í™˜
            if hasattr(embeddings, 'tolist'):
                embeddings = embeddings.tolist()
            
            # 2D ë°°ì—´ì¸ì§€ í™•ì¸í•˜ê³  1Dì¸ ê²½ìš° 2Dë¡œ ë³€í™˜
            if len(embeddings) > 0 and not isinstance(embeddings[0], list):
                embeddings = [embeddings]
            
            print(f"âœ… í—ˆê¹…í˜ì´ìŠ¤ ë¡œì»¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ - {len(input_texts)}ê°œ í…ìŠ¤íŠ¸, ì°¨ì›: {len(embeddings[0]) if embeddings else 0}")
            return embeddings
            
        except Exception as e:
            print(f"í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ì„ë² ë”© ë°˜í™˜
            dim = self.get_embedding_dimension()
            return [[0.0] * dim for _ in input_texts]

async def _create_embedding_function() -> Union[EmbeddingFunction, None]:
    """ì„ë² ë”© í•¨ìˆ˜ ìƒì„±"""
    try:
        # ì„¤ì •ì—ì„œ ì„ë² ë”© ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        model_settings = settings_service.get_section_settings("models")
        embedding_model = model_settings.get("embedding_model", "text-embedding-ada-002")
        
        return EmbeddingFunction(embedding_model)
    except Exception as e:
        print(f"ì„ë² ë”© í•¨ìˆ˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# --- Main Vector Service ---
class VectorService:
    _instance = None
    _initialized = False
    _client = None
    _collection = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(VectorService, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        with VectorService._lock:
            if VectorService._initialized:
                return
            self.vector_dir = os.path.join(settings.DATA_DIR, 'db', 'chromadb')
            self.metadata_dir = os.path.join(settings.DATA_DIR, 'vector_metadata')
            # ë””ë ‰í† ë¦¬ëŠ” í•„ìš”í•  ë•Œë§Œ ìƒì„±í•˜ë„ë¡ ë³€ê²½ (ìë™ ìƒì„± ì œê±°)
            self.metadata_service = VectorMetadataService()
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • (ì„±ëŠ¥ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´)
            from .settings_service import settings_service
            perf_settings = settings_service.get_section_settings("performance")
            self.enable_parallel = perf_settings.get("enableParallelProcessing", True)
            self.max_concurrent_embeddings = perf_settings.get("maxConcurrentEmbeddings", 5)
            self.max_concurrent_chunks = perf_settings.get("maxConcurrentChunks", 20)
            self.batch_size = perf_settings.get("batchSize", 10)
            self.embedding_pool_size = perf_settings.get("embeddingPoolSize", 3)
            
            # ë³‘ë ¬ ì²˜ë¦¬ìš© ì„¸ë§ˆí¬ì–´
            self.embedding_semaphore = asyncio.Semaphore(self.max_concurrent_embeddings)
            self.chunk_semaphore = asyncio.Semaphore(self.max_concurrent_chunks)
            
            # ì„ë² ë”© í•¨ìˆ˜ í’€
            self.embedding_pool = []
            self._embedding_pool_lock = threading.Lock()
            
            # ì„±ëŠ¥ í†µê³„
            self.stats = {
                "total_chunks_processed": 0,
                "total_embeddings_created": 0,
                "average_embedding_time": 0.0,
                "parallel_operations": 0,
                "sequential_operations": 0
            }
            
            print(f"âœ… Vector ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ - ë³‘ë ¬ ì²˜ë¦¬: {self.enable_parallel}")
            VectorService._initialized = True

    # --- í•µì‹¬ì ì¸ ìƒˆ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ---
    async def chunk_and_embed_text(
        self, 
        file_id: str, 
        text_content: str, 
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì²­í‚¹, ì„ë² ë”©, ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (PRD2: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì§€ì›)"""
        try:
            from .settings_service import settings_service
            system_settings = settings_service.get_section_settings("system")
            
            # PRD2: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ í™œì„±í™” ì—¬ë¶€ í™•ì¸
            use_smart_chunking = system_settings.get("use_smart_chunking", True)
            enable_heading_headers = system_settings.get("enable_heading_headers", True)
            
            if use_smart_chunking and ChunkProposal is not None:
                # 1. PRD2 ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì‚¬ìš©
                print(f"ğŸ§  ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ëª¨ë“œ ì‚¬ìš© - í—¤ë” ì„ë² ë”©: {enable_heading_headers}")
                
                from .chunking_service import chunking_service, ChunkingRules
                
                # ì²­í‚¹ ê·œì¹™ ì„¤ì •
                chunk_size = system_settings.get("chunkSize", settings.DEFAULT_CHUNK_SIZE)
                overlap_size = system_settings.get("chunkOverlap", settings.DEFAULT_CHUNK_OVERLAP)
                min_tokens = max(100, chunk_size // 4)  # ìµœì†Œ í† í°ì€ ìµœëŒ€ í† í°ì˜ 1/4
                
                rules = ChunkingRules(
                    max_tokens=chunk_size,
                    min_tokens=min_tokens,
                    overlap_tokens=overlap_size,
                    respect_headings=True,
                    preserve_tables=True,
                    preserve_lists=True,
                    hard_sentence_max_tokens=chunk_size // 2  # ê°•ì œ ë¶„ì ˆ ê¸°ì¤€
                )
                
                # PDF ê²½ë¡œ í™•ì¸ (ì´ë¯¸ì§€ ì—°ê´€ì„±ì„ ìœ„í•´)
                pdf_path = metadata.get("file_path") if metadata.get("filename", "").lower().endswith('.pdf') else None
                
                # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì œì•ˆ ìƒì„±
                chunk_proposals = chunking_service.propose_chunks(
                    text_content, 
                    rules, 
                    use_hierarchical=True, 
                    pdf_path=pdf_path
                )
                
                if not chunk_proposals:
                    return {"success": False, "error": "ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ì—ì„œ ìœ íš¨í•œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                
                print(f"ğŸ“‹ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì™„ë£Œ - {len(chunk_proposals)}ê°œ ì²­í¬ ìƒì„±")
                
                # 2. í—¤ë” í¬í•¨ ì„ë² ë”© ë° ì €ì¥
                if enable_heading_headers:
                    success = await self.add_document_chunks_with_headers(file_id, chunk_proposals, metadata)
                    processing_method = "smart_chunking_with_headers"
                else:
                    # í—¤ë” ì—†ì´ ì¼ë°˜ ì²­í¬ë¡œ ë³€í™˜
                    chunks = [chunk.text for chunk in chunk_proposals]
                    if self.enable_parallel and len(chunks) > 10:
                        success = await self._add_document_chunks_parallel(file_id, chunks, metadata)
                    else:
                        success = await self.add_document_chunks(file_id, chunks, metadata)
                    processing_method = "smart_chunking"
                
                chunks_count = len(chunk_proposals)
            else:
                # 1. ê¸°ì¡´ ê³ ì • í¬ê¸° ì²­í‚¹ ì‚¬ìš©
                print(f"ğŸ“„ ê¸°ì¡´ ì²­í‚¹ ëª¨ë“œ ì‚¬ìš©")
                chunk_size = system_settings.get("chunkSize", settings.DEFAULT_CHUNK_SIZE)
                overlap_size = system_settings.get("chunkOverlap", settings.DEFAULT_CHUNK_OVERLAP)
                
                chunks = self._robust_chunking(text_content, chunk_size, overlap_size)
                if not chunks:
                    return {"success": False, "error": "ìœ íš¨í•œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

                # 2. ì„ë² ë”© ë° ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬ ì˜µì…˜)
                if self.enable_parallel and len(chunks) > 10:  # ì²­í¬ê°€ ë§ì„ ë•Œë§Œ ë³‘ë ¬ ì²˜ë¦¬
                    success = await self._add_document_chunks_parallel(file_id, chunks, metadata)
                else:
                    success = await self.add_document_chunks(file_id, chunks, metadata)
                
                chunks_count = len(chunks)
                processing_method = "fixed_size_chunking"
            
            if success:
                print(f"ğŸ“Š ë²¡í„°í™” ì™„ë£Œ ì²˜ë¦¬ ì‹œì‘ - íŒŒì¼ ID: {file_id} ({processing_method})")
            
            if success:
                # 3. SQLite DBì— ë²¡í„° ë©”íƒ€ë°ì´í„° ì €ì¥
                print(f"ğŸ’¾ SQLite DBì— ë²¡í„° ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹œì‘")
                try:
                    # ì „ì²˜ë¦¬ ì†ŒìŠ¤ ê°ì§€ (ìˆ˜ë™ vs ìë™)
                    preprocessing_source = metadata.get("source", "auto")  # get_file_contentì—ì„œ ì „ë‹¬
                    if preprocessing_source == "manual_preprocessing":
                        preprocessing_source = "manual"
                    else:
                        preprocessing_source = "auto"
                        
                    vector_metadata = VectorMetadata(
                        file_id=file_id,
                        filename=metadata.get("filename", "Unknown"),
                        category_id=metadata.get("category_id"),
                        category_name=metadata.get("category_name"),
                        processing_method=processing_method,
                        preprocessing_source=preprocessing_source,
                        chunk_count=chunks_count,
                        file_size=metadata.get("file_size", 0),
                        page_count=metadata.get("page_count"),
                        table_count=metadata.get("table_count", 0),
                        image_count=metadata.get("image_count", 0),
                        processing_time=0.0  # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ì€ ìƒìœ„ì—ì„œ ê³„ì‚°
                    )
                    
                    # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                    existing_metadata = self.metadata_service.get_metadata(file_id)
                    if existing_metadata:
                        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                        update_success = self.metadata_service.update_metadata(
                            file_id=file_id,
                            chunk_count=chunks_count,
                            processing_method=processing_method,
                            page_count=metadata.get("page_count"),
                            table_count=metadata.get("table_count", 0),
                            image_count=metadata.get("image_count", 0),
                            updated_at=datetime.now()
                        )
                        if update_success:
                            print(f"âœ… SQLite DB ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì„±ê³µ - íŒŒì¼ ID: {file_id}")
                        else:
                            print(f"âš ï¸ SQLite DB ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ - íŒŒì¼ ID: {file_id}")
                    else:
                        # ìƒˆ ë©”íƒ€ë°ì´í„° ìƒì„±
                        create_success = self.metadata_service.create_metadata(vector_metadata)
                        if create_success:
                            print(f"âœ… SQLite DB ë©”íƒ€ë°ì´í„° ìƒì„± ì„±ê³µ - íŒŒì¼ ID: {file_id}")
                        else:
                            print(f"âš ï¸ SQLite DB ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨ - íŒŒì¼ ID: {file_id}")
                    
                except Exception as e:
                    print(f"âš ï¸ SQLite DB ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                    # ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨í•´ë„ ë²¡í„°í™”ëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                
                print(f"âœ… ë²¡í„°í™” ìµœì¢… ì™„ë£Œ - {chunks_count}ê°œ ì²­í¬ ì €ì¥ ì„±ê³µ")
                return {"success": True, "chunks_count": chunks_count}
            else:
                print(f"âŒ ë²¡í„°í™” ì‹¤íŒ¨ - ì²­í¬ ì €ì¥ ì‹¤íŒ¨")
                return {"success": False, "error": "ë²¡í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

        except Exception as e:
            import traceback
            print(f"âŒ ì²­í‚¹ ë° ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {"success": False, "error": str(e)}

    def _robust_chunking(self, content: str, chunk_size: int, overlap_size: int) -> List[str]:
        """
        ì•ˆì •ì„±ì„ ìœ„í•œ ê³ ì • í¬ê¸° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹.
        """
        if not content or chunk_size <= 0:
            return []
        chunks = []
        start_index = 0
        while start_index < len(content):
            end_index = start_index + chunk_size
            chunks.append(content[start_index:end_index])
            next_start = start_index + chunk_size - overlap_size
            if next_start <= start_index:
                start_index += 1
            else:
                start_index = next_start
        return [chunk for chunk in chunks if chunk.strip()]

    # --- ChromaDB ë° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ---
    async def _ensure_client(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if self._client is not None or not CHROMADB_AVAILABLE:
            return
            
        try:
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ í•„ìš”í•  ë•Œë§Œ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.vector_dir, exist_ok=True)
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± - ë‹¨ìˆœí™”ëœ ì„¤ì •
            self._client = chromadb.PersistentClient(path=self.vector_dir)
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.vector_dir}")
        except Exception as e:
            print(f"ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # ìœˆë„ìš° íŒŒì¼ ì ê¸ˆ ì˜¤ë¥˜ ì²˜ë¦¬
            if "WinError 32" in str(e) or "ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ íŒŒì¼ì„ ì‚¬ìš© ì¤‘" in str(e):
                print("ğŸ”„ ìœˆë„ìš° íŒŒì¼ ì ê¸ˆ ê°ì§€, ì¬ì‹œë„ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                success = await self._handle_file_lock_error()
                if not success:
                    self._client = None
                    return
            # tenant ê´€ë ¨ ì˜¤ë¥˜ëŠ” ë³´í†µ ë°ì´í„°ë² ì´ìŠ¤ ì†ìƒì´ë‚˜ ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ
            elif "tenant" in str(e).lower():
                print("ğŸ”„ ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ì†ìƒ ê°ì§€, ì¬ì´ˆê¸°í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                try:
                    # ê¸°ì¡´ ë°ì´í„° ë°±ì—…
                    backup_dir = f"{self.vector_dir}_backup_{int(time.time())}"
                    import shutil
                    if os.path.exists(self.vector_dir):
                        shutil.move(self.vector_dir, backup_dir)
                        print(f"ê¸°ì¡´ ë°ì´í„°ë¥¼ {backup_dir}ë¡œ ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")
                    
                    # ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±
                    os.makedirs(self.vector_dir, exist_ok=True)
                    
                    # ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    self._client = chromadb.PersistentClient(path=self.vector_dir)
                    print(f"âœ… ChromaDB í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì„±ê³µ: {self.vector_dir}")
                except Exception as retry_error:
                    print(f"âŒ ChromaDB ì¬ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {retry_error}")
                    self._client = None
            else:
                self._client = None

    async def _handle_file_lock_error(self) -> bool:
        """ìœˆë„ìš° íŒŒì¼ ì ê¸ˆ ì˜¤ë¥˜ ì²˜ë¦¬"""
        import time
        import gc
        import psutil
        
        try:
            # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
            if hasattr(self, '_client') and self._client:
                try:
                    self._client = None
                except:
                    pass
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            gc.collect()
            
            print("íŒŒì¼ ì ê¸ˆ í•´ì œë¥¼ ìœ„í•´ ì ì‹œ ëŒ€ê¸° ì¤‘...")
            for attempt in range(5):
                try:
                    # ì§§ì€ ëŒ€ê¸°
                    await asyncio.sleep(1)
                    
                    # í´ë¼ì´ì–¸íŠ¸ ì¬ìƒì„± ì‹œë„
                    self._client = chromadb.PersistentClient(path=self.vector_dir)
                    print(f"âœ… íŒŒì¼ ì ê¸ˆ í•´ì œ í›„ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
                    return True
                    
                except Exception as retry_e:
                    if attempt < 4:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´
                        print(f"ì¬ì‹œë„ {attempt + 1}/5 ì‹¤íŒ¨, ê³„ì† ì‹œë„ ì¤‘...")
                        continue
                    else:
                        print(f"âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨: {retry_e}")
                        
                        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ë²¡í„° ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½
                        try:
                            backup_dir = f"{self.vector_dir}_locked_{int(time.time())}"
                            import shutil
                            if os.path.exists(self.vector_dir):
                                shutil.move(self.vector_dir, backup_dir)
                                print(f"ì ê¸´ ë””ë ‰í† ë¦¬ë¥¼ {backup_dir}ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")
                            
                            # ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±
                            os.makedirs(self.vector_dir, exist_ok=True)
                            
                            # ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                            self._client = chromadb.PersistentClient(path=self.vector_dir)
                            print(f"âœ… ìƒˆ ë””ë ‰í† ë¦¬ë¡œ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
                            return True
                            
                        except Exception as final_error:
                            print(f"âŒ ìµœì¢… ë³µêµ¬ ì‹œë„ë„ ì‹¤íŒ¨: {final_error}")
                            return False
            
            return False
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì ê¸ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    async def create_chromadb_database(self) -> bool:
        """ChromaDB ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            print("ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        await self._ensure_client()
        if not self._client:
            print("ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # ì»¬ë ‰ì…˜ ì—°ê²° ì‹œë„
            success = await self._connect_to_chromadb()
            if success:
                print("ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            else:
                print("ChromaDB ì»¬ë ‰ì…˜ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            print(f"ChromaDB ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def _get_collection_dimension(self) -> Optional[int]:
        """ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ì°¨ì›ì„ í™•ì¸í•©ë‹ˆë‹¤."""
        if not self._collection:
            return None
        
        try:
            existing_data = self._collection.get(limit=1, include=['embeddings'])
            embeddings_data = existing_data.get('embeddings') if existing_data else None
            
            if embeddings_data is not None:
                # numpy ë°°ì—´ ì•ˆì „ ê²€ì‚¬
                if hasattr(embeddings_data, 'shape') and len(embeddings_data.shape) > 0 and embeddings_data.shape[0] > 0:
                    embedding_vector = embeddings_data[0]
                    if hasattr(embedding_vector, 'shape'):
                        return embedding_vector.shape[0]
                    else:
                        return len(embedding_vector)
            return None
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ ì°¨ì› í™•ì¸ ì‹¤íŒ¨: {e}")
            return None

    async def _connect_to_chromadb(self, create_if_missing: bool = False):
        """ChromaDB ì»¬ë ‰ì…˜ì— ì—°ê²°í•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE or not self._client:
            return False
        
        try:
            collection_name = "langflow"
            
            # í˜„ì¬ ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
            embedding_function = await _create_embedding_function()
            if not embedding_function:
                print("ì„ë² ë”© í•¨ìˆ˜ ìƒì„± ì‹¤íŒ¨")
                return False
            
            current_dimension = embedding_function.get_embedding_dimension()
            
            try:
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
                self._collection = self._client.get_collection(name=collection_name)
                print(f"âœ… ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ '{collection_name}' ë°œê²¬")
                
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ì°¨ì› í™•ì¸
                existing_dimension = await self._get_collection_dimension()
                
                if existing_dimension is None:
                    print("ğŸ“Š ê¸°ì¡´ ì»¬ë ‰ì…˜ì— ë²¡í„° ë°ì´í„°ê°€ ì—†ìŒ")
                    # ì„ë² ë”© í•¨ìˆ˜ ì ìš©í•˜ê³  ì§„í–‰
                    self._collection._embedding_function = embedding_function
                    return True
                
                print(f"ğŸ“Š ê¸°ì¡´ ì»¬ë ‰ì…˜ ì°¨ì›: {existing_dimension}ì°¨ì›")
                print(f"ğŸ“Š í˜„ì¬ ì„ë² ë”© ì„¤ì •: {embedding_function.embedding_model} ({current_dimension}ì°¨ì›)")
                
                # ì°¨ì› ë¶ˆì¼ì¹˜ ì²´í¬ - ëª…í™•í•œ ì˜¤ë¥˜ ë°˜í™˜
                if existing_dimension != current_dimension:
                    print(f"âŒ ì°¨ì› ë¶ˆì¼ì¹˜: ê¸°ì¡´ ì»¬ë ‰ì…˜({existing_dimension}ì°¨ì›) vs í˜„ì¬ ì„ë² ë”©({current_dimension}ì°¨ì›)")
                    return False
                
                # ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ ì„ë² ë”© í•¨ìˆ˜ ì ìš©í•˜ê³  ì§„í–‰
                self._collection._embedding_function = embedding_function
                print(f"âœ… ChromaDB ì»¬ë ‰ì…˜ ì—°ê²° ì™„ë£Œ (ì°¨ì›: {existing_dimension})")
                return True
                
            except Exception as get_error:
                if not create_if_missing:
                    print("ê²€ìƒ‰ ëª¨ë“œ: ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return False
                
                # ë²¡í„°í™” ì‹œì—ë§Œ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                self._collection = self._client.create_collection(
                    name=collection_name,
                    embedding_function=embedding_function
                )
                print(f"âœ… ìƒˆ ChromaDB ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì™„ë£Œ (ì°¨ì›: {current_dimension})")
                return True
            
        except Exception as e:
            print(f"ChromaDB ì»¬ë ‰ì…˜ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False


    async def add_document_chunks(self, file_id: str, chunks: List[str], metadata: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì²­í¬ë“¤ì„ ChromaDBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        print(f"ğŸ“ ë²¡í„°í™” ëª¨ë“œ: {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì‹œì‘")
        
        if not chunks or not CHROMADB_AVAILABLE:
            return False
        
        await self._ensure_client()
        if not self._client:
            return False
        
        # ë²¡í„°í™” ì „ ì°¨ì› ë¶ˆì¼ì¹˜ ê²€ì‚¬
        if not await self._connect_to_chromadb(create_if_missing=True):
            print("âŒ ë²¡í„°í™” ì‹¤íŒ¨: ì„ë² ë”© ëª¨ë¸ ì°¨ì›ì´ ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ê´€ë¦¬ì í˜ì´ì§€ì—ì„œ ì„ë² ë”© ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•˜ê±°ë‚˜ ë²¡í„° ë°ì´í„°ë¥¼ ì¬ìƒì„±í•´ì£¼ì„¸ìš”.")
            return False
        
        try:
            # ì²­í¬ë³„ë¡œ ê³ ìœ  ID ìƒì„±
            chunk_ids = [f"{file_id}_chunk_{i}" for i in range(len(chunks))]
            
            # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì´ë¯¸ì§€ ì—°ê²° ì •ë³´ í¬í•¨)
            chunk_metadatas = []
            for i, chunk in enumerate(chunks):
                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë§Œ ë³µì‚¬ (ChromaDB í˜¸í™˜ì„±ì„ ìœ„í•´ - None ê°’ ì œê±°)
                chunk_metadata = {
                    "file_id": file_id,
                    "filename": metadata.get("filename", "Unknown"),
                    "preprocessing_method": metadata.get("preprocessing_method", "basic"),
                    "chunk_index": i,
                    "chunk_length": len(chunk)
                }
                
                # Noneì´ ì•„ë‹Œ ê°’ë§Œ ì¶”ê°€ (ChromaDB MetadataValue ì˜¤ë¥˜ ë°©ì§€)
                if metadata.get("category_id"):
                    chunk_metadata["category_id"] = metadata.get("category_id")
                if metadata.get("category_name"):
                    chunk_metadata["category_name"] = metadata.get("category_name")
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€ (ê´€ë ¨ ì´ë¯¸ì§€ë§Œ í•„í„°ë§)
                file_has_images = metadata.get("image_count", 0) > 0
                file_images = metadata.get("images", [])
                
                if file_has_images and file_images:
                    # ì²­í¬ì™€ ê´€ë ¨ëœ ì´ë¯¸ì§€ë§Œ ì°¾ê¸°
                    related_images = self._find_related_images_for_chunk(chunk, metadata)
                    
                    if related_images:
                        chunk_metadata["has_images"] = True
                        chunk_metadata["chunk_image_count"] = len(related_images)
                        # ê´€ë ¨ëœ ì´ë¯¸ì§€ë§Œ JSONìœ¼ë¡œ ì €ì¥
                        import json
                        chunk_metadata["file_images_json"] = json.dumps(related_images, ensure_ascii=False)
                    else:
                        chunk_metadata["has_images"] = False
                        chunk_metadata["chunk_image_count"] = 0
                    
                    # ì „ì²´ íŒŒì¼ í†µê³„ëŠ” ìœ ì§€
                    chunk_metadata["file_image_count"] = metadata.get("image_count", 0)
                else:
                    chunk_metadata["has_images"] = False
                    chunk_metadata["file_image_count"] = 0
                    chunk_metadata["chunk_image_count"] = 0
                
                # ChromaDB í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì •ë¦¬ (None ê°’ ì œê±°)
                cleaned_metadata = self._clean_metadata_for_chromadb(chunk_metadata)
                chunk_metadatas.append(cleaned_metadata)
            
            # ChromaDBì— ì¶”ê°€í•˜ê¸° ì „ ë””ë²„ê¹… ë¡œê·¸
            print(f"ğŸ” ChromaDB ë©”íƒ€ë°ì´í„° ë””ë²„ê¹… - {len(chunk_metadatas)}ê°œ ì²­í¬")
            for i, metadata in enumerate(chunk_metadatas[:3]):  # ì²˜ìŒ 3ê°œë§Œ ë¡œê·¸
                print(f"   ì²­í¬ {i+1} ë©”íƒ€ë°ì´í„°:")
                for key, value in metadata.items():
                    print(f"     {key}: {value} ({type(value).__name__})")
            
            # ChromaDBì— ì¶”ê°€
            self._collection.add(
                documents=chunks,
                metadatas=chunk_metadatas,
                ids=chunk_ids
            )
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["total_chunks_processed"] += len(chunks)
            self.stats["total_embeddings_created"] += len(chunks)
            self.stats["sequential_operations"] += 1
            
            print(f"âœ… íŒŒì¼ {file_id}ì˜ {len(chunks)}ê°œ ì²­í¬ë¥¼ ChromaDBì— ì¶”ê°€ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ChromaDBì— ì²­í¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    # TODO(human): PRD2 í—¤ë”© í—¤ë” ì„ë² ë”© ê¸°ëŠ¥
    # ChunkProposal ê°ì²´ë“¤ì„ ë°›ì•„ í—¤ë”© ê²½ë¡œë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”©í•˜ëŠ” ë©”ì„œë“œë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
    # í—¤ë”© ê²½ë¡œê°€ ìˆìœ¼ë©´ "[í—¤ë”©1 > í—¤ë”©2] ë³¸ë¬¸ë‚´ìš©" í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ê³ ,
    # settingsì—ì„œ enable_heading_headers ì˜µì…˜ì„ í™•ì¸í•˜ì—¬ ê¸°ëŠ¥ì„ í™œì„±í™”í• ì§€ ê²°ì •í•˜ì„¸ìš”.
    async def add_document_chunks_with_headers(self, file_id: str, chunk_proposals: List[ChunkProposal], metadata: Dict[str, Any]) -> bool:
        """PRD2: í—¤ë”© í—¤ë”ë¥¼ í¬í•¨í•œ ì²­í¬ ì„ë² ë”© (ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ )"""
        print(f"ğŸ“ í—¤ë” í¬í•¨ ë²¡í„°í™” ëª¨ë“œ: {len(chunk_proposals)}ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œì‘")
        
        if not chunk_proposals or not CHROMADB_AVAILABLE:
            return False
        
        # ì„¤ì •ì—ì„œ í—¤ë”© í—¤ë” ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€ í™•ì¸
        try:
            from .settings_service import settings_service
            system_settings = settings_service.get_section_settings("system")
            enable_heading_headers = system_settings.get("enable_heading_headers", True)
        except:
            enable_heading_headers = True  # ê¸°ë³¸ê°’: í™œì„±í™”
        
        await self._ensure_client()
        if not self._client:
            return False
        
        # ë²¡í„°í™” ì „ ì°¨ì› ë¶ˆì¼ì¹˜ ê²€ì‚¬
        if not await self._connect_to_chromadb(create_if_missing=True):
            print("âŒ ë²¡í„°í™” ì‹¤íŒ¨: ì„ë² ë”© ëª¨ë¸ ì°¨ì›ì´ ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # ì²­í¬ë³„ë¡œ ê³ ìœ  ID ìƒì„±
            chunk_ids = [f"{file_id}_chunk_{chunk.order}" for chunk in chunk_proposals]
            
            # í—¤ë”© í—¤ë”ë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ ìƒì„±
            enhanced_texts = []
            chunk_metadatas = []
            
            for i, chunk in enumerate(chunk_proposals):
                # í—¤ë”© í—¤ë” ìƒì„±
                if enable_heading_headers and chunk.heading_path:
                    # "[í—¤ë”©1 > í—¤ë”©2 > í—¤ë”©3] ë³¸ë¬¸ë‚´ìš©" í˜•ì‹
                    heading_header = " > ".join(chunk.heading_path)
                    enhanced_text = f"[{heading_header}] {chunk.text}"
                    print(f"ğŸ“‹ ì²­í¬ {chunk.order}: í—¤ë”© í—¤ë” ì ìš© - {heading_header}")
                else:
                    enhanced_text = chunk.text
                
                enhanced_texts.append(enhanced_text)
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                chunk_metadata = {
                    "file_id": file_id,
                    "filename": metadata.get("filename", "Unknown"),
                    "category_id": metadata.get("category_id"),
                    "category_name": metadata.get("category_name"),
                    "preprocessing_method": metadata.get("preprocessing_method", "basic"),
                    "chunk_index": i,
                    "chunk_order": chunk.order,
                    "chunk_length": len(chunk.text),
                    "enhanced_length": len(enhanced_text),
                    "has_heading_header": enable_heading_headers and bool(chunk.heading_path),
                    "heading_path": " > ".join(chunk.heading_path) if chunk.heading_path else None,
                    "page_start": chunk.page_start,
                    "page_end": chunk.page_end,
                    "token_estimate": chunk.token_estimate,
                    "quality_warnings_count": len(chunk.quality_warnings) if chunk.quality_warnings else 0
                }
                
                # ì´ë¯¸ì§€ ì°¸ì¡° ì •ë³´ ì¶”ê°€ (PRD2 ê°œì„ )
                if chunk.image_refs:
                    chunk_metadata["has_images"] = True
                    chunk_metadata["chunk_image_count"] = len(chunk.image_refs)
                    # ì´ë¯¸ì§€ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
                    import json
                    image_data = []
                    for img_ref in chunk.image_refs:
                        image_data.append({
                            "image_id": img_ref.image_id,
                            "image_type": img_ref.image_type,
                            "distance_to_text": img_ref.distance_to_text,
                            "page": img_ref.bbox.page if img_ref.bbox else None,
                            "description": img_ref.description
                        })
                    chunk_metadata["chunk_images_json"] = json.dumps(image_data, ensure_ascii=False)
                else:
                    chunk_metadata["has_images"] = False
                    chunk_metadata["chunk_image_count"] = 0
                
                # ChromaDB í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì •ë¦¬ (None ê°’ ì œê±°)
                cleaned_metadata = self._clean_metadata_for_chromadb(chunk_metadata)
                chunk_metadatas.append(cleaned_metadata)
            
            # ChromaDBì— ì¶”ê°€
            self._collection.add(
                ids=chunk_ids,
                documents=enhanced_texts,
                metadatas=chunk_metadatas
            )
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            if hasattr(self, '_performance_stats'):
                self._performance_stats["chunks_added"] += len(chunk_proposals)
                self._performance_stats["sequential_operations"] += 1
            
            header_count = sum(1 for chunk in chunk_proposals if chunk.heading_path)
            print(f"âœ… í—¤ë” í¬í•¨ ë²¡í„°í™” ì™„ë£Œ - {len(chunk_proposals)}ê°œ ì²­í¬ ì €ì¥ (í—¤ë” ì ìš©: {header_count}ê°œ)")
            return True
            
        except Exception as e:
            print(f"âŒ í—¤ë” í¬í•¨ ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _clean_metadata_for_chromadb(self, metadata: Dict) -> Dict:
        """ChromaDB í˜¸í™˜ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì •ë¦¬ (None ê°’ ì œê±°)"""
        cleaned = {}
        for key, value in metadata.items():
            if value is not None:  # None ê°’ ì œì™¸
                if isinstance(value, (str, int, float, bool)):  # ChromaDB í—ˆìš© íƒ€ì…ë§Œ
                    cleaned[key] = value
                elif isinstance(value, (list, dict)):  # ì»¬ë ‰ì…˜ì€ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                    import json
                    cleaned[key] = json.dumps(value, ensure_ascii=False)
                else:  # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                    cleaned[key] = str(value)
        return cleaned
    
    def _find_related_images_for_chunk(self, chunk_text: str, metadata: Dict) -> List[Dict]:
        """ì²­í¬ í…ìŠ¤íŠ¸ì™€ ê´€ë ¨ëœ ì´ë¯¸ì§€ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        related_images = []
        
        try:
            # metadataì—ì„œ text_image_relations ê°€ì ¸ì˜¤ê¸°
            text_image_relations = metadata.get("text_image_relations", [])
            
            if not text_image_relations:
                return related_images
            
            # ì²­í¬ í…ìŠ¤íŠ¸ì™€ ê´€ë ¨ëœ ì´ë¯¸ì§€ë“¤ ì°¾ê¸°
            for relation in text_image_relations:
                related_text = relation.get("related_text", "")
                
                # í…ìŠ¤íŠ¸ ë§¤ì¹­ ë°©ë²•ë“¤
                # 1. ì§ì ‘ í¬í•¨ ì—¬ë¶€ í™•ì¸
                text_overlap = self._calculate_text_overlap(chunk_text, related_text)
                
                # 2. ë†’ì€ ë§¤ì¹­ë„ë¥¼ ê°€ì§„ ê´€ê³„ë§Œ ì„ íƒ
                if text_overlap > 0.3 or relation.get("confidence", 0) > 0.7:
                    image_info = {
                        "image_id": relation.get("image_id"),
                        "image_path": relation.get("image_path"),
                        "page": relation.get("page"),
                        "relationship_type": relation.get("relationship_type"),
                        "confidence": relation.get("confidence", 0),
                        "text_overlap": text_overlap
                    }
                    related_images.append(image_info)
            
            # ì¤‘ë³µ ì œê±° (ê°™ì€ ì´ë¯¸ì§€ê°€ ì—¬ëŸ¬ ê´€ê³„ë¡œ ë§¤ì¹­ëœ ê²½ìš°)
            seen_image_ids = set()
            unique_related_images = []
            for img in related_images:
                if img["image_id"] not in seen_image_ids:
                    seen_image_ids.add(img["image_id"])
                    unique_related_images.append(img)
            
            # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            unique_related_images.sort(key=lambda x: x.get("confidence", 0), reverse=True)
            
            return unique_related_images[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
            
        except Exception as e:
            return []
    
    def _calculate_text_overlap(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ê²¹ì¹˜ëŠ” ì •ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not text1 or not text2:
            return 0.0
        
        try:
            # ê°„ë‹¨í•œ ë‹¨ì–´ ê¸°ë°˜ ë§¤ì¹­
            words1 = set(re.findall(r'\w+', text1.lower()))
            words2 = set(re.findall(r'\w+', text2.lower()))
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            
            return intersection / union if union > 0 else 0.0
            
        except Exception:
            return 0.0

    async def search_similar_chunks(self, query: str, top_k: int = 5, category_ids: List[str] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ì²­í¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        print(f"ğŸ” ê²€ìƒ‰ ëª¨ë“œ: ì¿¼ë¦¬ '{query[:50]}...' (top_k={top_k})")
        
        if not query or not CHROMADB_AVAILABLE:
            return []
        
        await self._ensure_client()
        if not self._client:
            return []
        
        # ê²€ìƒ‰ ì „ ì°¨ì› ë¶ˆì¼ì¹˜ ê²€ì‚¬
        if not await self._connect_to_chromadb(create_if_missing=False):
            print("âŒ ê²€ìƒ‰ ì‹¤íŒ¨: ì„ë² ë”© ëª¨ë¸ ì°¨ì›ì´ ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šê±°ë‚˜ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # ì¹´í…Œê³ ë¦¬ í•„í„° ì„¤ì •
            where_clause = None
            if category_ids:
                where_clause = {"category_id": {"$in": category_ids}}
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰
            results = self._collection.query(
                query_texts=[query],
                n_results=top_k,
                where=where_clause,
                include=["documents", "metadatas", "distances"]
            )
            
            if not results or not results['documents'] or not results['documents'][0]:
                return []
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
            similar_chunks = []
            for i in range(len(results['documents'][0])):
                metadata = results['metadatas'][0][i] if i < len(results['metadatas'][0]) else {}
                
                chunk_data = {
                    "content": results['documents'][0][i],
                    "metadata": metadata,
                    "similarity": 1 - results['distances'][0][i] if i < len(results['distances'][0]) else 0.0,
                    "has_images": metadata.get("has_images", False),
                    "related_images": metadata.get("related_images", []),
                    "image_count": metadata.get("image_count", 0)
                }
                similar_chunks.append(chunk_data)
            
            return similar_chunks
            
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_status(self) -> Dict[str, Any]:
        """
        Provides a standardized status report for the ChromaDB connection and data.
        """
        # ChromaDB íŒ¨í‚¤ì§€ ê°€ìš©ì„± ë¨¼ì € í™•ì¸
        if not CHROMADB_AVAILABLE:
            return {
                "connected": False,
                "total_vectors": 0,
                "collection_count": 0,
                "collections": [],
                "error": "ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œë„
        await self._ensure_client()
        
        if not self._client:
            return {
                "connected": False,
                "total_vectors": 0,
                "collection_count": 0,
                "collections": [],
                "error": "ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            }

        try:
            # Check connection
            self._client.heartbeat() # Returns a nanosecond timestamp
            
            collections = self._client.list_collections()
            total_vectors = 0
            collection_names = []
            collection_dimension = None
            
            for collection in collections:
                try:
                    total_vectors += collection.count()
                    collection_names.append(collection.name)
                    
                    # ì°¨ì› ì •ë³´ í™•ì¸ (ì²« ë²ˆì§¸ ì»¬ë ‰ì…˜ì—ì„œ)
                    if collection_dimension is None and collection.name == "langflow":
                        self._collection = collection
                        collection_dimension = await self._get_collection_dimension()
                        
                except Exception as e:
                    # Could fail if a collection is corrupt, but we can still report others
                    print(f"Could not get count for collection {collection.name}: {e}")

            return {
                "connected": True,
                "total_vectors": total_vectors,
                "collection_count": len(collections),
                "collections": collection_names,
                "dimension": collection_dimension,
                "error": None
            }

        except Exception as e:
            return {
                "connected": False,
                "total_vectors": 0,
                "collection_count": 0,
                "collections": [],
                "error": f"Failed to connect or retrieve status from ChromaDB: {str(e)}"
            }
    
    async def delete_document_vectors(self, file_id: str) -> bool:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ë²¡í„° ë°ì´í„°ë¥¼ ChromaDBì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            print("ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë²¡í„° ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return True
        
        await self._ensure_client()
        if not self._client:
            print("ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë²¡í„° ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return True
        
        try:
            # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            if not await self._connect_to_chromadb(create_if_missing=False):
                print("ChromaDB ì»¬ë ‰ì…˜ì´ ì—†ì–´ ë²¡í„° ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return True
            
            # íŒŒì¼ IDë¡œ í•„í„°ë§í•˜ì—¬ í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ
            # ChromaDBì—ì„œ where ì¡°ê±´ìœ¼ë¡œ file_idê°€ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°ë¥¼ ì°¾ì•„ ì‚­ì œ
            try:
                # ë¨¼ì € í•´ë‹¹ file_idì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                existing_data = self._collection.get(
                    where={"file_id": file_id},
                    include=["metadatas"]
                )
                
                if existing_data and existing_data['ids']:
                    # ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì˜ IDë“¤ì„ ëª¨ë‘ ì‚­ì œ
                    self._collection.delete(ids=existing_data['ids'])
                    print(f"âœ… íŒŒì¼ {file_id}ì˜ ë²¡í„° ë°ì´í„° {len(existing_data['ids'])}ê°œ ì‚­ì œ ì™„ë£Œ")
                else:
                    print(f"íŒŒì¼ {file_id}ì˜ ë²¡í„° ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                # ë©”íƒ€ë°ì´í„° ì„œë¹„ìŠ¤ì—ì„œë„ í•´ë‹¹ íŒŒì¼ ë°ì´í„° ì‚­ì œ
                await self.metadata_service.delete_file_metadata(file_id)
                
                return True
                
            except Exception as delete_error:
                print(f"ë²¡í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {delete_error}")
                return False
                
        except Exception as e:
            print(f"âŒ ë²¡í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    async def get_document_chunks(self, file_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        if not CHROMADB_AVAILABLE:
            return []
        
        await self._ensure_client()
        if not self._client:
            return []
        
        try:
            if not await self._connect_to_chromadb(create_if_missing=False):
                return []
            
            # íŒŒì¼ IDë¡œ í•„í„°ë§í•˜ì—¬ í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
            results = self._collection.get(
                where={"file_id": file_id},
                include=["documents", "metadatas", "distances"]
            )
            
            if not results or not results['ids']:
                return []
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            chunks = []
            for i in range(len(results['ids'])):
                chunk_data = {
                    "id": results['ids'][i],
                    "content": results['documents'][i] if i < len(results['documents']) else "",
                    "metadata": results['metadatas'][i] if i < len(results['metadatas']) else {}
                }
                chunks.append(chunk_data)
            
            return chunks
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    async def clear_all_data(self) -> bool:
        """ChromaDBì˜ ëª¨ë“  ë²¡í„° ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤ (ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°ëŠ” ìœ ì§€)"""
        if not CHROMADB_AVAILABLE:
            print("ChromaDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë°ì´í„° í´ë¦¬ì–´ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return True
        
        await self._ensure_client()
        if not self._client:
            print("ChromaDB í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë°ì´í„° í´ë¦¬ì–´ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return True
        
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì—°ê²°
            if not await self._connect_to_chromadb(create_if_missing=False):
                print("ChromaDB ì»¬ë ‰ì…˜ì´ ì—†ì–´ ë°ì´í„° í´ë¦¬ì–´ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return True
            
            if self._collection:
                # ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                try:
                    all_data = self._collection.get(include=["metadatas"])
                    
                    if all_data and all_data['ids']:
                        # ëª¨ë“  ë²¡í„° ë°ì´í„° ì‚­ì œ
                        self._collection.delete(ids=all_data['ids'])
                        print(f"âœ… ChromaDBì—ì„œ {len(all_data['ids'])}ê°œì˜ ë²¡í„° ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
                    else:
                        print("ì‚­ì œí•  ë²¡í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        
                except Exception as delete_error:
                    print(f"ë²¡í„° ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {delete_error}")
                    return False
            
            # ë©”íƒ€ë°ì´í„° ì„œë¹„ìŠ¤ì—ì„œë„ ëª¨ë“  ë°ì´í„° ì‚­ì œ
            try:
                success = await self.metadata_service.clear_all_metadata()
                if success:
                    print("âœ… ë©”íƒ€ë°ì´í„° í…Œì´ë¸” í´ë¦¬ì–´ ì™„ë£Œ")
                else:
                    print("âš ï¸ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” í´ë¦¬ì–´ ì‹¤íŒ¨")
            except Exception as meta_error:
                print(f"ë©”íƒ€ë°ì´í„° í´ë¦¬ì–´ ì¤‘ ì˜¤ë¥˜: {meta_error}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì „ì²´ ë°ì´í„° í´ë¦¬ì–´ ì‹¤íŒ¨: {e}")
            return False
    
    # --- ë³‘ë ¬ ì²˜ë¦¬ ë©”ì„œë“œë“¤ ---
    async def _get_embedding_function(self):
        """ì„ë² ë”© í•¨ìˆ˜ í’€ì—ì„œ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ì—°ê²° í’€ë§)"""
        with self._embedding_pool_lock:
            if len(self.embedding_pool) < self.embedding_pool_size:
                # ìƒˆ ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
                embedding_func = await _create_embedding_function()
                if embedding_func:
                    self.embedding_pool.append(embedding_func)
                return embedding_func
            else:
                # ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš© (ë¼ìš´ë“œ ë¡œë¹ˆ)
                return self.embedding_pool[len(self.embedding_pool) % self.embedding_pool_size]
    
    async def _create_single_embedding(self, embedding_func, chunk: str) -> List[float]:
        """ë‹¨ì¼ ì²­í¬ ì„ë² ë”© ìƒì„±"""
        try:
            # í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_chunk = chunk.strip()
            if not normalized_chunk:
                # ì„ë² ë”© í•¨ìˆ˜ì—ì„œ ì°¨ì› ê°€ì ¸ì˜¤ê¸°
                embedding_func = await _create_embedding_function()
                dim = embedding_func.get_embedding_dimension() if embedding_func else 1536
                return [0.0] * dim
            
            # ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°) - ChromaDB 0.4.16+ í˜¸í™˜
            embeddings = await asyncio.to_thread(embedding_func, [normalized_chunk])
            if embeddings:
                return embeddings[0]
            else:
                dim = embedding_func.get_embedding_dimension() if hasattr(embedding_func, 'get_embedding_dimension') else 1536
                return [0.0] * dim
            
        except Exception as e:
            print(f"ë‹¨ì¼ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œì—ë„ ì ì ˆí•œ ì°¨ì› ë°˜í™˜
            try:
                dim = embedding_func.get_embedding_dimension() if hasattr(embedding_func, 'get_embedding_dimension') else 1536
            except:
                dim = 1536
            return [0.0] * dim
    
    async def _add_document_chunks_parallel(self, file_id: str, chunks: List[str], metadata: Dict[str, Any]) -> bool:
        """ë³‘ë ¬ë¡œ ë¬¸ì„œ ì²­í¬ë“¤ì„ ì²˜ë¦¬í•˜ê³  ChromaDBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        if not chunks or not CHROMADB_AVAILABLE:
            return False
        
        await self._ensure_client()
        if not self._client:
            return False
        
        if not await self._connect_to_chromadb(create_if_missing=True):
            return False
        
        try:
            start_time = time.time()
            print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ - {len(chunks)}ê°œ ì²­í¬")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²­í¬ ì²˜ë¦¬
            batch_size = self.batch_size
            all_chunk_data = []
            
            for batch_start in range(0, len(chunks), batch_size):
                batch_end = min(batch_start + batch_size, len(chunks))
                batch_chunks = chunks[batch_start:batch_end]
                
                # ë³‘ë ¬ ì„ë² ë”© ìƒì„±
                async with self.embedding_semaphore:
                    embedding_func = await self._get_embedding_function()
                    if not embedding_func:
                        print(f"ì„ë² ë”© í•¨ìˆ˜ ìƒì„± ì‹¤íŒ¨")
                        continue
                    
                    # ë°°ì¹˜ ë‚´ ê° ì²­í¬ì— ëŒ€í•´ ë³‘ë ¬ë¡œ ì„ë² ë”© ìƒì„±
                    embedding_tasks = []
                    for chunk in batch_chunks:
                        task = asyncio.create_task(self._create_single_embedding(embedding_func, chunk))
                        embedding_tasks.append(task)
                    
                    # ëª¨ë“  ì„ë² ë”© ì™„ë£Œ ëŒ€ê¸°
                    embeddings = await asyncio.gather(*embedding_tasks, return_exceptions=True)
                    
                    # ê²°ê³¼ ìˆ˜ì§‘
                    for i, (chunk, embedding) in enumerate(zip(batch_chunks, embeddings)):
                        if isinstance(embedding, Exception):
                            print(f"ì²­í¬ {batch_start + i} ì„ë² ë”© ì‹¤íŒ¨: {embedding}")
                            # ì‹¤íŒ¨ ì‹œ ì„ë² ë”© í•¨ìˆ˜ì—ì„œ ì°¨ì› ê°€ì ¸ì˜¤ê¸°
                            dim = embedding_func.get_embedding_dimension() if hasattr(embedding_func, 'get_embedding_dimension') else 1536
                            embedding = [0.0] * dim
                        else:
                            # ì„ë² ë”© ê°’ ê²€ì¦
                            if not embedding or len(embedding) == 0:
                                print(f"âš ï¸ ì²­í¬ {batch_start + i} ì„ë² ë”©ì´ ë¹„ì–´ìˆìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
                                dim = embedding_func.get_embedding_dimension() if hasattr(embedding_func, 'get_embedding_dimension') else 1536
                                embedding = [0.0] * dim
                            elif not isinstance(embedding, list) or not isinstance(embedding[0], (int, float)):
                                print(f"âš ï¸ ì²­í¬ {batch_start + i} ì„ë² ë”© í˜•ì‹ ì˜¤ë¥˜: {type(embedding)}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                                dim = embedding_func.get_embedding_dimension() if hasattr(embedding_func, 'get_embedding_dimension') else 1536
                                embedding = [0.0] * dim
                            else:
                                print(f"âœ… ì²­í¬ {batch_start + i} ì„ë² ë”© ìƒì„± ì„±ê³µ (ì°¨ì›: {len(embedding)})")
                        
                        chunk_id = f"{file_id}_chunk_{batch_start + i}"
                        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë§Œ ìƒì„± (ChromaDB í˜¸í™˜ì„±ì„ ìœ„í•´)
                        chunk_metadata = {
                            "file_id": file_id,
                            "filename": metadata.get("filename", "Unknown"),
                            "category_id": metadata.get("category_id"),
                            "category_name": metadata.get("category_name"),
                            "preprocessing_method": metadata.get("preprocessing_method", "basic"),
                            "chunk_index": batch_start + i,
                            "chunk_length": len(chunk)
                        }
                        
                        # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€ (í˜ì´ì§€ ê¸°ë°˜ í•„í„°ë§, ë³‘ë ¬ ì²˜ë¦¬ìš©)
                        file_has_images = metadata.get("image_count", 0) > 0
                        file_images = metadata.get("images", [])
                        
                        if file_has_images and file_images:
                            # ì²­í¬ì˜ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (chunk_metadataì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì¶”ë¡ )
                            chunk_page = chunk_metadata.get("page", 0)
                            
                            # ê°™ì€ í˜ì´ì§€ì— ìˆëŠ” ì´ë¯¸ì§€ë§Œ í•„í„°ë§
                            page_images = []
                            for img in file_images:
                                img_page = img.get("page", 0)
                                if img_page == chunk_page:
                                    page_images.append(img)
                            
                            # í˜ì´ì§€ì— ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            if page_images:
                                chunk_metadata["has_images"] = True
                                chunk_metadata["chunk_image_count"] = len(page_images)
                                # í•´ë‹¹ í˜ì´ì§€ì˜ ì´ë¯¸ì§€ë§Œ JSONìœ¼ë¡œ ì €ì¥
                                import json
                                chunk_metadata["file_images_json"] = json.dumps(page_images, ensure_ascii=False)
                            else:
                                chunk_metadata["has_images"] = False
                                chunk_metadata["chunk_image_count"] = 0
                            
                            # ì „ì²´ íŒŒì¼ í†µê³„ëŠ” ìœ ì§€
                            chunk_metadata["file_image_count"] = metadata.get("image_count", 0)
                        else:
                            chunk_metadata["has_images"] = False
                            chunk_metadata["file_image_count"] = 0
                        
                        # ChromaDB í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì •ë¦¬ (None ê°’ ì œê±°)
                        cleaned_metadata = self._clean_metadata_for_chromadb(chunk_metadata)
                        
                        all_chunk_data.append({
                            "id": chunk_id,
                            "document": chunk,
                            "metadata": cleaned_metadata,
                            "embedding": embedding
                        })
            
            # ChromaDBì— ì¼ê´„ ì¶”ê°€
            if all_chunk_data:
                print(f"ğŸ”„ ChromaDBì— {len(all_chunk_data)}ê°œ ì²­í¬ ì €ì¥ ì‹œì‘")
                self._collection.add(
                    documents=[d["document"] for d in all_chunk_data],
                    metadatas=[d["metadata"] for d in all_chunk_data],
                    ids=[d["id"] for d in all_chunk_data],
                    embeddings=[d["embedding"] for d in all_chunk_data]
                )
                
                # ì €ì¥ í›„ ì‹¤ì œ ê°œìˆ˜ í™•ì¸
                collection_count = self._collection.count()
                print(f"âœ… ChromaDB ì €ì¥ ì™„ë£Œ - {len(all_chunk_data)}ê°œ ì²­í¬ ì €ì¥")
                print(f"ğŸ“Š í˜„ì¬ ì»¬ë ‰ì…˜ ì´ ë²¡í„° ìˆ˜: {collection_count}ê°œ")
                
                processing_time = time.time() - start_time
                self.stats["total_chunks_processed"] += len(chunks)
                self.stats["total_embeddings_created"] += len(all_chunk_data)
                self.stats["parallel_operations"] += 1
                
                print(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ - {len(all_chunk_data)}ê°œ ì²­í¬, {processing_time:.2f}ì´ˆ")
                return True
            else:
                print(f"âŒ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ - ìœ íš¨í•œ ì²­í¬ ë°ì´í„° ì—†ìŒ")
                return False
            
        except Exception as e:
            print(f"âŒ ë³‘ë ¬ ì²­í¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        total_ops = self.stats["parallel_operations"] + self.stats["sequential_operations"]
        return {
            **self.stats,
            "embedding_pool_size": len(self.embedding_pool),
            "parallel_ratio": self.stats["parallel_operations"] / max(1, total_ops),
            "average_chunks_per_operation": self.stats["total_chunks_processed"] / max(1, total_ops)
        }


    def cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
            if hasattr(self, '_client') and self._client:
                try:
                    self._client = None
                except:
                    pass
            
            # ì»¬ë ‰ì…˜ ì •ë¦¬
            if hasattr(self, '_collection') and self._collection:
                try:
                    self._collection = None
                except:
                    pass
            
            # ì„ë² ë”© í’€ ì •ë¦¬
            if hasattr(self, 'embedding_pool'):
                self.embedding_pool.clear()
            
            print("VectorService ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def vectorize_with_docling_pipeline(
        self,
        file_path: str,
        file_id: str,
        metadata: Dict[str, Any],
        enable_docling: bool = True,
        docling_options: Optional[DoclingOptions] = None
    ) -> Dict[str, Any]:
        """Docling í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ - ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° í¬í•¨"""
        print(f"ğŸš€ Docling í†µí•© ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘: {file_path}")
        start_time = time.time()
        
        try:
            # Docling ì²˜ë¦¬ ìˆ˜í–‰
            if enable_docling and docling_options:
                from .docling_service import DoclingService
                docling_service = DoclingService()
                
                if docling_service.is_available:
                    print("ğŸ“„ Docling ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...")
                    docling_result = await docling_service.process_document(file_path, docling_options)
                    
                    if docling_result.success:
                        print(f"âœ… Docling ì²˜ë¦¬ ì„±ê³µ - ì´ë¯¸ì§€: {len(docling_result.images)}ê°œ, í…Œì´ë¸”: {len(docling_result.tables)}ê°œ")
                        
                        # ë©”ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        text_content = docling_result.content.get("markdown", "") or docling_result.content.get("text", "")
                        
                        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ì •ë¦¬ (base64 ë°ì´í„° ì œê±°)
                        safe_images = []
                        for img in docling_result.images:
                            safe_img = {
                                "id": img.get("id"),
                                "page": img.get("page"),
                                "bbox": img.get("bbox"),
                                "description": img.get("description"),  # ì´ë¯¸ ì•ˆì „í•œ ì„¤ëª…ìœ¼ë¡œ ì„¤ì •ë¨
                                "image_path": img.get("image_path"),
                                "caption": img.get("caption"),
                                "label": img.get("label"),
                                "source": img.get("source")
                            }
                            safe_images.append(safe_img)
                        
                        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (add_document_chunks í•¨ìˆ˜ì™€ í˜¸í™˜ë˜ë„ë¡)
                        image_metadata = {
                            "has_images": len(safe_images) > 0,
                            "image_count": len(safe_images),
                            "table_count": len(docling_result.tables),
                            "page_count": docling_result.metadata.get("page_count", 0),
                            "images": safe_images,  # ì •ë¦¬ëœ ì•ˆì „í•œ ì´ë¯¸ì§€ ë°ì´í„°
                            "text_image_relations": docling_result.content.get("text_image_relations", []),
                            "related_images": [img.get("image_path") for img in safe_images if img.get("image_path")],
                            "image_details": json.dumps(safe_images) if safe_images else "[]"
                        }
                        
                        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì™€ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ê²°í•©
                        enhanced_metadata = {**metadata, **image_metadata}
                        
                        # í…ìŠ¤íŠ¸ ì²­í‚¹ ë° ë²¡í„°í™” (ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
                        chunks_result = await self.chunk_and_embed_text(file_id, text_content, enhanced_metadata)
                        
                        if chunks_result.get("success"):
                            processing_time = time.time() - start_time
                            print(f"âœ… Docling í†µí•© ë²¡í„°í™” ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
                            
                            return {
                                "success": True,
                                "chunks_count": chunks_result.get("chunks_count", 0),
                                "processing_method": "docling",
                                "processing_time": processing_time,
                                "image_count": len(docling_result.images),
                                "table_count": len(docling_result.tables)
                            }
                        else:
                            return {"success": False, "error": chunks_result.get("error", "ì²­í‚¹ ë° ë²¡í„°í™” ì‹¤íŒ¨")}
                    else:
                        print(f"âš ï¸ Docling ì²˜ë¦¬ ì‹¤íŒ¨: {docling_result.error}")
                        # í´ë°±: ê¸°ì¡´ ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš©
                        return await self._fallback_vectorization(file_path, file_id, metadata)
                else:
                    print("âš ï¸ Docling ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€")
                    return await self._fallback_vectorization(file_path, file_id, metadata)
            else:
                # Docling ë¹„í™œì„±í™” ì‹œ ê¸°ì¡´ ì²˜ë¦¬ ë°©ì‹
                return await self._fallback_vectorization(file_path, file_id, metadata)
                
        except Exception as e:
            print(f"âŒ Docling í†µí•© ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {"success": False, "error": str(e)}
    
    async def _fallback_vectorization(self, file_path: str, file_id: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ì¡´ ë²¡í„°í™” ë°©ì‹ (í´ë°±)"""
        print("ğŸ”„ ê¸°ì¡´ ë²¡í„°í™” ë°©ì‹ìœ¼ë¡œ í´ë°±")
        
        try:
            # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹)
            with open(file_path, 'rb') as file:
                import PyPDF2
                reader = PyPDF2.PdfReader(file)
                text_content = ""
                for page in reader.pages:
                    text_content += page.extract_text() + "\n"
            
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° (ì´ë¯¸ì§€ ì •ë³´ ì—†ìŒ)
            basic_metadata = {
                **metadata,
                "has_images": False,
                "image_count": 0,
                "table_count": 0,
                "page_count": len(reader.pages) if 'reader' in locals() else 0,
                "related_images": [],
                "image_details": "[]"
            }
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹ ë° ë²¡í„°í™” (ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
            chunks_result = await self.chunk_and_embed_text(file_id, text_content, basic_metadata)
            
            if chunks_result.get("success"):
                return {
                    "success": True,
                    "chunks_count": chunks_result.get("chunks_count", 0),
                    "processing_method": "fallback",
                    "processing_time": time.time() - time.time(),
                    "image_count": 0,
                    "table_count": 0
                }
            else:
                return {"success": False, "error": chunks_result.get("error", "í´ë°± ë²¡í„°í™” ì‹¤íŒ¨")}
                
        except Exception as e:
            print(f"âŒ í´ë°± ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    @classmethod
    def reset_instance(cls):
        """VectorService ì¸ìŠ¤í„´ìŠ¤ ì¬ì„¤ì •"""
        try:
            if cls._instance:
                cls._instance.cleanup_resources()
            cls._instance = None
            cls._initialized = False
            print("VectorService ì¸ìŠ¤í„´ìŠ¤ ì¬ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            print(f"ì¸ìŠ¤í„´ìŠ¤ ì¬ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")

