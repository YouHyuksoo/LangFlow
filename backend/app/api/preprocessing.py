"""
ìˆ˜ë™ ì „ì²˜ë¦¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ API ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form
from fastapi.security import HTTPBearer
from typing import Optional, List, Dict, Any
from datetime import datetime
import logging
import os
import json

from ..models.vector_models import (
    manual_preprocessing_service, 
    file_metadata_service,
    PreprocessingRunStatus,
    AnnotationType,
    AnnotationRelationType
)
from ..services.chunking_service import (
    chunking_service,
    ChunkingRules,
    ChunkProposal,
    QualityWarning,
    ChunkQualityIssue
)
from ..services.file_service import FileService
from .users import get_admin_user
from ..core.config import settings
from ..core.logger import get_console_logger

logger = get_console_logger()
router = APIRouter()
security = HTTPBearer()

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
file_service = FileService()

# ==================== ë°ì´í„° ìŠ¤í‚¤ë§ˆ ====================

class PreprocessingFileResponse:
    """ì „ì²˜ë¦¬ íŒŒì¼ ì •ë³´ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    def __init__(self, **data):
        self.file_id = data.get("file_id")
        self.filename = data.get("filename")
        self.upload_time = data.get("upload_time")
        self.file_size = data.get("file_size")
        self.category_name = data.get("category_name")
        self.preprocessing_status = data.get("preprocessing_status", "NOT_STARTED")
        self.preprocessing_completed_at = data.get("preprocessing_completed_at")
        self.processing_time = data.get("processing_time", 0.0)

class AnnotationRequest:
    """ì£¼ì„ ìƒì„±/ìˆ˜ì • ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    def __init__(self, **data):
        self.order = data.get("order", 0)
        self.label = data.get("label", "")
        self.annotation_type = data.get("type", "paragraph")
        self.coordinates = data.get("coordinates", {})
        self.ocr_text = data.get("ocr_text")
        self.extracted_text = data.get("extracted_text")
        self.processing_options = data.get("processing_options", {})
        self.temp_id = data.get("temp_id")  # í´ë¼ì´ì–¸íŠ¸ ì„ì‹œ ID

class RelationshipRequest:
    """ê´€ê³„ ìƒì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    def __init__(self, **data):
        self.from_annotation_id = data.get("from_annotation_id")
        self.to_annotation_id = data.get("to_annotation_id")
        self.relationship_type = data.get("type", "connects_to")
        self.description = data.get("description")
        self.weight = data.get("weight", 1.0)


# ==================== PRD ë°©ì‹ ì²­í‚¹ ìŠ¤í‚¤ë§ˆ ====================

class ChunkingRulesRequest:
    """ì²­í‚¹ ê·œì¹™ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    def __init__(self, **data):
        # ê³µí†µ ê·œì¹™
        self.max_tokens = data.get("max_tokens", 800)
        self.min_tokens = data.get("min_tokens", 200)
        self.overlap_tokens = data.get("overlap_tokens", 80)
        self.respect_headings = data.get("respect_headings", True)
        self.preserve_tables = data.get("preserve_tables", True)
        self.preserve_lists = data.get("preserve_lists", True)
        self.drop_short_chunks = data.get("drop_short_chunks", False)
        self.snap_to_sentence = data.get("snap_to_sentence", True)
        self.use_hierarchical = data.get("use_hierarchical", True)
        self.hard_sentence_max_tokens = data.get("hard_sentence_max_tokens", 1000)
        self.version = data.get("version", "2.0")
        self.created_at = data.get("created_at")
        
        # ë¬¸ì¥ ë¶„í•  ë°©ë²• ì„ íƒ
        self.sentence_splitter = data.get("sentence_splitter", "kss")
        
        # ë°©ë²•ë³„ ì „ìš© ì˜µì…˜
        # KSS ì˜µì…˜ (Python KSS 6.0.5 í˜¸í™˜)
        kss_options = data.get("kss_options", {})
        self.kss_backend = kss_options.get("backend", "punct")
        self.kss_num_workers = kss_options.get("num_workers", 1)
        self.kss_strip = kss_options.get("strip", True)
        self.kss_return_morphemes = kss_options.get("return_morphemes", False)
        self.kss_ignores = kss_options.get("ignores", [])
        
        kiwi_options = data.get("kiwi_options", {})
        self.kiwi_model_path = kiwi_options.get("model_path", "")
        self.kiwi_integrate_allomorph = kiwi_options.get("integrate_allomorph", True)
        self.kiwi_load_default_dict = kiwi_options.get("load_default_dict", True)
        self.kiwi_max_unk_form_len = kiwi_options.get("max_unk_form_len", 8)
        
        regex_options = data.get("regex_options", {})
        self.regex_sentence_endings = regex_options.get("sentence_endings", "[.!?]")
        self.regex_preserve_abbreviations = regex_options.get("preserve_abbreviations", True)
        self.regex_custom_patterns = regex_options.get("custom_patterns", [])
        
        # Recursive ì˜µì…˜
        recursive_options = data.get("recursive_options", {})
        self.recursive_separators = recursive_options.get("separators", ["\n\n", "\n", " ", ""])
        self.recursive_keep_separator = recursive_options.get("keep_separator", False)
        self.recursive_is_separator_regex = recursive_options.get("is_separator_regex", False)
    
    def to_chunking_rules(self) -> ChunkingRules:
        """ChunkingRules ê°ì²´ë¡œ ë³€í™˜ (ì¤‘ì•™ ì§‘ì¤‘ ì„¤ì • ê¸°ë°˜)"""
        # TODO(human): ì—ë””í„°ì—ì„œ ì „ë‹¬ë°›ì€ ê·œì¹™ì„ ì¤‘ì•™ ì§‘ì¤‘ ì„¤ì •ê³¼ ë³‘í•©í•˜ëŠ” ë¡œì§ êµ¬í˜„
        # ì¤‘ì•™ ì§‘ì¤‘ ì„¤ì •ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ í•˜ê³ , ì—ë””í„°ì—ì„œ ì˜¤ë²„ë¼ì´ë“œëœ ê°’ë§Œ ì ìš©
        return ChunkingRules.from_settings({
            # ì—ë””í„°ì—ì„œ ì˜¤ë²„ë¼ì´ë“œëœ ê°’ë“¤ ì „ë‹¬
            "max_tokens": getattr(self, 'max_tokens', None),
            "min_tokens": getattr(self, 'min_tokens', None),
            "overlap_tokens": getattr(self, 'overlap_tokens', None),
            "sentence_splitter": getattr(self, 'sentence_splitter', None),
            # ê¸°íƒ€ í•„ìš”í•œ ì˜¤ë²„ë¼ì´ë“œ ê°’ë“¤...
        })


class ChunkEditRequest:
    """ì²­í¬ í¸ì§‘ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    def __init__(self, **data):
        self.chunk_id = data.get("chunk_id")
        self.text = data.get("text")
        self.label = data.get("label")
        self.order = data.get("order")


class ChunkMergeRequest:
    """ì²­í¬ ë³‘í•© ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    def __init__(self, **data):
        self.chunk_ids = data.get("chunk_ids", [])  # ë³‘í•©í•  ì²­í¬ ID ë¦¬ìŠ¤íŠ¸


class ChunkSplitRequest:
    """ì²­í¬ ë¶„í•  ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    def __init__(self, **data):
        self.chunk_id = data.get("chunk_id")
        self.split_position = data.get("split_position", 0)  # ë¬¸ì¥ ì¸ë±ìŠ¤


class SaveChunksRequest:
    """ì²­í¬ ì €ì¥ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    def __init__(self, **data):
        self.file_id = data.get("file_id")
        self.chunks = data.get("chunks", [])
        self.embed_now = data.get("embed_now", True)

# ==================== ì¸ì¦ ë° ê¶Œí•œ ====================
# get_admin_userë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ APIì™€ ë™ì¼í•œ ê¶Œí•œ ì²´í¬ ë°©ì‹ ì‚¬ìš©

# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@router.get("/files", 
           summary="ì „ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ì¡°íšŒ",
           description="ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ê³¼ ê° íŒŒì¼ì˜ ì „ì²˜ë¦¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
async def get_preprocessing_files(
    limit: Optional[int] = None,
    admin_user = Depends(get_admin_user)
):
    """ì „ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        files_data = manual_preprocessing_service.get_files_for_preprocessing(limit=limit)
        
        response_data = []
        for file_data in files_data:
            # ë‚ ì§œ í•„ë“œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            upload_time = file_data["upload_time"]
            if upload_time and hasattr(upload_time, 'isoformat'):
                upload_time = upload_time.isoformat()
            elif upload_time and not isinstance(upload_time, str):
                upload_time = str(upload_time)
            
            completed_at = file_data["preprocessing_completed_at"]
            if completed_at and hasattr(completed_at, 'isoformat'):
                completed_at = completed_at.isoformat()
            elif completed_at and not isinstance(completed_at, str):
                completed_at = str(completed_at)
            
            response_data.append({
                "file_id": file_data["file_id"],
                "filename": file_data["filename"],
                "upload_time": upload_time,
                "file_size": file_data["file_size"],
                "category_name": file_data["category_name"],
                "preprocessing_status": file_data["preprocessing_status"],
                "preprocessing_completed_at": completed_at,
                "processing_time": file_data["processing_time"]
            })
        
        return {
            "success": True,
            "data": response_data,
            "total": len(response_data)
        }
        
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/files/{file_id}/start", 
            summary="ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘",
            description="íŠ¹ì • íŒŒì¼ì— ëŒ€í•œ ìˆ˜ë™ ì „ì²˜ë¦¬ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
async def start_preprocessing(
    file_id: str,
    admin_user = Depends(get_admin_user)
):
    """ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘"""
    try:
        logger.info(f"ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘ ìš”ì²­ ë°›ìŒ - file_id: {file_id}")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        file_metadata = file_metadata_service.get_file(file_id)
        if not file_metadata:
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - file_id: {file_id}")
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info(f"íŒŒì¼ í™•ì¸ ì™„ë£Œ - filename: {file_metadata.filename}")
        
        # ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘
        run_id = manual_preprocessing_service.start_preprocessing(file_id)
        if not run_id:
            logger.error(f"ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘ ì‹¤íŒ¨ - file_id: {file_id}")
            raise HTTPException(status_code=500, detail="ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        logger.info(f"ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘ ì„±ê³µ - run_id: {run_id}")
        
        return {
            "success": True,
            "message": "ì „ì²˜ë¦¬ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
            "data": {
                "run_id": run_id,
                "file_id": file_id,
                "status": "IN_PROGRESS"
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì „ì²˜ë¦¬ ì‘ì—… ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/files/{file_id}/metadata", 
           summary="ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ",
           description="ì™„ë£Œëœ ì „ì²˜ë¦¬ ì‘ì—…ì˜ ì£¼ì„ ë° ê´€ê³„ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
async def get_preprocessing_metadata(
    file_id: str,
    admin_user = Depends(get_admin_user)
):
    """ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
    try:
        preprocessing_data = manual_preprocessing_service.get_preprocessing_data(file_id)
        
        if not preprocessing_data:
            return {
                "success": True,
                "message": "ì „ì²˜ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤",
                "data": None
            }
        
        # ISO í˜•ì‹ìœ¼ë¡œ ë‚ ì§œ ë³€í™˜ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        completed_at = preprocessing_data.get("completed_at")
        if completed_at:
            if hasattr(completed_at, 'isoformat'):
                preprocessing_data["completed_at"] = completed_at.isoformat()
            elif not isinstance(completed_at, str):
                preprocessing_data["completed_at"] = str(completed_at)
        
        return {
            "success": True,
            "data": preprocessing_data
        }
        
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/files/{file_id}/metadata", 
            summary="ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì €ì¥",
            description="ìˆ˜ë™ ì „ì²˜ë¦¬ ì‘ì—…ì˜ ì£¼ì„ ë° ê´€ê³„ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
async def save_preprocessing_metadata(
    file_id: str,
    request_data: Dict[str, Any],
    admin_user = Depends(get_admin_user)
):
    """ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì €ì¥"""
    try:
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        file_metadata = file_metadata_service.get_file(file_id)
        if not file_metadata:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìš”ì²­ ë°ì´í„° ê²€ì¦
        annotations_data = request_data.get("annotations", [])
        relationships_data = request_data.get("relationships", [])
        
        if not annotations_data:
            raise HTTPException(status_code=400, detail="ì£¼ì„ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ë°ì´í„° ì €ì¥
        success = manual_preprocessing_service.save_preprocessing_data(
            file_id=file_id,
            annotations_data=annotations_data,
            relationships_data=relationships_data
        )
        
        if not success:
            raise HTTPException(status_code=500, detail="ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "message": "ì „ì²˜ë¦¬ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤",
            "data": {
                "file_id": file_id,
                "annotations_count": len(annotations_data),
                "relationships_count": len(relationships_data) if relationships_data else 0
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/simulate_chunking", 
            summary="ì²­í‚¹ ì‹œë®¬ë ˆì´ì…˜",
            description="ì €ì¥í•˜ì§€ ì•Šê³  í˜„ì¬ ì£¼ì„ ì„¤ì •ìœ¼ë¡œ ì²­í‚¹ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ë³´ê¸°í•©ë‹ˆë‹¤.")
async def simulate_chunking(
    request_data: Dict[str, Any],
    admin_user = Depends(get_admin_user)
):
    """ì²­í‚¹ ì‹œë®¬ë ˆì´ì…˜"""
    try:
        file_id = request_data.get("file_id")
        annotations_data = request_data.get("annotations", [])
        
        if not file_id:
            raise HTTPException(status_code=400, detail="file_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        if not annotations_data:
            raise HTTPException(status_code=400, detail="ì£¼ì„ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # íŒŒì¼ ì •ë³´ ì¡°íšŒ
        file_metadata = file_metadata_service.get_file(file_id)
        if not file_metadata:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ìƒì„± (ì‹¤ì œë¡œëŠ” ì£¼ì„ ìˆœì„œì— ë”°ë¼ í…ìŠ¤íŠ¸ ì²­í‚¹)
        chunks = []
        for i, annotation in enumerate(sorted(annotations_data, key=lambda x: x.get("order", 0))):
            chunk = {
                "chunk_id": i + 1,
                "order": annotation.get("order", 0),
                "label": annotation.get("label", f"ì²­í¬ {i + 1}"),
                "type": annotation.get("type", "paragraph"),
                "text": annotation.get("extracted_text", "") or annotation.get("ocr_text", "") or f"[{annotation.get('label', 'í…ìŠ¤íŠ¸')} ì˜ì—­]",
                "coordinates": annotation.get("coordinates", {}),
                "estimated_tokens": len((annotation.get("extracted_text", "") or annotation.get("ocr_text", "")).split()) * 1.3  # ëŒ€ëµì ì¸ í† í° ìˆ˜
            }
            chunks.append(chunk)
        
        # í†µê³„ ì •ë³´
        total_text_length = sum(len(chunk["text"]) for chunk in chunks)
        total_tokens = sum(chunk["estimated_tokens"] for chunk in chunks)
        
        return {
            "success": True,
            "message": "ì²­í‚¹ ì‹œë®¬ë ˆì´ì…˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "data": {
                "file_id": file_id,
                "filename": file_metadata.filename,
                "chunks": chunks,
                "statistics": {
                    "total_chunks": len(chunks),
                    "total_text_length": total_text_length,
                    "estimated_total_tokens": total_tokens,
                    "average_chunk_size": total_text_length / max(1, len(chunks)),
                    "average_tokens_per_chunk": total_tokens / max(1, len(chunks))
                }
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì²­í‚¹ ì‹œë®¬ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‹œë®¬ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


# ==================== PRD ë°©ì‹ ì²­í‚¹ ì—”ë“œí¬ì¸íŠ¸ ====================

@router.post("/propose_chunks/{file_id}", 
            summary="ìë™ ì²­í‚¹ ì œì•ˆ (PRD ë°©ì‹)",
            description="íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì²­í¬ ë¶„í• ì„ ìë™ìœ¼ë¡œ ì œì•ˆí•©ë‹ˆë‹¤.")
async def propose_chunks(
    file_id: str,
    request_data: Dict[str, Any]
    # ì„ì‹œë¡œ ì¸ì¦ ì œê±°: admin_user = Depends(get_admin_user)
):
    """PRD ë°©ì‹: ìë™ ì²­í‚¹ ì œì•ˆ"""
    try:
        logger.info("="*80)
        logger.info(f"ğŸš€ ì²­í‚¹ ì œì•ˆ API í˜¸ì¶œ ì‹œì‘ - file_id: {file_id}")
        logger.info(f"ğŸ“Š ìš”ì²­ ë°ì´í„° í¬ê¸°: {len(str(request_data))} characters")
        
        # ì²­í‚¹ ê·œì¹™ íŒŒì‹±
        rules_data = request_data.get("rules", {})
        logger.info(f"âš™ï¸  ì²­í‚¹ ê·œì¹™ ì„¤ì •:")
        logger.info(f"   - ìµœëŒ€ í† í°: {rules_data.get('max_tokens', 'N/A')}")
        logger.info(f"   - ìµœì†Œ í† í°: {rules_data.get('min_tokens', 'N/A')}")
        logger.info(f"   - ì˜¤ë²„ë©: {rules_data.get('overlap_tokens', 'N/A')}")
        logger.info(f"   - ë¬¸ì¥ ë¶„í• ê¸°: {rules_data.get('sentence_splitter', 'N/A')}")
        logger.info(f"   - ê³„ì¸µì  ëª¨ë“œ: {rules_data.get('use_hierarchical', 'N/A')}")
        
        # KSS ì˜µì…˜ ë¡œê¹…
        kss_options = rules_data.get('kss_options', {})
        if kss_options:
            logger.info(f"   - KSS ë°±ì—”ë“œ: {kss_options.get('backend', 'N/A')}")
            logger.info(f"   - KSS ì›Œì»¤ ìˆ˜: {kss_options.get('num_workers', 'N/A')}")
        
        rules_request = ChunkingRulesRequest(**rules_data)
        rules = rules_request.to_chunking_rules()
        
        # íŒŒì¼ ì •ë³´ ì¡°íšŒ
        file_metadata = file_metadata_service.get_file(file_id)
        if not file_metadata:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # íŒŒì¼ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        try:
            # file_serviceë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_response = await file_service.get_file_content(file_id)
            if not content_response["success"]:
                raise HTTPException(status_code=400, detail="íŒŒì¼ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            full_text = content_response["content"]
            if not full_text or not full_text.strip():
                raise HTTPException(status_code=400, detail="íŒŒì¼ì— í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ì²­í‚¹ ì œì•ˆ ìƒì„±
        try:
            proposed_chunks = chunking_service.propose_chunks(
                full_text, 
                rules, 
                use_hierarchical=rules_request.use_hierarchical
            )
            logger.info(f"ì²­í‚¹ ì œì•ˆ ì™„ë£Œ - {len(proposed_chunks)}ê°œ ì²­í¬ ìƒì„± (ê³„ì¸µì : {rules_request.use_hierarchical})")
        except Exception as e:
            logger.error(f"ì²­í‚¹ ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=f"ì²­í‚¹ ì œì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        chunks_data = []
        total_tokens = 0
        
        for chunk in proposed_chunks:
            # í’ˆì§ˆ ê²½ê³  ì§ë ¬í™”
            warnings_data = []
            for warning in chunk.quality_warnings:
                warnings_data.append({
                    "issue_type": warning.issue_type.value,
                    "severity": warning.severity,
                    "message": warning.message,
                    "suggestion": warning.suggestion
                })
            
            chunk_data = {
                "chunk_id": chunk.chunk_id,
                "order": chunk.order,
                "text": chunk.text,
                "token_estimate": chunk.token_estimate,
                "page_start": chunk.page_start,
                "page_end": chunk.page_end,
                "heading_path": chunk.heading_path,
                "quality_warnings": warnings_data
            }
            chunks_data.append(chunk_data)
            total_tokens += chunk.token_estimate
        
        logger.info(f"ğŸ ì²­í‚¹ ì œì•ˆ API ì™„ë£Œ - ì´ {len(proposed_chunks)}ê°œ ì²­í¬, {total_tokens}ê°œ í† í°")
        logger.info("="*80)
        
        return {
            "success": True,
            "message": f"ì²­í‚¹ ì œì•ˆì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({len(proposed_chunks)}ê°œ ì²­í¬)",
            "data": {
                "file_id": file_id,
                "filename": file_metadata.filename,
                "chunks": chunks_data,
                "statistics": {
                    "total_chunks": len(proposed_chunks),
                    "total_tokens": total_tokens,
                    "average_tokens_per_chunk": total_tokens / max(1, len(proposed_chunks)),
                    "rules_applied": {
                        "max_tokens": rules.max_tokens,
                        "min_tokens": rules.min_tokens,
                        "overlap_tokens": rules.overlap_tokens,
                        "respect_headings": rules.respect_headings
                    }
                }
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì²­í‚¹ ì œì•ˆ API ì‹¤íŒ¨: {e}")
        logger.info("="*80)
        raise HTTPException(status_code=500, detail=f"ì²­í‚¹ ì œì•ˆ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/merge_chunks", 
            summary="ì²­í¬ ë³‘í•©",
            description="ì„ íƒëœ ì²­í¬ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.")
async def merge_chunks(
    request_data: Dict[str, Any]
    # ì„ì‹œë¡œ ì¸ì¦ ì œê±°: admin_user = Depends(get_admin_user)
):
    """ì²­í¬ ë³‘í•©"""
    try:
        merge_request = ChunkMergeRequest(**request_data)
        
        if len(merge_request.chunk_ids) < 2:
            raise HTTPException(status_code=400, detail="ë³‘í•©í•˜ë ¤ë©´ ìµœì†Œ 2ê°œì˜ ì²­í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # TODO: ì‹¤ì œ ì²­í¬ ë°ì´í„° ì¡°íšŒ ë° ë³‘í•© ë¡œì§ êµ¬í˜„
        # í˜„ì¬ëŠ” ì„±ê³µ ì‘ë‹µë§Œ ë°˜í™˜
        
        return {
            "success": True,
            "message": f"{len(merge_request.chunk_ids)}ê°œ ì²­í¬ê°€ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤",
            "data": {
                "merged_chunk_id": f"merged_{'-'.join(merge_request.chunk_ids[:2])}",
                "original_chunk_ids": merge_request.chunk_ids
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì²­í¬ ë³‘í•© ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ë³‘í•© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/split_chunk", 
            summary="ì²­í¬ ë¶„í• ",
            description="ì„ íƒëœ ì²­í¬ë¥¼ ì§€ì •ëœ ìœ„ì¹˜ì—ì„œ ë¶„í• í•©ë‹ˆë‹¤.")
async def split_chunk(
    request_data: Dict[str, Any]
    # ì„ì‹œë¡œ ì¸ì¦ ì œê±°: admin_user = Depends(get_admin_user)
):
    """ì²­í¬ ë¶„í• """
    try:
        split_request = ChunkSplitRequest(**request_data)
        
        if not split_request.chunk_id:
            raise HTTPException(status_code=400, detail="ë¶„í• í•  ì²­í¬ IDê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        if split_request.split_position <= 0:
            raise HTTPException(status_code=400, detail="ìœ íš¨í•œ ë¶„í•  ìœ„ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # TODO: ì‹¤ì œ ì²­í¬ ë°ì´í„° ì¡°íšŒ ë° ë¶„í•  ë¡œì§ êµ¬í˜„
        # í˜„ì¬ëŠ” ì„±ê³µ ì‘ë‹µë§Œ ë°˜í™˜
        
        return {
            "success": True,
            "message": "ì²­í¬ê°€ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤",
            "data": {
                "original_chunk_id": split_request.chunk_id,
                "split_position": split_request.split_position,
                "new_chunk_ids": [
                    f"{split_request.chunk_id}_part1",
                    f"{split_request.chunk_id}_part2"
                ]
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì²­í¬ ë¶„í•  ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ë¶„í•  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/save_chunks", 
            summary="í¸ì§‘ëœ ì²­í¬ ì €ì¥ (PRD ë°©ì‹)",
            description="ì‚¬ìš©ìê°€ í¸ì§‘í•œ ì²­í¬ë“¤ì„ ì €ì¥í•˜ê³  ì„ë² ë”©ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
async def save_chunks(
    request_data: Dict[str, Any]
    # ì„ì‹œë¡œ ì¸ì¦ ì œê±°: admin_user = Depends(get_admin_user)
):
    """í¸ì§‘ëœ ì²­í¬ ì €ì¥"""
    try:
        save_request = SaveChunksRequest(**request_data)
        
        if not save_request.file_id:
            raise HTTPException(status_code=400, detail="file_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        if not save_request.chunks:
            raise HTTPException(status_code=400, detail="ì €ì¥í•  ì²­í¬ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # íŒŒì¼ ì •ë³´ ì¡°íšŒ
        file_metadata = file_metadata_service.get_file(save_request.file_id)
        if not file_metadata:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì²­í¬ ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
        normalized_chunks = []
        for i, chunk_data in enumerate(save_request.chunks):
            normalized_chunk = {
                "order": chunk_data.get("order", i + 1),
                "label": chunk_data.get("label", f"ì²­í¬ {i + 1}"),
                "type": chunk_data.get("type", "paragraph"),
                "text": chunk_data.get("text", ""),
                "coordinates": chunk_data.get("coordinates", {}),
                "ocr_text": chunk_data.get("ocr_text"),
                "extracted_text": chunk_data.get("extracted_text") or chunk_data.get("text", ""),
                "processing_options": chunk_data.get("processing_options", {}),
                "temp_id": chunk_data.get("chunk_id", f"chunk_{i + 1}")
            }
            normalized_chunks.append(normalized_chunk)
        
        # ìˆ˜ë™ ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì €ì¥
        success = manual_preprocessing_service.save_preprocessing_data(
            save_request.file_id,
            normalized_chunks,
            []  # relationshipsëŠ” í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        )
        
        if not success:
            raise HTTPException(status_code=500, detail="ì²­í¬ ë°ì´í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        # ì„ë² ë”© ì‘ì—… í ì¶”ê°€ (ì˜µì…˜)
        embed_job = None
        if save_request.embed_now:
            embed_job = {
                "file_id": save_request.file_id,
                "chunk_count": len(normalized_chunks),
                "status": "queued"
            }
            # TODO: ì‹¤ì œ ì„ë² ë”© ì„œë¹„ìŠ¤ ì—°ë™
        
        return {
            "success": True,
            "message": f"ì²­í¬ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ ({len(normalized_chunks)}ê°œ ì²­í¬)",
            "data": {
                "file_id": save_request.file_id,
                "filename": file_metadata.filename,
                "saved_chunks": len(normalized_chunks),
                "embed_job": embed_job
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/annotation_types", 
           summary="ì£¼ì„ íƒ€ì… ëª©ë¡ ì¡°íšŒ",
           description="ì‚¬ìš© ê°€ëŠ¥í•œ ì£¼ì„ íƒ€ì…ë“¤ì˜ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.")
async def get_annotation_types(admin_user = Depends(get_admin_user)):
    """ì£¼ì„ íƒ€ì… ëª©ë¡ ì¡°íšŒ"""
    try:
        # AnnotationType enumì—ì„œ íƒ€ì… ëª©ë¡ ìƒì„±
        annotation_types = []
        for annotation_type in AnnotationType:
            type_info = {
                "value": annotation_type.value,
                "label": {
                    "title": "ì œëª©",
                    "paragraph": "ë³¸ë¬¸ ë‹¨ë½", 
                    "list": "ëª©ë¡",
                    "table": "í‘œ",
                    "image": "ì´ë¯¸ì§€",
                    "caption": "ìº¡ì…˜",
                    "header": "í—¤ë”",
                    "footer": "í‘¸í„°", 
                    "sidebar": "ì‚¬ì´ë“œë°”",
                    "custom": "ì‚¬ìš©ì ì •ì˜"
                }.get(annotation_type.value, annotation_type.value),
                "description": {
                    "title": "ë¬¸ì„œì˜ ì œëª©ì´ë‚˜ ì„¹ì…˜ í—¤ë”©",
                    "paragraph": "ì¼ë°˜ì ì¸ ë³¸ë¬¸ ë‹¨ë½",
                    "list": "ëª©ë¡ì´ë‚˜ ë‚˜ì—´ëœ í•­ëª©ë“¤",
                    "table": "í‘œ í˜•íƒœì˜ ë°ì´í„°", 
                    "image": "ì´ë¯¸ì§€ë‚˜ ê·¸ë¦¼",
                    "caption": "ì´ë¯¸ì§€ë‚˜ í‘œì˜ ìº¡ì…˜",
                    "header": "í˜ì´ì§€ë‚˜ ì„¹ì…˜ì˜ í—¤ë”",
                    "footer": "í˜ì´ì§€ë‚˜ ì„¹ì…˜ì˜ í‘¸í„°",
                    "sidebar": "ì‚¬ì´ë“œë°”ë‚˜ ë³„ë„ ì •ë³´",
                    "custom": "ì‚¬ìš©ìê°€ ì •ì˜í•œ íŠ¹ë³„í•œ íƒ€ì…"
                }.get(annotation_type.value, "")
            }
            annotation_types.append(type_info)
        
        return {
            "success": True,
            "data": annotation_types
        }
        
    except Exception as e:
        logger.error(f"ì£¼ì„ íƒ€ì… ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì£¼ì„ íƒ€ì… ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/stats", 
           summary="ì „ì²˜ë¦¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ í†µê³„",
           description="ì „ì²˜ë¦¬ ì‘ì—… í†µê³„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
async def get_preprocessing_stats(
    admin_user = Depends(get_admin_user)
):
    """ì „ì²˜ë¦¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ í†µê³„"""
    try:
        files_data = manual_preprocessing_service.get_files_for_preprocessing()
        
        # ìƒíƒœë³„ íŒŒì¼ ìˆ˜ ê³„ì‚°
        status_counts = {}
        for file_data in files_data:
            status = file_data["preprocessing_status"]
            status_counts[status] = status_counts.get(status, 0) + 1
        
        # ì „ì²´ í†µê³„
        total_files = len(files_data)
        completed_files = status_counts.get("COMPLETED", 0)
        in_progress_files = status_counts.get("IN_PROGRESS", 0)
        not_started_files = status_counts.get("NOT_STARTED", 0)
        
        # ì™„ë£Œëœ ì‘ì—…ì˜ í‰ê·  ì²˜ë¦¬ ì‹œê°„
        completed_processing_times = [
            file_data["processing_time"] 
            for file_data in files_data 
            if file_data["preprocessing_status"] == "COMPLETED" and file_data["processing_time"]
        ]
        average_processing_time = sum(completed_processing_times) / max(1, len(completed_processing_times))
        
        return {
            "success": True,
            "data": {
                "total_files": total_files,
                "completed_files": completed_files,
                "in_progress_files": in_progress_files,
                "not_started_files": not_started_files,
                "completion_rate": completed_files / max(1, total_files),
                "average_processing_time": average_processing_time,
                "status_distribution": status_counts
            }
        }
        
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/annotation-types", 
           summary="ì£¼ì„ íƒ€ì… ëª©ë¡",
           description="ì‚¬ìš© ê°€ëŠ¥í•œ ì£¼ì„ íƒ€ì… ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
async def get_annotation_types(
    admin_user = Depends(get_admin_user)
):
    """ì£¼ì„ íƒ€ì… ëª©ë¡"""
    return {
        "success": True,
        "data": [
            {"value": "title", "label": "ì œëª©", "description": "ë¬¸ì„œ ì œëª©ì´ë‚˜ í—¤ë”©"},
            {"value": "paragraph", "label": "ë³¸ë¬¸", "description": "ì¼ë°˜ì ì¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸"},
            {"value": "list", "label": "ëª©ë¡", "description": "ìˆœì„œê°€ ìˆê±°ë‚˜ ì—†ëŠ” ëª©ë¡"},
            {"value": "table", "label": "í‘œ", "description": "í…Œì´ë¸” ë°ì´í„°"},
            {"value": "image", "label": "ì´ë¯¸ì§€", "description": "ì´ë¯¸ì§€ ì˜ì—­"},
            {"value": "caption", "label": "ìº¡ì…˜", "description": "ì´ë¯¸ì§€ë‚˜ í‘œì˜ ì„¤ëª…"},
            {"value": "header", "label": "í—¤ë”", "description": "í˜ì´ì§€ ìƒë‹¨ ì˜ì—­"},
            {"value": "footer", "label": "í‘¸í„°", "description": "í˜ì´ì§€ í•˜ë‹¨ ì˜ì—­"},
            {"value": "sidebar", "label": "ì‚¬ì´ë“œë°”", "description": "ì¸¡ë©´ ì˜ì—­"},
            {"value": "custom", "label": "ì‚¬ìš©ì ì •ì˜", "description": "ê¸°íƒ€ ì‚¬ìš©ì ì •ì˜ íƒ€ì…"}
        ]
    }


@router.get("/relationship-types", 
           summary="ê´€ê³„ íƒ€ì… ëª©ë¡",
           description="ì‚¬ìš© ê°€ëŠ¥í•œ ì£¼ì„ ê°„ ê´€ê³„ íƒ€ì… ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
async def get_relationship_types(
    admin_user = Depends(get_admin_user)
):
    """ê´€ê³„ íƒ€ì… ëª©ë¡"""
    return {
        "success": True,
        "data": [
            {"value": "connects_to", "label": "ì—°ê²°ë¨", "description": "ë‘ ì˜ì—­ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë¨"},
            {"value": "part_of", "label": "ë¶€ë¶„ì„", "description": "í•œ ì˜ì—­ì´ ë‹¤ë¥¸ ì˜ì—­ì˜ ì¼ë¶€ì„"},
            {"value": "follows", "label": "ë’¤ë”°ë¦„", "description": "ìˆœì°¨ì ìœ¼ë¡œ ë’¤ë”°ë¥´ëŠ” ê´€ê³„"},
            {"value": "references", "label": "ì°¸ì¡°í•¨", "description": "ë‹¤ë¥¸ ì˜ì—­ì„ ì°¸ì¡°í•˜ëŠ” ê´€ê³„"},
            {"value": "caption_of", "label": "ìº¡ì…˜ì„", "description": "ì´ë¯¸ì§€ë‚˜ í‘œì˜ ìº¡ì…˜ ê´€ê³„"}
        ]
    }


@router.post("/reset-status", 
            summary="ì „ì²˜ë¦¬ ìƒíƒœ ë¦¬ì…‹",
            description="ëª¨ë“  ì§„í–‰ì¤‘ ìƒíƒœë¥¼ ë¯¸ì‹œì‘ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤ (ê°œë°œ/ê´€ë¦¬ìš©)")
async def reset_preprocessing_status(
    admin_user = Depends(get_admin_user)
):
    """ì „ì²˜ë¦¬ ìƒíƒœ ë¦¬ì…‹ (ê´€ë¦¬ììš©)"""
    try:
        import sqlite3
        from ..core.config import settings
        
        db_path = os.path.join(settings.DATA_DIR, "db", "users.db")
        
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()
            
            # ì§„í–‰ì¤‘ ìƒíƒœë¥¼ ë¯¸ì‹œì‘ìœ¼ë¡œ ë³€ê²½
            cursor.execute('UPDATE preprocessing_runs SET status = "NOT_STARTED" WHERE status = "IN_PROGRESS"')
            affected = cursor.rowcount
            
        logger.info(f"ì „ì²˜ë¦¬ ìƒíƒœ ë¦¬ì…‹ ì™„ë£Œ - {affected}ê°œ íŒŒì¼ ë³€ê²½")
        
        return {
            "success": True,
            "message": f"{affected}ê°œ íŒŒì¼ì˜ ìƒíƒœë¥¼ ë¯¸ì‹œì‘ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤",
            "data": {
                "affected_count": affected
            }
        }
        
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ìƒíƒœ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ ë¦¬ì…‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


# ==================== ì²­í‚¹ ê·œì¹™ ì„¤ì • ì €ì¥ ====================

@router.post("/save_chunking_settings",
            summary="ì²­í‚¹ ê·œì¹™ ì„¤ì • ì €ì¥",
            description="ì²­í‚¹ ê·œì¹™ ì„¤ì •ì„ ì „ì—­ ì„¤ì •ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
async def save_chunking_settings(
    request_data: Dict[str, Any]
    # ì„ì‹œë¡œ ì¸ì¦ ì œê±°: admin_user = Depends(get_admin_user)
):
    """ì²­í‚¹ ê·œì¹™ ì„¤ì • ì €ì¥"""
    try:
        logger.info("ğŸ’¾ ì²­í‚¹ ê·œì¹™ ì„¤ì • ì €ì¥ ìš”ì²­")
        
        rules_data = request_data.get("rules", {})
        setting_name = request_data.get("name", "ê¸°ë³¸ ì„¤ì •")
        
        logger.info(f"   - ì„¤ì •ëª…: {setting_name}")
        logger.info(f"   - ê·œì¹™ ë°ì´í„°: {rules_data}")
        
        # íŒŒì´ì¬ ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ ì„¤ì • ì €ì¥
        import json
        import os
        from pathlib import Path
        
        # ì„¤ì • ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        settings_dir = Path("chunking_settings")
        settings_dir.mkdir(exist_ok=True)
        
        # ì„¤ì • íŒŒì¼ ì €ì¥
        settings_file = settings_dir / f"{setting_name.replace(' ', '_')}.json"
        
        settings_data = {
            "name": setting_name,
            "rules": rules_data,
            "created_at": datetime.now().isoformat(),
            "version": "1.0"
        }
        
        with open(settings_file, 'w', encoding='utf-8') as f:
            json.dump(settings_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ì²­í‚¹ ê·œì¹™ ì„¤ì • ì €ì¥ ì™„ë£Œ: {settings_file}")
        
        return {
            "success": True,
            "message": f"ì²­í‚¹ ê·œì¹™ '{setting_name}'ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "setting_name": setting_name,
                "file_path": str(settings_file),
                "created_at": settings_data["created_at"]
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ì²­í‚¹ ê·œì¹™ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/load_chunking_settings",
           summary="ì²­í‚¹ ê·œì¹™ ì„¤ì • ëª©ë¡ ì¡°íšŒ",
           description="ì €ì¥ëœ ì²­í‚¹ ê·œì¹™ ì„¤ì • ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.")
async def load_chunking_settings(
    # ì„ì‹œë¡œ ì¸ì¦ ì œê±°: admin_user = Depends(get_admin_user)
):
    """ì €ì¥ëœ ì²­í‚¹ ê·œì¹™ ì„¤ì • ëª©ë¡ ì¡°íšŒ"""
    try:
        import json
        from pathlib import Path
        
        logger.info("ğŸ“ ì²­í‚¹ ê·œì¹™ ì„¤ì • ëª©ë¡ ì¡°íšŒ")
        
        settings_dir = Path("chunking_settings")
        if not settings_dir.exists():
            return {
                "success": True,
                "message": "ì €ì¥ëœ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.",
                "data": []
            }
        
        settings_list = []
        for settings_file in settings_dir.glob("*.json"):
            try:
                with open(settings_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    settings_list.append({
                        "name": data.get("name", settings_file.stem),
                        "created_at": data.get("created_at"),
                        "version": data.get("version", "1.0"),
                        "file_path": str(settings_file)
                    })
            except Exception as e:
                logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {settings_file}: {e}")
                continue
        
        # ìƒì„±ì¼ìˆœ ì •ë ¬
        settings_list.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        
        logger.info(f"âœ… {len(settings_list)}ê°œ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        
        return {
            "success": True,
            "message": f"{len(settings_list)}ê°œì˜ ì €ì¥ëœ ì„¤ì •ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
            "data": settings_list
        }
        
    except Exception as e:
        logger.error(f"âŒ ì²­í‚¹ ê·œì¹™ ì„¤ì • ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/load_chunking_settings/{setting_name}",
           summary="ì²­í‚¹ ê·œì¹™ ì„¤ì • ë¡œë“œ",
           description="ì§€ì •ëœ ì²­í‚¹ ê·œì¹™ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
async def load_chunking_setting(
    setting_name: str
    # ì„ì‹œë¡œ ì¸ì¦ ì œê±°: admin_user = Depends(get_admin_user)
):
    """ì§€ì •ëœ ì²­í‚¹ ê·œì¹™ ì„¤ì • ë¡œë“œ"""
    try:
        import json
        from pathlib import Path
        
        logger.info(f"ğŸ“ ì²­í‚¹ ê·œì¹™ ì„¤ì • ë¡œë“œ: {setting_name}")
        
        settings_dir = Path("chunking_settings")
        settings_file = settings_dir / f"{setting_name.replace(' ', '_')}.json"
        
        if not settings_file.exists():
            raise HTTPException(status_code=404, detail=f"ì„¤ì • '{setting_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with open(settings_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"âœ… ì²­í‚¹ ê·œì¹™ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {setting_name}")
        
        return {
            "success": True,
            "message": f"ì²­í‚¹ ê·œì¹™ '{setting_name}'ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì²­í‚¹ ê·œì¹™ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì„¤ì • ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")