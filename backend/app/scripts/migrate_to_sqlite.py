"""
JSON íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ SQLiteë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import os
import json
import sys
from datetime import datetime
from typing import Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from app.models.vector_models import FileMetadata, FileMetadataService, FileStatus
from app.core.config import settings

class DataMigrator:
    def __init__(self):
        self.file_metadata_service = FileMetadataService()
        self.json_file_path = os.path.join(settings.DATA_DIR, 'files_metadata.json')
        
    def load_json_metadata(self) -> Dict[str, Any]:
        """ê¸°ì¡´ JSON íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if not os.path.exists(self.json_file_path):
            print(f"JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.json_file_path}")
            return {}
        
        try:
            with open(self.json_file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def convert_status(self, old_status: str) -> FileStatus:
        """ê¸°ì¡´ ìƒíƒœë¥¼ ìƒˆ FileStatusë¡œ ë³€í™˜"""
        status_mapping = {
            "uploaded": FileStatus.UPLOADED,
            "preprocessing": FileStatus.PREPROCESSING,
            "preprocessed": FileStatus.PREPROCESSED,
            "vectorizing": FileStatus.VECTORIZING,
            "completed": FileStatus.COMPLETED,
            "failed": FileStatus.FAILED,
            "deleted": FileStatus.DELETED
        }
        return status_mapping.get(old_status, FileStatus.UPLOADED)
    
    def parse_datetime(self, date_str: str) -> datetime:
        """ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜"""
        if not date_str:
            return datetime.now()
        
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í˜•ì‹ ì‹œë„
        formats = [
            "%Y-%m-%d %H:%M:%S.%f",
            "%Y-%m-%dT%H:%M:%S.%f",
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%dT%H:%M:%S"
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        
        # ëª¨ë“  í˜•ì‹ì´ ì‹¤íŒ¨í•˜ë©´ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        print(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {date_str}")
        return datetime.now()
    
    def migrate_file_data(self, file_id: str, file_data: Dict[str, Any]) -> bool:
        """ê°œë³„ íŒŒì¼ ë°ì´í„°ë¥¼ SQLiteë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        try:
            # ê¸°ë³¸ í•„ë“œ ë§¤í•‘
            file_metadata = FileMetadata(
                file_id=file_id,
                filename=file_data.get('filename', ''),
                saved_filename=file_data.get('saved_filename', f"{file_id}.pdf"),
                file_path=file_data.get('file_path', ''),
                file_size=file_data.get('file_size', 0),
                file_hash=file_data.get('file_hash', ''),
                category_id=file_data.get('category_id'),
                category_name=file_data.get('category_name'),
                status=self.convert_status(file_data.get('status', 'uploaded')),
                vectorized=file_data.get('vectorized', False),
                
                # ì‹œê°„ í•„ë“œë“¤
                upload_time=self.parse_datetime(file_data.get('upload_time', '')),
                preprocessing_started_at=self.parse_datetime(file_data.get('preprocessing_started_at', '')) if file_data.get('preprocessing_started_at') else None,
                preprocessing_completed_at=self.parse_datetime(file_data.get('preprocessing_completed_at', '')) if file_data.get('preprocessing_completed_at') else None,
                vectorization_started_at=self.parse_datetime(file_data.get('vectorization_started_at', '')) if file_data.get('vectorization_started_at') else None,
                vectorization_completed_at=self.parse_datetime(file_data.get('vectorization_completed_at', '')) if file_data.get('vectorization_completed_at') else None,
                deleted_at=self.parse_datetime(file_data.get('deleted_at', '')) if file_data.get('deleted_at') else None,
                
                # ì²˜ë¦¬ ì •ë³´
                preprocessing_method=file_data.get('preprocessing_method', 'basic'),
                chunk_count=file_data.get('chunk_count', 0),
                flow_id=file_data.get('used_flow_id') or file_data.get('flow_id'),
                
                # ì—ëŸ¬ ì •ë³´
                error_message=file_data.get('error_message') or file_data.get('error'),
                error_type=file_data.get('error_type'),
                
                # ë¬¸ì„œ ë¶„ì„ ì •ë³´ (ìˆë‹¤ë©´)
                page_count=file_data.get('page_count'),
                table_count=file_data.get('table_count'),
                image_count=file_data.get('image_count'),
            )
            
            # ì²˜ë¦¬ ì˜µì…˜ ì„¤ì • (ìˆë‹¤ë©´)
            if 'processing_options' in file_data:
                file_metadata.set_processing_options(file_data['processing_options'])
            
            # SQLiteì— ì €ì¥
            success = self.file_metadata_service.create_file(file_metadata)
            if success:
                print(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ: {file_id} ({file_data.get('filename')})")
                return True
            else:
                print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {file_id}")
                return False
                
        except Exception as e:
            print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜¤ë¥˜ {file_id}: {e}")
            return False
    
    def migrate_all(self) -> Dict[str, Any]:
        """ëª¨ë“  ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        print("ğŸ”„ JSONì—ì„œ SQLiteë¡œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        # ê¸°ì¡´ JSON ë°ì´í„° ë¡œë“œ
        json_data = self.load_json_metadata()
        if not json_data:
            return {
                "success": False,
                "message": "ë§ˆì´ê·¸ë ˆì´ì…˜í•  JSON ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            }
        
        total_files = len(json_data)
        migrated_count = 0
        failed_count = 0
        
        print(f"ğŸ“‹ ì´ {total_files}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ê° íŒŒì¼ë³„ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
        for file_id, file_data in json_data.items():
            if self.migrate_file_data(file_id, file_data):
                migrated_count += 1
            else:
                failed_count += 1
        
        # ê²°ê³¼ ìš”ì•½
        result = {
            "success": True,
            "total_files": total_files,
            "migrated_count": migrated_count,
            "failed_count": failed_count,
            "message": f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {migrated_count}/{total_files} ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨"
        }
        
        print(f"\nğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼:")
        print(f"   - ì´ íŒŒì¼: {total_files}ê°œ")
        print(f"   - ì„±ê³µ: {migrated_count}ê°œ")
        print(f"   - ì‹¤íŒ¨: {failed_count}ê°œ")
        
        if failed_count == 0:
            print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print(f"âš ï¸  {failed_count}ê°œ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        return result
    
    def create_backup(self) -> bool:
        """JSON íŒŒì¼ ë°±ì—… ìƒì„±"""
        if not os.path.exists(self.json_file_path):
            return True
        
        try:
            backup_path = f"{self.json_file_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            import shutil
            shutil.copy2(self.json_file_path, backup_path)
            print(f"ğŸ“¦ ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_path}")
            return True
        except Exception as e:
            print(f"âŒ ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def verify_migration(self) -> Dict[str, Any]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"""
        print("ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì¤‘...")
        
        # JSON ë°ì´í„°ì™€ SQLite ë°ì´í„° ë¹„êµ
        json_data = self.load_json_metadata()
        sqlite_files = self.file_metadata_service.list_files(include_deleted=True)
        
        json_count = len(json_data)
        sqlite_count = len(sqlite_files)
        
        print(f"ğŸ“Š ë°ì´í„° ìˆ˜ ë¹„êµ:")
        print(f"   - JSON: {json_count}ê°œ")
        print(f"   - SQLite: {sqlite_count}ê°œ")
        
        # ëˆ„ë½ëœ íŒŒì¼ í™•ì¸
        json_file_ids = set(json_data.keys())
        sqlite_file_ids = {f.file_id for f in sqlite_files}
        
        missing_in_sqlite = json_file_ids - sqlite_file_ids
        extra_in_sqlite = sqlite_file_ids - json_file_ids
        
        result = {
            "json_count": json_count,
            "sqlite_count": sqlite_count,
            "missing_in_sqlite": list(missing_in_sqlite),
            "extra_in_sqlite": list(extra_in_sqlite),
            "verified": len(missing_in_sqlite) == 0
        }
        
        if missing_in_sqlite:
            print(f"âš ï¸  SQLiteì— ëˆ„ë½ëœ íŒŒì¼: {len(missing_in_sqlite)}ê°œ")
            for file_id in missing_in_sqlite:
                print(f"   - {file_id}")
        
        if extra_in_sqlite:
            print(f"â„¹ï¸  SQLiteì—ë§Œ ìˆëŠ” íŒŒì¼: {len(extra_in_sqlite)}ê°œ")
        
        if result["verified"]:
            print("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì„±ê³µ!")
        else:
            print("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹¤íŒ¨!")
        
        return result


def main():
    """ë©”ì¸ ë§ˆì´ê·¸ë ˆì´ì…˜ í•¨ìˆ˜"""
    # ìœˆë„ìš° ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
    import sys
    if sys.platform.startswith('win'):
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    
    migrator = DataMigrator()
    
    print("=" * 60)
    print("ğŸš€ LangFlow íŒŒì¼ ë©”íƒ€ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("   JSON â†’ SQLite")
    print("=" * 60)
    
    # 1. ë°±ì—… ìƒì„±
    print("\n1ï¸âƒ£ ë°±ì—… ìƒì„±...")
    if not migrator.create_backup():
        print("âŒ ë°±ì—… ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    # 2. ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    print("\n2ï¸âƒ£ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜...")
    result = migrator.migrate_all()
    
    if not result["success"]:
        print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {result['message']}")
        return
    
    # 3. ê²€ì¦
    print("\n3ï¸âƒ£ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦...")
    verification = migrator.verify_migration()
    
    # 4. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“‹ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ìš”ì•½")
    print("=" * 60)
    print(f"ì´ íŒŒì¼: {result['total_files']}ê°œ")
    print(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ: {result['migrated_count']}ê°œ")
    print(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {result['failed_count']}ê°œ")
    print(f"ê²€ì¦ ê²°ê³¼: {'âœ… ì„±ê³µ' if verification['verified'] else 'âŒ ì‹¤íŒ¨'}")
    
    if verification['verified'] and result['failed_count'] == 0:
        print("\nğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ SQLiteë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ì´ì œ JSON íŒŒì¼ ê¸°ë°˜ ì½”ë“œë¥¼ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸  ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì™„ì „í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()